
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorials/pendulum.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_tutorials_pendulum.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorials_pendulum.py:


Writing your environment with TorchRL
=====================================
**Author**: `Vincent Moens <https://github.com/vmoens>`_

Creating an environment (a simulator or a interface to a physical control system)
is an integrative part of reinforcement learning and control engineering.

TorchRL provides a set of tools to do this in multiple contexts.
This tutorial demonstrates how to use PyTorch and :py:mod:`torchrl` code a pendulum
simulator from the ground up.
It is freely inspired by the Pendulum-v1 implementation from `OpenAI-Gym/Farama-Gymnasium
control library <https://github.com/Farama-Foundation/Gymnasium>`__.

.. figure:: /_static/img/pendulum.gif
   :alt: Pendulum

   Simple Pendulum

Key learnings:

- How to design an environment in TorchRL:

    - Specs (input, observation and reward);
    - Methods: seeding, reset and step;
- Transforming your environment inputs and outputs;
- How to use :class:`tensordict.TensorDict` to carry arbitrary data structures
  from sep to step.

We will touch three crucial components of TorchRL:

* `environments <https://pytorch.org/rl/reference/envs.html>`__
* `transforms <https://pytorch.org/rl/reference/envs.html#transforms>`__
* `models (policy and value function) <https://pytorch.org/rl/reference/modules.html>`__

.. GENERATED FROM PYTHON SOURCE LINES 39-69

To give a sense of what can be achieved with TorchRL's environments, we will
be designing a stateless environment. While stateful environments keep track of
the latest physical state encountered and rely on this to simulate the state-to-state
transition, stateless environments expect the current state to be provided to
them at each step, along with the action undertaken.

Modelling stateless environments gives users full control over the input and
outputs of the simulator: one can reset an experiment at any stage. It also
assumes that we have some control over a task, which may not always be the case
(solving a problem where we cannot control the current state is more challenging
but has a much wider set of applications).

Another advantage of stateless environments is that most of the time they allow
for batched execution of transition simulations. If the backend and the
implementation allow it, an algebraic operation can be executed seamlessly on
scalars, vectors or tensors. This tutorial gives such examples.

This tutorial will be structured as follows:

* We will first get acquainted with the environment properties:
  its shape (``batch_size``), its methods (mainly `step`, `reset` and `set_seed`)
  and finally its specs.
* After having coded our simulator, we will demonstrate how it can be used
  during training with transforms.
* We will explore surprising new avenues that follow from the TorchRL's API,
  including: the possibility of transforming inputs, the vectorized execution
  of the simulation and the possibility of backpropagating through the
  simulation.
* Finally, will train a simple policy to solve the system we implemented.


.. GENERATED FROM PYTHON SOURCE LINES 69-92

.. code-block:: default

    from collections import defaultdict
    from typing import Optional

    import numpy as np
    import torch
    import tqdm
    from tensordict.nn import TensorDictModule
    from tensordict.tensordict import TensorDict, TensorDictBase
    from torch import nn

    from torchrl.data import BoundedTensorSpec, CompositeSpec, UnboundedContinuousTensorSpec
    from torchrl.envs import (
        CatTensors,
        Compose,
        EnvBase,
        TransformedEnv,
        UnsqueezeTransform,
    )
    from torchrl.envs.utils import check_env_specs, step_mdp

    DEFAULT_X = np.pi
    DEFAULT_Y = 1.0








.. GENERATED FROM PYTHON SOURCE LINES 93-152

There are four things one must take care of when designing a new environment
class: ``_reset``, which codes for the resetting of the simulator at a (potentially
random) initial state, ``_step`` which codes for the state transition dynamic,
``_set_seed`` which implements the seeding mechanism and finally the
environment specs.

:func:`_step`
~~~~~~~~~~~~~

The step method is the first thing to consider, as it will encode
the simulation that is of interest to us. In TorchRL, the :class:`torchrl.envs.EnvBase`
class has a :func:`EnvBase.step(tensordict)` method that receives a :class:`tensordict.TensorDict`
instance with an ``"action"`` entry indicating what action is to be taken.
To facilitate the reading and writing from that tensordict and to make sure
that the keys are consistent with what's expected from the library, the
simulation part has been delegated to a private abstract method :func:`_step`
which reads input data from a tensordict, and writes a new tensordict with the
output data.
The :func:`_step` method should:

  1. read the input keys (such as ``"action"``) and execute the simulation
     based on these;
  2. retrieve observations, done state and reward;
  3. write the set of observation value along with the reward and done state
     at the corresponding entries in a new :class:`TensorDict`.

Next, the `step` method will rearrange this output and move the key-pair
values of the observation in a new entry named ``"next"`` and leave the ``"reward"``
and ``"done"`` state at the root level. It will also run some sanity checks
on the shapes of the tensordict content.

Typically, this will look like

.. code-block::

  >>> print(tensordict)
  TensorDict(TODO)
  >>> env.step(tensordict)
  >>> print(tensordict)
  TensorDict(TODO)

In the Pendulum example, our :func:`_step` method will read the relevant entries
from the input tensordict and compute the position and velocity of the
pendulum after the force encoded by the ``"action"`` key has been applied
onto it. We compute the new angular position of the pendulum ``new_th`` as the result
of the previous position ``th`` plus the new velocity ``new_thdot`` over a
time interval ``dt``. Additionally, we pass the ``sin`` and ``cos`` of the
angle to facilitate learning.

Since our goal is to turn the pendulum up and maintain it still in that
position, our ``cost`` (negative reward) function is lower for positions
close to the target and low speeds.
Indeed, we want to punish positions that are far from being "upward"
and/or speeds that are far from 0.

In our example, :func:`_step` is encoded as a static method since our
environment is stateless. In stateful settings, the ``self`` argument is
needed as the state needs to be read from the environment.


.. GENERATED FROM PYTHON SOURCE LINES 152-194

.. code-block:: default



    def _step(tensordict):
        th, thdot = tensordict["th"], tensordict["thdot"]  # th := theta

        g_force = tensordict["params", "g"]
        mass = tensordict["params", "m"]
        length = tensordict["params", "l"]
        dt = tensordict["params", "dt"]
        u = tensordict["action"].squeeze(-1)
        u = u.clamp(-tensordict["params", "max_torque"], tensordict["params", "max_torque"])
        costs = angle_normalize(th) ** 2 + 0.1 * thdot**2 + 0.001 * (u**2)

        new_thdot = (
            thdot
            + (3 * g_force / (2 * length) * th.sin() + 3.0 / (mass * length**2) * u) * dt
        )
        new_thdot = new_thdot.clamp(
            -tensordict["params", "max_speed"], tensordict["params", "max_speed"]
        )
        new_th = th + new_thdot * dt
        reward = -costs.view(*tensordict.shape, 1)
        done = torch.zeros_like(reward, dtype=torch.bool)
        out = TensorDict(
            {
                "th": new_th,
                "sin": new_th.sin(),
                "cos": new_th.cos(),
                "thdot": new_thdot,
                "params": tensordict["params"],
                "reward": reward,
                "done": done,
            },
            tensordict.shape,
        )
        return out


    def angle_normalize(x):
        return ((x + torch.pi) % (2 * torch.pi)) - torch.pi









.. GENERATED FROM PYTHON SOURCE LINES 195-220

:func:`_reset`
~~~~~~~~~~~~~

The second method we need to care about is the :func:`_reset` method. Like
:func:`_step`, it should write the observation entries and possibly a done state
in the tensordict it outputs. In some contexts, it is required that the `_reset`
method receives a command from the function that called it (e.g. in multi-agent
settings we may want to indicate which agent needs to be reset). This is
why the :func:`_reset` method also expects a tensordict as input, albeit
it may perfectly be empty.

The parent :class:`EnvBase.reset` does some simple checks like the
:class:`EnvBase.step` does, such as making sure that a ``"done"`` state
is returned in the output tensordict and that the shapes match what is
expected from the specs.

For us, the only important thing to consider is whether
:class:`EnvBase._reset` contains all the expected observations. Once more,
since we are working with a stateless environment, we pass the configuration
of the pendulum in a ``"params"`` nested tensordict.
Comparing the output of :class:`EnvBase._reset` with :class:`EnvBase._step`

We do not pass a done state as this is not mandatory for :func:`_reset` and
our environment is non-terminating.


.. GENERATED FROM PYTHON SOURCE LINES 220-260

.. code-block:: default



    def _reset(self, tensordict):
        if tensordict is None or tensordict.is_empty():
            # if no tensordict is passed, we generate a single set of hyperparameters
            # Otherwise, we assume that the input tensordict contains all the relevant
            # parameters to get started.
            tensordict = self.gen_params(batch_size=self.batch_size)

        high_th = torch.tensor(DEFAULT_X, device=self.device)
        high_thdot = torch.tensor(DEFAULT_Y, device=self.device)
        low_th = -high_th
        low_thdot = -high_thdot

        # for non batch-locked envs, the input tensordict shape dictates the number
        # of simulators run simultaneously. In other contexts, the initial
        # random state's shape will depend upon the environment batch-size instead.
        th = (
            torch.rand(tensordict.shape, generator=self.rng, device=self.device)
            * (high_th - low_th)
            + low_th
        )
        thdot = (
            torch.rand(tensordict.shape, generator=self.rng, device=self.device)
            * (high_thdot - low_thdot)
            + low_thdot
        )
        out = TensorDict(
            {
                "th": th,
                "sin": th.sin(),
                "cos": th.cos(),
                "thdot": thdot,
                "params": tensordict["params"],
            },
            batch_size=tensordict.shape,
        )
        return out









.. GENERATED FROM PYTHON SOURCE LINES 261-304

Specs
~~~~~

The specs define the input and output domain of the environment.
It is important that the specs accurately define the tensors that will be
received at runtime, as they are often used to carry information about
environments in multiprocessing and distributed settings.
There four specs that we must code in our environment:

* :obj:`EnvBase.observation_spec`: This will be a :class:`torchrl.data.CompositeSpec`
  instance where each key is an observation.
* :obj:`EnvBase.action_spec`: It can be any type of spec, but it is required that it
  corresponds to the ``"action"`` entry in the input tensordict.
* :obj:`EnvBase.input_spec`: contains all the input entries,
  including the :obj:`EnvBase.action_spec` (which is just a pointer to
  :obj:`EnvBase.input_spec['action_spec']`. As for :obj:`EnvBase.ObservationSpec`,
  it is expected that this spec is of type :obj:`torchrl.data.CompositeSpec`.
  to accommodate environments where multiple inputs are expected.
* :obj:`EnvBase.reward_spec`: the reward spec have the particularity of
  having a singleton trailing dimension if the environment has an empty
  batch size. The reason is that we often pass observations in torch models
  that estimate a value estimate with non-empty shape:

  .. code-block::

    >>> next_value = reward + (1 - done) * fun(observation)

  Working with *unsqueezed* rewards allows us to build algorithms that are
  not polluted with squeezing and unsqueezing operations.

TorchRL offers multiple :class:`torchrl.data.TensorSpec`
`subclasses <https://pytorch.org/rl/reference/data.html#tensorspec>`_ to
encode the environment's input and output characteristics.

*Specs shape*: The environment specs leading dimensions must match the
environment batch-size. This is done to enforce that every component of an
environment (including its transforms) have an accurate representation of
the expected input and output shapes. This is something that should be
accurately coded in stateful settings.

For non batch-locked environments such as the one in our example (see below),
this is irrelevant as the environment batch-size will most likely be empty.


.. GENERATED FROM PYTHON SOURCE LINES 304-357

.. code-block:: default



    def _make_spec(self, td_params):
        self.observation_spec = CompositeSpec(
            sin=BoundedTensorSpec(minimum=-1.0, maximum=1.0, shape=(), dtype=torch.float32),
            cos=BoundedTensorSpec(minimum=-1.0, maximum=1.0, shape=(), dtype=torch.float32),
            th=BoundedTensorSpec(
                minimum=-torch.pi,
                maximum=torch.pi,
                shape=(),
                dtype=torch.float32,
            ),
            thdot=BoundedTensorSpec(
                minimum=-td_params["params", "max_speed"],
                maximum=td_params["params", "max_speed"],
                shape=(),
                dtype=torch.float32,
            ),
            # we need to add the "params" to the observation specs, as we want
            # to pass it at each step during a rollout
            params=make_composite_from_td(td_params["params"]),
            shape=(),
        )
        # since the environment is stateless, we expect the previous output as input
        self.input_spec = self.observation_spec.clone()
        # action-spec will be automatically wrapped in input_spec, but the convenient
        # self.action_spec = spec is supported
        self.action_spec = BoundedTensorSpec(
            minimum=-td_params["params", "max_torque"],
            maximum=td_params["params", "max_torque"],
            shape=(1,),
            dtype=torch.float32,
        )
        self.reward_spec = UnboundedContinuousTensorSpec(shape=(*td_params.shape, 1))


    def make_composite_from_td(td):
        # custom funtion to convert a tensordict in a similar spec structure
        # of unbounded values.
        composite = CompositeSpec(
            {
                key: make_composite_from_td(tensor)
                if isinstance(tensor, TensorDictBase)
                else UnboundedContinuousTensorSpec(
                    dtype=tensor.dtype, device=tensor.device, shape=tensor.shape
                )
                for key, tensor in td.items()
            },
            shape=td.shape,
        )
        return composite









.. GENERATED FROM PYTHON SOURCE LINES 358-368

Seeding
~~~~~~~

Seeding an environment is a commong operation when initializing an experiment.
:func:`EnvBase._set_seed` only goal is to set the seed of the contained
simulator. If possible, this operation should not call `reset()` or interact
with the environment execution. The parent :func:`EnvBase.set_seed` method
incorporates a mechanism that allows seeding multiple environments with a
different pseudo-random and reproducible seed.


.. GENERATED FROM PYTHON SOURCE LINES 368-375

.. code-block:: default



    def _set_seed(self, seed: Optional[int]):
        rng = torch.manual_seed(seed)
        self.rng = rng









.. GENERATED FROM PYTHON SOURCE LINES 376-391

Wrapping things together: the :class:`torchrl.envs.EnvBase` class
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We can finally put together the pieces and design our environment class.
The specs initialization needs to be performed during the environment
construction so we must take care of calling the :func:`_make_spec` method
within :func:`PendulumEnv.__init__`.

We add a class method :func:`PendulumEnv.gen_params` which deterministically
generates a set of hyperparameters to be used during execution.

We define the environment as non-`batch-locked` by turning the homonymous
attribute to ``False``. This means that we will not enforce the input
tensordict to have a batch-size that matches the one of the environment


.. GENERATED FROM PYTHON SOURCE LINES 391-440

.. code-block:: default



    class PendulumEnv(EnvBase):
        metadata = {
            "render_modes": ["human", "rgb_array"],
            "render_fps": 30,
        }
        batch_locked = False

        @classmethod
        def gen_params(cls, g=10.0, batch_size=None) -> TensorDictBase:
            if batch_size is None:
                batch_size = []
            td = TensorDict(
                {
                    "params": TensorDict(
                        {
                            "max_speed": 8,
                            "max_torque": 2.0,
                            "dt": 0.05,
                            "g": g,
                            "m": 1.0,
                            "l": 1.0,
                        },
                        [],
                    )
                },
                [],
            )
            if batch_size:
                td = td.expand(batch_size).contiguous()
            return td

        def __init__(self, td_params=None, seed=None, device="cpu"):
            if td_params is None:
                td_params = self.gen_params()

            super().__init__(device=device, batch_size=[])
            self._make_spec(td_params)
            if seed is None:
                seed = torch.empty((), dtype=torch.int64).random_().item()
            self.set_seed(seed)

        _make_spec = _make_spec
        _reset = _reset
        _step = staticmethod(_step)
        _set_seed = _set_seed









.. GENERATED FROM PYTHON SOURCE LINES 441-449

Testing our environment
-----------------------

TorchRL provides a simple function :func:`torchrl.envs.utils.check_env_specs`
to check that a (transformed) environment has an input/output structure that
matches the one dictated by its specs.
Let us try it out:


.. GENERATED FROM PYTHON SOURCE LINES 449-453

.. code-block:: default


    env = PendulumEnv()
    check_env_specs(env)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    check_env_specs succeeded!




.. GENERATED FROM PYTHON SOURCE LINES 454-457

We can have a look at our specs to have a visual representation of the environment
signature


.. GENERATED FROM PYTHON SOURCE LINES 457-461

.. code-block:: default

    print("observation_spec:", env.observation_spec)
    print("input_spec:", env.input_spec)
    print("reward_spec:", env.reward_spec)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    observation_spec: CompositeSpec(
        sin: BoundedTensorSpec(
             shape=torch.Size([]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True), maximum=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)), device=cpu, dtype=torch.float32, domain=continuous),
        cos: BoundedTensorSpec(
             shape=torch.Size([]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True), maximum=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)), device=cpu, dtype=torch.float32, domain=continuous),
        th: BoundedTensorSpec(
             shape=torch.Size([]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True), maximum=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)), device=cpu, dtype=torch.float32, domain=continuous),
        thdot: BoundedTensorSpec(
             shape=torch.Size([]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True), maximum=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)), device=cpu, dtype=torch.float32, domain=continuous),
        params: CompositeSpec(
            max_speed: UnboundedContinuousTensorSpec(
                 shape=torch.Size([]), space=None, device=cpu, dtype=torch.int64, domain=continuous),
            max_torque: UnboundedContinuousTensorSpec(
                 shape=torch.Size([]), space=None, device=cpu, dtype=torch.float32, domain=continuous),
            dt: UnboundedContinuousTensorSpec(
                 shape=torch.Size([]), space=None, device=cpu, dtype=torch.float32, domain=continuous),
            g: UnboundedContinuousTensorSpec(
                 shape=torch.Size([]), space=None, device=cpu, dtype=torch.float32, domain=continuous),
            m: UnboundedContinuousTensorSpec(
                 shape=torch.Size([]), space=None, device=cpu, dtype=torch.float32, domain=continuous),
            l: UnboundedContinuousTensorSpec(
                 shape=torch.Size([]), space=None, device=cpu, dtype=torch.float32, domain=continuous), device=cpu, shape=torch.Size([])), device=cpu, shape=torch.Size([]))
    input_spec: CompositeSpec(
        sin: BoundedTensorSpec(
             shape=torch.Size([]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True), maximum=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)), device=cpu, dtype=torch.float32, domain=continuous),
        cos: BoundedTensorSpec(
             shape=torch.Size([]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True), maximum=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)), device=cpu, dtype=torch.float32, domain=continuous),
        th: BoundedTensorSpec(
             shape=torch.Size([]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True), maximum=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)), device=cpu, dtype=torch.float32, domain=continuous),
        thdot: BoundedTensorSpec(
             shape=torch.Size([]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True), maximum=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)), device=cpu, dtype=torch.float32, domain=continuous),
        params: CompositeSpec(
            max_speed: UnboundedContinuousTensorSpec(
                 shape=torch.Size([]), space=None, device=cpu, dtype=torch.int64, domain=continuous),
            max_torque: UnboundedContinuousTensorSpec(
                 shape=torch.Size([]), space=None, device=cpu, dtype=torch.float32, domain=continuous),
            dt: UnboundedContinuousTensorSpec(
                 shape=torch.Size([]), space=None, device=cpu, dtype=torch.float32, domain=continuous),
            g: UnboundedContinuousTensorSpec(
                 shape=torch.Size([]), space=None, device=cpu, dtype=torch.float32, domain=continuous),
            m: UnboundedContinuousTensorSpec(
                 shape=torch.Size([]), space=None, device=cpu, dtype=torch.float32, domain=continuous),
            l: UnboundedContinuousTensorSpec(
                 shape=torch.Size([]), space=None, device=cpu, dtype=torch.float32, domain=continuous), device=cpu, shape=torch.Size([])),
        action: BoundedTensorSpec(
             shape=torch.Size([1]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True), maximum=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True)), device=cpu, dtype=torch.float32, domain=continuous), device=cpu, shape=torch.Size([]))
    reward_spec: UnboundedContinuousTensorSpec(
         shape=torch.Size([1]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True), maximum=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)), device=cpu, dtype=torch.float32, domain=continuous)




.. GENERATED FROM PYTHON SOURCE LINES 462-466

We can execute a couple of commands too to check that the output structure
matches what is expected. We can run the :func:`env.rand_step` to generate
an action randomly from the ``action_spec`` domain:


.. GENERATED FROM PYTHON SOURCE LINES 466-472

.. code-block:: default


    td = env.reset()
    print("reset tensordict", td)
    td = env.rand_step(td)
    print("random step tensordict", td)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    reset tensordict TensorDict(
        fields={
            cos: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
            done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),
            params: TensorDict(
                fields={
                    dt: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
                    g: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
                    l: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
                    m: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
                    max_speed: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),
                    max_torque: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
                batch_size=torch.Size([]),
                device=cpu,
                is_shared=False),
            sin: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
            th: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
            thdot: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
        batch_size=torch.Size([]),
        device=cpu,
        is_shared=False)
    random step tensordict TensorDict(
        fields={
            action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),
            cos: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
            done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),
            next: TensorDict(
                fields={
                    cos: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
                    params: TensorDict(
                        fields={
                            dt: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
                            g: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
                            l: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
                            m: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
                            max_speed: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),
                            max_torque: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
                        batch_size=torch.Size([]),
                        device=cpu,
                        is_shared=False),
                    sin: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
                    th: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
                    thdot: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
                batch_size=torch.Size([]),
                device=cpu,
                is_shared=False),
            params: TensorDict(
                fields={
                    dt: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
                    g: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
                    l: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
                    m: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
                    max_speed: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),
                    max_torque: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
                batch_size=torch.Size([]),
                device=cpu,
                is_shared=False),
            reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),
            sin: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
            th: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
            thdot: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
        batch_size=torch.Size([]),
        device=cpu,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 473-485

Transforming an environment
---------------------------

Writing environment transforms for stateless simulators is slightly more
complicated than for stateful ones: transforming an output entry that needs
to be read at the following iteration requires to apply the inverse transform
before calling :func:`env.step` at the next step.
For instance, in the following transformed environment we unsqueeze the entries
``["sin", "cos", "thdot"]`` to be able to stack them along the last
dimension. We also pass them as ``in_keys_inv`` to squeeze them back to their
original shape once they are passed as input in the next iteration.


.. GENERATED FROM PYTHON SOURCE LINES 485-503

.. code-block:: default

    env = TransformedEnv(
        env,
        Compose(
            # Unsqueezes the observations that we will concatenate
            UnsqueezeTransform(
                unsqueeze_dim=-1,
                in_keys=["sin", "cos", "thdot"],
                in_keys_inv=["sin", "cos", "thdot"],
            ),
            # Concatenates the observations onto an "observation" entry.
            # del_keys=False ensures that we keep these values for the next
            # iteration.
            CatTensors(
                in_keys=["sin", "cos", "thdot"], out_key="observation", del_keys=False
            ),
        ),
    )








.. GENERATED FROM PYTHON SOURCE LINES 504-521

Executing a rollout
-------------------

Executing a rollout is a succession of simple steps:

* reset the environment
* while some condition is not met:

  * compute an action given a policy
  * execute a step given this action
  * collect the data
  * make a MDP step

* gather the data and return

These operations have been convinently wrapped in the :func:`EnvBase.rollout`
method, from which we provide a simplified version here below.

.. GENERATED FROM PYTHON SOURCE LINES 521-538

.. code-block:: default



    def simple_rollout(steps=100):
        # preallocate:
        data = TensorDict({}, [steps])
        # reset
        _data = env.reset()
        for i in range(steps):
            _data["action"] = env.action_spec.rand()
            _data = env.step(_data)
            data[i] = _data
            _data = step_mdp(_data, keep_other=True)
        return data


    print("data from rollout:", simple_rollout(100))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    data from rollout: TensorDict(
        fields={
            action: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False),
            cos: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False),
            done: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.bool, is_shared=False),
            next: TensorDict(
                fields={
                    cos: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False),
                    observation: Tensor(shape=torch.Size([100, 3]), device=cpu, dtype=torch.float32, is_shared=False),
                    params: TensorDict(
                        fields={
                            dt: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),
                            g: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),
                            l: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),
                            m: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),
                            max_speed: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.int64, is_shared=False),
                            max_torque: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False)},
                        batch_size=torch.Size([100]),
                        device=None,
                        is_shared=False),
                    sin: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False),
                    th: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),
                    thdot: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False)},
                batch_size=torch.Size([100]),
                device=None,
                is_shared=False),
            observation: Tensor(shape=torch.Size([100, 3]), device=cpu, dtype=torch.float32, is_shared=False),
            params: TensorDict(
                fields={
                    dt: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),
                    g: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),
                    l: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),
                    m: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),
                    max_speed: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.int64, is_shared=False),
                    max_torque: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False)},
                batch_size=torch.Size([100]),
                device=None,
                is_shared=False),
            reward: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False),
            sin: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False),
            th: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),
            thdot: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False)},
        batch_size=torch.Size([100]),
        device=None,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 539-548

Batching computations
---------------------

The last unexplored end of our tutorial is the ability that we have to
batch computations in TorchRL. Because our environment does not
make any assumptions regarding the input data shape, we can seamlessly
execute it over batches of data. To do this, we just generate parameters
with the desired shape.


.. GENERATED FROM PYTHON SOURCE LINES 548-568

.. code-block:: default


    batch_size = 10  # number of environments to be executed in batch
    td = env.reset(env.gen_params(batch_size=[batch_size]))
    print("reset (batch size of 10)", td)
    td = env.rand_step(td)
    print("rand step (batch size of 10)", td)

    # executing a rollout with a batch of data requires us to reset the env
    # out of the rollout function, since we need to define the batch_size
    # dynamically and this is not supported by :func:`EnvBase.rollout`:
    #

    rollout = env.rollout(
        3,
        auto_reset=False,
        tensordict=env.reset(env.gen_params(batch_size=[batch_size])),
    )
    print("rollout of len 3 (batch size of 10):", rollout)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    reset (batch size of 10) TensorDict(
        fields={
            cos: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),
            done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),
            observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),
            params: TensorDict(
                fields={
                    dt: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),
                    g: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),
                    l: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),
                    m: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),
                    max_speed: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False),
                    max_torque: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False)},
                batch_size=torch.Size([10]),
                device=None,
                is_shared=False),
            sin: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),
            th: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),
            thdot: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False)},
        batch_size=torch.Size([10]),
        device=None,
        is_shared=False)
    rand step (batch size of 10) TensorDict(
        fields={
            action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),
            cos: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),
            done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),
            next: TensorDict(
                fields={
                    cos: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),
                    observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),
                    params: TensorDict(
                        fields={
                            dt: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),
                            g: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),
                            l: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),
                            m: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),
                            max_speed: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False),
                            max_torque: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False)},
                        batch_size=torch.Size([10]),
                        device=None,
                        is_shared=False),
                    sin: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),
                    th: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),
                    thdot: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False)},
                batch_size=torch.Size([10]),
                device=None,
                is_shared=False),
            observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),
            params: TensorDict(
                fields={
                    dt: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),
                    g: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),
                    l: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),
                    m: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),
                    max_speed: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False),
                    max_torque: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False)},
                batch_size=torch.Size([10]),
                device=None,
                is_shared=False),
            reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),
            sin: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),
            th: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),
            thdot: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False)},
        batch_size=torch.Size([10]),
        device=None,
        is_shared=False)
    rollout of len 3 (batch size of 10): TensorDict(
        fields={
            action: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),
            cos: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),
            done: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.bool, is_shared=False),
            next: TensorDict(
                fields={
                    cos: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),
                    observation: Tensor(shape=torch.Size([10, 3, 3]), device=cpu, dtype=torch.float32, is_shared=False),
                    params: TensorDict(
                        fields={
                            dt: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),
                            g: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),
                            l: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),
                            m: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),
                            max_speed: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.int64, is_shared=False),
                            max_torque: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False)},
                        batch_size=torch.Size([10, 3]),
                        device=None,
                        is_shared=False),
                    sin: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),
                    th: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),
                    thdot: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False)},
                batch_size=torch.Size([10, 3]),
                device=None,
                is_shared=False),
            observation: Tensor(shape=torch.Size([10, 3, 3]), device=cpu, dtype=torch.float32, is_shared=False),
            params: TensorDict(
                fields={
                    dt: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),
                    g: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),
                    l: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),
                    m: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),
                    max_speed: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.int64, is_shared=False),
                    max_torque: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False)},
                batch_size=torch.Size([10, 3]),
                device=None,
                is_shared=False),
            reward: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),
            sin: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),
            th: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),
            thdot: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False)},
        batch_size=torch.Size([10, 3]),
        device=None,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 569-585

Training a simple policy
------------------------

In this example, we will train a simple policy using the reward as a
differentiable objective (i.e. a negative loss).
We will take advantage of the fact that our dynamic system is fully
differentiable to backpropagate through the trajectory return and adjust the
weights of our policy to maximise this value directly. Of course, in many
settings many of the assumptions we make do not hold, such as
differentiability of the system and full access to the underlying mechanics.

Still, this is a very simple example that showcases how a training loop can
be coded with a custom environment in TorchRL.

Let us first write the policy network:


.. GENERATED FROM PYTHON SOURCE LINES 585-603

.. code-block:: default

    torch.manual_seed(0)
    env.set_seed(0)

    net = nn.Sequential(
        nn.LazyLinear(64),
        nn.Tanh(),
        nn.LazyLinear(64),
        nn.Tanh(),
        nn.LazyLinear(64),
        nn.Tanh(),
        nn.LazyLinear(1),
    )
    policy = TensorDictModule(
        net,
        in_keys=["observation"],
        out_keys=["action"],
    )








.. GENERATED FROM PYTHON SOURCE LINES 604-606

and our optimizer:


.. GENERATED FROM PYTHON SOURCE LINES 606-609

.. code-block:: default


    optim = torch.optim.Adam(policy.parameters(), lr=2e-3)








.. GENERATED FROM PYTHON SOURCE LINES 610-612

Finally, let us re-create our environment:


.. GENERATED FROM PYTHON SOURCE LINES 612-627

.. code-block:: default


    env = TransformedEnv(
        PendulumEnv(),
        Compose(
            UnsqueezeTransform(
                unsqueeze_dim=-1,
                in_keys=["sin", "cos", "thdot"],
                in_keys_inv=["sin", "cos", "thdot"],
            ),
            CatTensors(
                in_keys=["sin", "cos", "thdot"], out_key="observation", del_keys=False
            ),
        ),
    )








.. GENERATED FROM PYTHON SOURCE LINES 628-642

Training loop
~~~~~~~~~~~~~

We will successively:

* generate a trajectory
* sum the rewards
* backpropagate through the graph defined by these operations
* clip the gradient norm and make an optimization step
* repeat

At the end of the training loop, we should have a final reward close to 0
which demonstrates that the pendulum is upward and still as desired.


.. GENERATED FROM PYTHON SOURCE LINES 642-690

.. code-block:: default

    batch_size = 32
    pbar = tqdm.tqdm(range(20_000 // batch_size))
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, 20_000)
    logs = defaultdict(list)

    for _ in pbar:
        init_td = env.reset(env.gen_params(batch_size=[batch_size]))
        rollout = env.rollout(100, policy, tensordict=init_td, auto_reset=False)
        traj_return = rollout["reward"].mean()
        (-traj_return).backward()
        gn = torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)
        optim.step()
        optim.zero_grad()
        pbar.set_description(
            f"reward: {traj_return: 4.4f}, "
            f"last reward: {rollout[..., -1]['reward'].mean(): 4.4f}, gradient norm: {gn: 4.4}"
        )
        logs["return"].append(traj_return.item())
        logs["last_reward"].append(rollout[..., -1]["reward"].mean().item())
        scheduler.step()


    def plot():
        import matplotlib
        from matplotlib import pyplot as plt

        is_ipython = "inline" in matplotlib.get_backend()
        if is_ipython:
            from IPython import display

        with plt.ion():
            plt.figure(figsize=(10, 5))
            plt.subplot(1, 2, 1)
            plt.plot(logs["return"])
            plt.title("returns")
            plt.xlabel("iteration")
            plt.subplot(1, 2, 2)
            plt.plot(logs["last_reward"])
            plt.title("last reward")
            plt.xlabel("iteration")
            if is_ipython:
                display.display(plt.gcf())
                display.clear_output(wait=True)
            plt.show()


    plot()




.. image-sg:: /tutorials/images/sphx_glr_pendulum_001.png
   :alt: returns, last reward
   :srcset: /tutorials/images/sphx_glr_pendulum_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/625 [00:00<?, ?it/s]    reward: -5.9266, last reward: -5.8855, gradient norm:  15.47:   0%|          | 0/625 [00:00<?, ?it/s]    reward: -5.9266, last reward: -5.8855, gradient norm:  15.47:   0%|          | 1/625 [00:00<02:21,  4.42it/s]    reward: -6.1505, last reward: -4.6494, gradient norm:  8.425:   0%|          | 1/625 [00:00<02:21,  4.42it/s]    reward: -6.1505, last reward: -4.6494, gradient norm:  8.425:   0%|          | 2/625 [00:00<02:17,  4.52it/s]    reward: -6.4027, last reward: -4.1463, gradient norm:  0.783:   0%|          | 2/625 [00:00<02:17,  4.52it/s]    reward: -6.4027, last reward: -4.1463, gradient norm:  0.783:   0%|          | 3/625 [00:00<02:16,  4.56it/s]    reward: -5.8107, last reward: -3.6160, gradient norm:  11.52:   0%|          | 3/625 [00:00<02:16,  4.56it/s]    reward: -5.8107, last reward: -3.6160, gradient norm:  11.52:   1%|          | 4/625 [00:00<02:15,  4.57it/s]    reward: -5.9038, last reward: -4.3170, gradient norm:  1.538:   1%|          | 4/625 [00:01<02:15,  4.57it/s]    reward: -5.9038, last reward: -4.3170, gradient norm:  1.538:   1%|          | 5/625 [00:01<02:16,  4.55it/s]    reward: -5.4996, last reward: -6.2572, gradient norm:  38.47:   1%|          | 5/625 [00:01<02:16,  4.55it/s]    reward: -5.4996, last reward: -6.2572, gradient norm:  38.47:   1%|          | 6/625 [00:01<02:17,  4.49it/s]    reward: -6.4428, last reward: -6.1989, gradient norm:  1.322:   1%|          | 6/625 [00:01<02:17,  4.49it/s]    reward: -6.4428, last reward: -6.1989, gradient norm:  1.322:   1%|1         | 7/625 [00:01<02:21,  4.36it/s]    reward: -6.0616, last reward: -5.4965, gradient norm:  9.44:   1%|1         | 7/625 [00:01<02:21,  4.36it/s]     reward: -6.0616, last reward: -5.4965, gradient norm:  9.44:   1%|1         | 8/625 [00:01<02:23,  4.30it/s]    reward: -5.6983, last reward: -6.4596, gradient norm:  2.562:   1%|1         | 8/625 [00:02<02:23,  4.30it/s]    reward: -5.6983, last reward: -6.4596, gradient norm:  2.562:   1%|1         | 9/625 [00:02<02:24,  4.26it/s]    reward: -6.5580, last reward: -6.3342, gradient norm:  65.08:   1%|1         | 9/625 [00:02<02:24,  4.26it/s]    reward: -6.5580, last reward: -6.3342, gradient norm:  65.08:   2%|1         | 10/625 [00:02<02:25,  4.22it/s]    reward: -6.1839, last reward: -6.1143, gradient norm:  19.24:   2%|1         | 10/625 [00:02<02:25,  4.22it/s]    reward: -6.1839, last reward: -6.1143, gradient norm:  19.24:   2%|1         | 11/625 [00:02<02:26,  4.20it/s]    reward: -5.7682, last reward: -5.7979, gradient norm:  11.19:   2%|1         | 11/625 [00:02<02:26,  4.20it/s]    reward: -5.7682, last reward: -5.7979, gradient norm:  11.19:   2%|1         | 12/625 [00:02<02:26,  4.19it/s]    reward: -6.0317, last reward: -5.4728, gradient norm:  9.218:   2%|1         | 12/625 [00:03<02:26,  4.19it/s]    reward: -6.0317, last reward: -5.4728, gradient norm:  9.218:   2%|2         | 13/625 [00:03<02:26,  4.18it/s]    reward: -6.1692, last reward: -6.9600, gradient norm:  7.129:   2%|2         | 13/625 [00:03<02:26,  4.18it/s]    reward: -6.1692, last reward: -6.9600, gradient norm:  7.129:   2%|2         | 14/625 [00:03<02:26,  4.17it/s]    reward: -6.0109, last reward: -7.0531, gradient norm:  1.781:   2%|2         | 14/625 [00:03<02:26,  4.17it/s]    reward: -6.0109, last reward: -7.0531, gradient norm:  1.781:   2%|2         | 15/625 [00:03<02:26,  4.16it/s]    reward: -5.9979, last reward: -5.9649, gradient norm:  20.18:   2%|2         | 15/625 [00:03<02:26,  4.16it/s]    reward: -5.9979, last reward: -5.9649, gradient norm:  20.18:   3%|2         | 16/625 [00:03<02:26,  4.15it/s]    reward: -5.8525, last reward: -4.9764, gradient norm:  3.109:   3%|2         | 16/625 [00:03<02:26,  4.15it/s]    reward: -5.8525, last reward: -4.9764, gradient norm:  3.109:   3%|2         | 17/625 [00:03<02:26,  4.15it/s]    reward: -5.5275, last reward: -5.1344, gradient norm:  3.148:   3%|2         | 17/625 [00:04<02:26,  4.15it/s]    reward: -5.5275, last reward: -5.1344, gradient norm:  3.148:   3%|2         | 18/625 [00:04<02:26,  4.15it/s]    reward: -5.9773, last reward: -5.5141, gradient norm:  20.54:   3%|2         | 18/625 [00:04<02:26,  4.15it/s]    reward: -5.9773, last reward: -5.5141, gradient norm:  20.54:   3%|3         | 19/625 [00:04<02:26,  4.15it/s]    reward: -5.8143, last reward: -6.1775, gradient norm:  25.25:   3%|3         | 19/625 [00:04<02:26,  4.15it/s]    reward: -5.8143, last reward: -6.1775, gradient norm:  25.25:   3%|3         | 20/625 [00:04<02:25,  4.15it/s]    reward: -6.2208, last reward: -6.6846, gradient norm:  6.21:   3%|3         | 20/625 [00:04<02:25,  4.15it/s]     reward: -6.2208, last reward: -6.6846, gradient norm:  6.21:   3%|3         | 21/625 [00:04<02:25,  4.14it/s]    reward: -5.8642, last reward: -7.1940, gradient norm:  9.542:   3%|3         | 21/625 [00:05<02:25,  4.14it/s]    reward: -5.8642, last reward: -7.1940, gradient norm:  9.542:   4%|3         | 22/625 [00:05<02:25,  4.14it/s]    reward: -5.6518, last reward: -6.5410, gradient norm:  33.68:   4%|3         | 22/625 [00:05<02:25,  4.14it/s]    reward: -5.6518, last reward: -6.5410, gradient norm:  33.68:   4%|3         | 23/625 [00:05<02:25,  4.14it/s]    reward: -6.2140, last reward: -6.1722, gradient norm:  6.921:   4%|3         | 23/625 [00:05<02:25,  4.14it/s]    reward: -6.2140, last reward: -6.1722, gradient norm:  6.921:   4%|3         | 24/625 [00:05<02:22,  4.23it/s]    reward: -6.2054, last reward: -5.5124, gradient norm:  5.424:   4%|3         | 24/625 [00:05<02:22,  4.23it/s]    reward: -6.2054, last reward: -5.5124, gradient norm:  5.424:   4%|4         | 25/625 [00:05<02:18,  4.34it/s]    reward: -5.4949, last reward: -3.6917, gradient norm:  14.75:   4%|4         | 25/625 [00:06<02:18,  4.34it/s]    reward: -5.4949, last reward: -3.6917, gradient norm:  14.75:   4%|4         | 26/625 [00:06<02:15,  4.43it/s]    reward: -5.6888, last reward: -7.9270, gradient norm:  7.772:   4%|4         | 26/625 [00:06<02:15,  4.43it/s]    reward: -5.6888, last reward: -7.9270, gradient norm:  7.772:   4%|4         | 27/625 [00:06<02:13,  4.49it/s]    reward: -5.8562, last reward: -7.1902, gradient norm:  10.66:   4%|4         | 27/625 [00:06<02:13,  4.49it/s]    reward: -5.8562, last reward: -7.1902, gradient norm:  10.66:   4%|4         | 28/625 [00:06<02:11,  4.53it/s]    reward: -5.8441, last reward: -5.1587, gradient norm:  14.63:   4%|4         | 28/625 [00:06<02:11,  4.53it/s]    reward: -5.8441, last reward: -5.1587, gradient norm:  14.63:   5%|4         | 29/625 [00:06<02:10,  4.57it/s]    reward: -5.7674, last reward: -6.0118, gradient norm:  55.11:   5%|4         | 29/625 [00:06<02:10,  4.57it/s]    reward: -5.7674, last reward: -6.0118, gradient norm:  55.11:   5%|4         | 30/625 [00:06<02:10,  4.56it/s]    reward: -6.2092, last reward: -5.1204, gradient norm:  1.067:   5%|4         | 30/625 [00:07<02:10,  4.56it/s]    reward: -6.2092, last reward: -5.1204, gradient norm:  1.067:   5%|4         | 31/625 [00:07<02:10,  4.57it/s]    reward: -6.1962, last reward: -5.9235, gradient norm:  27.71:   5%|4         | 31/625 [00:07<02:10,  4.57it/s]    reward: -6.1962, last reward: -5.9235, gradient norm:  27.71:   5%|5         | 32/625 [00:07<02:09,  4.58it/s]    reward: -6.2727, last reward: -6.8757, gradient norm:  8.8:   5%|5         | 32/625 [00:07<02:09,  4.58it/s]      reward: -6.2727, last reward: -6.8757, gradient norm:  8.8:   5%|5         | 33/625 [00:07<02:08,  4.59it/s]    reward: -6.4235, last reward: -6.5884, gradient norm:  1.779:   5%|5         | 33/625 [00:07<02:08,  4.59it/s]    reward: -6.4235, last reward: -6.5884, gradient norm:  1.779:   5%|5         | 34/625 [00:07<02:08,  4.60it/s]    reward: -6.4090, last reward: -6.7606, gradient norm:  17.54:   5%|5         | 34/625 [00:08<02:08,  4.60it/s]    reward: -6.4090, last reward: -6.7606, gradient norm:  17.54:   6%|5         | 35/625 [00:08<02:07,  4.61it/s]    reward: -6.2187, last reward: -5.6999, gradient norm:  130.8:   6%|5         | 35/625 [00:08<02:07,  4.61it/s]    reward: -6.2187, last reward: -5.6999, gradient norm:  130.8:   6%|5         | 36/625 [00:08<02:07,  4.62it/s]    reward: -6.2978, last reward: -6.4193, gradient norm:  9.815:   6%|5         | 36/625 [00:08<02:07,  4.62it/s]    reward: -6.2978, last reward: -6.4193, gradient norm:  9.815:   6%|5         | 37/625 [00:08<02:07,  4.62it/s]    reward: -6.1240, last reward: -6.7324, gradient norm:  19.53:   6%|5         | 37/625 [00:08<02:07,  4.62it/s]    reward: -6.1240, last reward: -6.7324, gradient norm:  19.53:   6%|6         | 38/625 [00:08<02:30,  3.89it/s]    reward: -6.1505, last reward: -6.0441, gradient norm:  10.84:   6%|6         | 38/625 [00:09<02:30,  3.89it/s]    reward: -6.1505, last reward: -6.0441, gradient norm:  10.84:   6%|6         | 39/625 [00:09<02:23,  4.09it/s]    reward: -6.0848, last reward: -7.2587, gradient norm:  1.319:   6%|6         | 39/625 [00:09<02:23,  4.09it/s]    reward: -6.0848, last reward: -7.2587, gradient norm:  1.319:   6%|6         | 40/625 [00:09<02:17,  4.24it/s]    reward: -6.0140, last reward: -7.5505, gradient norm:  5.367:   6%|6         | 40/625 [00:09<02:17,  4.24it/s]    reward: -6.0140, last reward: -7.5505, gradient norm:  5.367:   7%|6         | 41/625 [00:09<02:14,  4.35it/s]    reward: -5.6425, last reward: -6.2510, gradient norm:  12.1:   7%|6         | 41/625 [00:09<02:14,  4.35it/s]     reward: -5.6425, last reward: -6.2510, gradient norm:  12.1:   7%|6         | 42/625 [00:09<02:11,  4.43it/s]    reward: -5.7728, last reward: -5.0743, gradient norm:  9.255:   7%|6         | 42/625 [00:09<02:11,  4.43it/s]    reward: -5.7728, last reward: -5.0743, gradient norm:  9.255:   7%|6         | 43/625 [00:09<02:09,  4.49it/s]    reward: -6.1644, last reward: -7.6137, gradient norm:  12.43:   7%|6         | 43/625 [00:10<02:09,  4.49it/s]    reward: -6.1644, last reward: -7.6137, gradient norm:  12.43:   7%|7         | 44/625 [00:10<02:08,  4.53it/s]    reward: -5.8888, last reward: -6.5967, gradient norm:  7.583:   7%|7         | 44/625 [00:10<02:08,  4.53it/s]    reward: -5.8888, last reward: -6.5967, gradient norm:  7.583:   7%|7         | 45/625 [00:10<02:07,  4.56it/s]    reward: -5.8059, last reward: -6.7800, gradient norm:  13.75:   7%|7         | 45/625 [00:10<02:07,  4.56it/s]    reward: -5.8059, last reward: -6.7800, gradient norm:  13.75:   7%|7         | 46/625 [00:10<02:06,  4.58it/s]    reward: -6.0819, last reward: -6.2592, gradient norm:  3.691:   7%|7         | 46/625 [00:10<02:06,  4.58it/s]    reward: -6.0819, last reward: -6.2592, gradient norm:  3.691:   8%|7         | 47/625 [00:10<02:05,  4.60it/s]    reward: -6.0502, last reward: -7.2924, gradient norm:  3.054:   8%|7         | 47/625 [00:10<02:05,  4.60it/s]    reward: -6.0502, last reward: -7.2924, gradient norm:  3.054:   8%|7         | 48/625 [00:10<02:05,  4.61it/s]    reward: -5.8846, last reward: -5.1838, gradient norm:  43.77:   8%|7         | 48/625 [00:11<02:05,  4.61it/s]    reward: -5.8846, last reward: -5.1838, gradient norm:  43.77:   8%|7         | 49/625 [00:11<02:05,  4.57it/s]    reward: -5.5595, last reward: -6.1471, gradient norm:  12.59:   8%|7         | 49/625 [00:11<02:05,  4.57it/s]    reward: -5.5595, last reward: -6.1471, gradient norm:  12.59:   8%|8         | 50/625 [00:11<02:05,  4.59it/s]    reward: -5.7703, last reward: -5.6139, gradient norm:  2.219:   8%|8         | 50/625 [00:11<02:05,  4.59it/s]    reward: -5.7703, last reward: -5.6139, gradient norm:  2.219:   8%|8         | 51/625 [00:11<02:05,  4.59it/s]    reward: -5.1751, last reward: -4.6989, gradient norm:  8.118:   8%|8         | 51/625 [00:11<02:05,  4.59it/s]    reward: -5.1751, last reward: -4.6989, gradient norm:  8.118:   8%|8         | 52/625 [00:11<02:04,  4.60it/s]    reward: -5.4951, last reward: -5.5348, gradient norm:  4.398:   8%|8         | 52/625 [00:12<02:04,  4.60it/s]    reward: -5.4951, last reward: -5.5348, gradient norm:  4.398:   8%|8         | 53/625 [00:12<02:03,  4.62it/s]    reward: -5.9085, last reward: -6.3170, gradient norm:  2.468:   8%|8         | 53/625 [00:12<02:03,  4.62it/s]    reward: -5.9085, last reward: -6.3170, gradient norm:  2.468:   9%|8         | 54/625 [00:12<02:03,  4.62it/s]    reward: -5.5493, last reward: -6.6267, gradient norm:  27.44:   9%|8         | 54/625 [00:12<02:03,  4.62it/s]    reward: -5.5493, last reward: -6.6267, gradient norm:  27.44:   9%|8         | 55/625 [00:12<02:03,  4.63it/s]    reward: -5.6030, last reward: -6.4769, gradient norm:  7.569:   9%|8         | 55/625 [00:12<02:03,  4.63it/s]    reward: -5.6030, last reward: -6.4769, gradient norm:  7.569:   9%|8         | 56/625 [00:12<02:03,  4.62it/s]    reward: -5.6288, last reward: -7.2236, gradient norm:  8.619:   9%|8         | 56/625 [00:12<02:03,  4.62it/s]    reward: -5.6288, last reward: -7.2236, gradient norm:  8.619:   9%|9         | 57/625 [00:12<02:03,  4.62it/s]    reward: -5.5154, last reward: -6.9968, gradient norm:  20.38:   9%|9         | 57/625 [00:13<02:03,  4.62it/s]    reward: -5.5154, last reward: -6.9968, gradient norm:  20.38:   9%|9         | 58/625 [00:13<02:02,  4.62it/s]    reward: -5.6179, last reward: -7.7323, gradient norm:  5.42:   9%|9         | 58/625 [00:13<02:02,  4.62it/s]     reward: -5.6179, last reward: -7.7323, gradient norm:  5.42:   9%|9         | 59/625 [00:13<02:02,  4.62it/s]    reward: -5.2981, last reward: -6.3913, gradient norm:  5.034:   9%|9         | 59/625 [00:13<02:02,  4.62it/s]    reward: -5.2981, last reward: -6.3913, gradient norm:  5.034:  10%|9         | 60/625 [00:13<02:02,  4.62it/s]    reward: -5.5014, last reward: -6.3918, gradient norm:  5.44:  10%|9         | 60/625 [00:13<02:02,  4.62it/s]     reward: -5.5014, last reward: -6.3918, gradient norm:  5.44:  10%|9         | 61/625 [00:13<02:02,  4.62it/s]    reward: -5.2370, last reward: -6.3614, gradient norm:  8.781:  10%|9         | 61/625 [00:14<02:02,  4.62it/s]    reward: -5.2370, last reward: -6.3614, gradient norm:  8.781:  10%|9         | 62/625 [00:14<02:01,  4.62it/s]    reward: -5.7929, last reward: -5.8821, gradient norm:  20.07:  10%|9         | 62/625 [00:14<02:01,  4.62it/s]    reward: -5.7929, last reward: -5.8821, gradient norm:  20.07:  10%|#         | 63/625 [00:14<02:01,  4.61it/s]    reward: -5.3610, last reward: -4.1901, gradient norm:  6.915:  10%|#         | 63/625 [00:14<02:01,  4.61it/s]    reward: -5.3610, last reward: -4.1901, gradient norm:  6.915:  10%|#         | 64/625 [00:14<02:01,  4.62it/s]    reward: -5.2983, last reward: -4.7640, gradient norm:  18.63:  10%|#         | 64/625 [00:14<02:01,  4.62it/s]    reward: -5.2983, last reward: -4.7640, gradient norm:  18.63:  10%|#         | 65/625 [00:14<02:01,  4.61it/s]    reward: -5.5792, last reward: -5.3962, gradient norm:  9.075:  10%|#         | 65/625 [00:14<02:01,  4.61it/s]    reward: -5.5792, last reward: -5.3962, gradient norm:  9.075:  11%|#         | 66/625 [00:14<02:01,  4.61it/s]    reward: -6.3180, last reward: -6.1949, gradient norm:  5.502:  11%|#         | 66/625 [00:15<02:01,  4.61it/s]    reward: -6.3180, last reward: -6.1949, gradient norm:  5.502:  11%|#         | 67/625 [00:15<02:00,  4.61it/s]    reward: -6.1380, last reward: -7.5574, gradient norm:  9.253:  11%|#         | 67/625 [00:15<02:00,  4.61it/s]    reward: -6.1380, last reward: -7.5574, gradient norm:  9.253:  11%|#         | 68/625 [00:15<02:00,  4.61it/s]    reward: -5.8458, last reward: -5.7408, gradient norm:  8.937:  11%|#         | 68/625 [00:15<02:00,  4.61it/s]    reward: -5.8458, last reward: -5.7408, gradient norm:  8.937:  11%|#1        | 69/625 [00:15<02:00,  4.61it/s]    reward: -5.5840, last reward: -5.4463, gradient norm:  13.77:  11%|#1        | 69/625 [00:15<02:00,  4.61it/s]    reward: -5.5840, last reward: -5.4463, gradient norm:  13.77:  11%|#1        | 70/625 [00:15<02:00,  4.62it/s]    reward: -5.5086, last reward: -5.5344, gradient norm:  4.795:  11%|#1        | 70/625 [00:15<02:00,  4.62it/s]    reward: -5.5086, last reward: -5.5344, gradient norm:  4.795:  11%|#1        | 71/625 [00:15<01:59,  4.63it/s]    reward: -5.3694, last reward: -5.8463, gradient norm:  3.287:  11%|#1        | 71/625 [00:16<01:59,  4.63it/s]    reward: -5.3694, last reward: -5.8463, gradient norm:  3.287:  12%|#1        | 72/625 [00:16<01:59,  4.63it/s]    reward: -5.3028, last reward: -3.2446, gradient norm:  74.35:  12%|#1        | 72/625 [00:16<01:59,  4.63it/s]    reward: -5.3028, last reward: -3.2446, gradient norm:  74.35:  12%|#1        | 73/625 [00:16<01:59,  4.63it/s]    reward: -5.4761, last reward: -2.7600, gradient norm:  3.627:  12%|#1        | 73/625 [00:16<01:59,  4.63it/s]    reward: -5.4761, last reward: -2.7600, gradient norm:  3.627:  12%|#1        | 74/625 [00:16<01:58,  4.64it/s]    reward: -5.3842, last reward: -3.4801, gradient norm:  9.816:  12%|#1        | 74/625 [00:16<01:58,  4.64it/s]    reward: -5.3842, last reward: -3.4801, gradient norm:  9.816:  12%|#2        | 75/625 [00:16<01:58,  4.64it/s]    reward: -5.1771, last reward: -2.9593, gradient norm:  5.886:  12%|#2        | 75/625 [00:17<01:58,  4.64it/s]    reward: -5.1771, last reward: -2.9593, gradient norm:  5.886:  12%|#2        | 76/625 [00:17<01:58,  4.64it/s]    reward: -5.2737, last reward: -2.9358, gradient norm:  7.678:  12%|#2        | 76/625 [00:17<01:58,  4.64it/s]    reward: -5.2737, last reward: -2.9358, gradient norm:  7.678:  12%|#2        | 77/625 [00:17<01:58,  4.64it/s]    reward: -5.3900, last reward: -2.6653, gradient norm:  3.292:  12%|#2        | 77/625 [00:17<01:58,  4.64it/s]    reward: -5.3900, last reward: -2.6653, gradient norm:  3.292:  12%|#2        | 78/625 [00:17<01:57,  4.64it/s]    reward: -5.2661, last reward: -3.8185, gradient norm:  45.93:  12%|#2        | 78/625 [00:17<01:57,  4.64it/s]    reward: -5.2661, last reward: -3.8185, gradient norm:  45.93:  13%|#2        | 79/625 [00:17<01:57,  4.64it/s]    reward: -5.4661, last reward: -3.8268, gradient norm:  61.06:  13%|#2        | 79/625 [00:17<01:57,  4.64it/s]    reward: -5.4661, last reward: -3.8268, gradient norm:  61.06:  13%|#2        | 80/625 [00:17<01:57,  4.65it/s]    reward: -5.3443, last reward: -4.6326, gradient norm:  3.38:  13%|#2        | 80/625 [00:18<01:57,  4.65it/s]     reward: -5.3443, last reward: -4.6326, gradient norm:  3.38:  13%|#2        | 81/625 [00:18<01:57,  4.64it/s]    reward: -5.5140, last reward: -5.6207, gradient norm:  9.892:  13%|#2        | 81/625 [00:18<01:57,  4.64it/s]    reward: -5.5140, last reward: -5.6207, gradient norm:  9.892:  13%|#3        | 82/625 [00:18<01:56,  4.65it/s]    reward: -5.7227, last reward: -4.5777, gradient norm:  9.064:  13%|#3        | 82/625 [00:18<01:56,  4.65it/s]    reward: -5.7227, last reward: -4.5777, gradient norm:  9.064:  13%|#3        | 83/625 [00:18<01:56,  4.65it/s]    reward: -5.7155, last reward: -5.5219, gradient norm:  11.97:  13%|#3        | 83/625 [00:18<01:56,  4.65it/s]    reward: -5.7155, last reward: -5.5219, gradient norm:  11.97:  13%|#3        | 84/625 [00:18<01:56,  4.64it/s]    reward: -5.4743, last reward: -4.4839, gradient norm:  3.515:  13%|#3        | 84/625 [00:18<01:56,  4.64it/s]    reward: -5.4743, last reward: -4.4839, gradient norm:  3.515:  14%|#3        | 85/625 [00:18<01:56,  4.64it/s]    reward: -5.5471, last reward: -3.3322, gradient norm:  1.751:  14%|#3        | 85/625 [00:19<01:56,  4.64it/s]    reward: -5.5471, last reward: -3.3322, gradient norm:  1.751:  14%|#3        | 86/625 [00:19<01:56,  4.65it/s]    reward: -5.3552, last reward: -5.1040, gradient norm:  25.98:  14%|#3        | 86/625 [00:19<01:56,  4.65it/s]    reward: -5.3552, last reward: -5.1040, gradient norm:  25.98:  14%|#3        | 87/625 [00:19<01:55,  4.65it/s]    reward: -5.2691, last reward: -4.0538, gradient norm:  1.739:  14%|#3        | 87/625 [00:19<01:55,  4.65it/s]    reward: -5.2691, last reward: -4.0538, gradient norm:  1.739:  14%|#4        | 88/625 [00:19<01:55,  4.65it/s]    reward: -5.1406, last reward: -4.6926, gradient norm:  8.241:  14%|#4        | 88/625 [00:19<01:55,  4.65it/s]    reward: -5.1406, last reward: -4.6926, gradient norm:  8.241:  14%|#4        | 89/625 [00:19<01:55,  4.65it/s]    reward: -5.1092, last reward: -2.7926, gradient norm:  2.091:  14%|#4        | 89/625 [00:20<01:55,  4.65it/s]    reward: -5.1092, last reward: -2.7926, gradient norm:  2.091:  14%|#4        | 90/625 [00:20<01:55,  4.64it/s]    reward: -5.3232, last reward: -3.6219, gradient norm:  2.8:  14%|#4        | 90/625 [00:20<01:55,  4.64it/s]      reward: -5.3232, last reward: -3.6219, gradient norm:  2.8:  15%|#4        | 91/625 [00:20<01:55,  4.63it/s]    reward: -5.1943, last reward: -3.8801, gradient norm:  10.74:  15%|#4        | 91/625 [00:20<01:55,  4.63it/s]    reward: -5.1943, last reward: -3.8801, gradient norm:  10.74:  15%|#4        | 92/625 [00:20<01:55,  4.63it/s]    reward: -5.2436, last reward: -4.1146, gradient norm:  8.385:  15%|#4        | 92/625 [00:20<01:55,  4.63it/s]    reward: -5.2436, last reward: -4.1146, gradient norm:  8.385:  15%|#4        | 93/625 [00:20<01:54,  4.63it/s]    reward: -5.2468, last reward: -5.6970, gradient norm:  87.88:  15%|#4        | 93/625 [00:20<01:54,  4.63it/s]    reward: -5.2468, last reward: -5.6970, gradient norm:  87.88:  15%|#5        | 94/625 [00:20<01:54,  4.63it/s]    reward: -5.3437, last reward: -6.7919, gradient norm:  12.68:  15%|#5        | 94/625 [00:21<01:54,  4.63it/s]    reward: -5.3437, last reward: -6.7919, gradient norm:  12.68:  15%|#5        | 95/625 [00:21<01:55,  4.61it/s]    reward: -5.0362, last reward: -4.4871, gradient norm:  4.134:  15%|#5        | 95/625 [00:21<01:55,  4.61it/s]    reward: -5.0362, last reward: -4.4871, gradient norm:  4.134:  15%|#5        | 96/625 [00:21<01:54,  4.61it/s]    reward: -4.9510, last reward: -3.6218, gradient norm:  4.873:  15%|#5        | 96/625 [00:21<01:54,  4.61it/s]    reward: -4.9510, last reward: -3.6218, gradient norm:  4.873:  16%|#5        | 97/625 [00:21<01:54,  4.62it/s]    reward: -5.1963, last reward: -3.7772, gradient norm:  6.586:  16%|#5        | 97/625 [00:21<01:54,  4.62it/s]    reward: -5.1963, last reward: -3.7772, gradient norm:  6.586:  16%|#5        | 98/625 [00:21<01:53,  4.63it/s]    reward: -4.6380, last reward: -5.4336, gradient norm:  23.53:  16%|#5        | 98/625 [00:21<01:53,  4.63it/s]    reward: -4.6380, last reward: -5.4336, gradient norm:  23.53:  16%|#5        | 99/625 [00:21<01:53,  4.64it/s]    reward: -4.8875, last reward: -3.7949, gradient norm:  12.16:  16%|#5        | 99/625 [00:22<01:53,  4.64it/s]    reward: -4.8875, last reward: -3.7949, gradient norm:  12.16:  16%|#6        | 100/625 [00:22<01:53,  4.64it/s]    reward: -4.6233, last reward: -3.2998, gradient norm:  6.254:  16%|#6        | 100/625 [00:22<01:53,  4.64it/s]    reward: -4.6233, last reward: -3.2998, gradient norm:  6.254:  16%|#6        | 101/625 [00:22<01:52,  4.65it/s]    reward: -4.6591, last reward: -6.1574, gradient norm:  6.845:  16%|#6        | 101/625 [00:22<01:52,  4.65it/s]    reward: -4.6591, last reward: -6.1574, gradient norm:  6.845:  16%|#6        | 102/625 [00:22<01:52,  4.65it/s]    reward: -4.7564, last reward: -6.7148, gradient norm:  127.0:  16%|#6        | 102/625 [00:22<01:52,  4.65it/s]    reward: -4.7564, last reward: -6.7148, gradient norm:  127.0:  16%|#6        | 103/625 [00:22<01:52,  4.65it/s]    reward: -4.8592, last reward: -5.6204, gradient norm:  15.69:  16%|#6        | 103/625 [00:23<01:52,  4.65it/s]    reward: -4.8592, last reward: -5.6204, gradient norm:  15.69:  17%|#6        | 104/625 [00:23<01:52,  4.65it/s]    reward: -4.7275, last reward: -8.2646, gradient norm:  37.45:  17%|#6        | 104/625 [00:23<01:52,  4.65it/s]    reward: -4.7275, last reward: -8.2646, gradient norm:  37.45:  17%|#6        | 105/625 [00:23<01:51,  4.65it/s]    reward: -4.5618, last reward: -5.6144, gradient norm:  105.3:  17%|#6        | 105/625 [00:23<01:51,  4.65it/s]    reward: -4.5618, last reward: -5.6144, gradient norm:  105.3:  17%|#6        | 106/625 [00:23<01:51,  4.65it/s]    reward: -4.3800, last reward: -3.7663, gradient norm:  44.71:  17%|#6        | 106/625 [00:23<01:51,  4.65it/s]    reward: -4.3800, last reward: -3.7663, gradient norm:  44.71:  17%|#7        | 107/625 [00:23<01:51,  4.66it/s]    reward: -4.8524, last reward: -4.4591, gradient norm:  12.54:  17%|#7        | 107/625 [00:24<01:51,  4.66it/s]    reward: -4.8524, last reward: -4.4591, gradient norm:  12.54:  17%|#7        | 108/625 [00:24<02:11,  3.93it/s]    reward: -5.0510, last reward: -4.3500, gradient norm:  37.74:  17%|#7        | 108/625 [00:24<02:11,  3.93it/s]    reward: -5.0510, last reward: -4.3500, gradient norm:  37.74:  17%|#7        | 109/625 [00:24<02:05,  4.11it/s]    reward: -5.1634, last reward: -5.5899, gradient norm:  9.264:  17%|#7        | 109/625 [00:24<02:05,  4.11it/s]    reward: -5.1634, last reward: -5.5899, gradient norm:  9.264:  18%|#7        | 110/625 [00:24<02:00,  4.26it/s]    reward: -5.3976, last reward: -3.7117, gradient norm:  5.718:  18%|#7        | 110/625 [00:24<02:00,  4.26it/s]    reward: -5.3976, last reward: -3.7117, gradient norm:  5.718:  18%|#7        | 111/625 [00:24<01:57,  4.36it/s]    reward: -5.3677, last reward: -3.6074, gradient norm:  6.778:  18%|#7        | 111/625 [00:24<01:57,  4.36it/s]    reward: -5.3677, last reward: -3.6074, gradient norm:  6.778:  18%|#7        | 112/625 [00:24<01:55,  4.44it/s]    reward: -5.0525, last reward: -4.8793, gradient norm:  11.43:  18%|#7        | 112/625 [00:25<01:55,  4.44it/s]    reward: -5.0525, last reward: -4.8793, gradient norm:  11.43:  18%|#8        | 113/625 [00:25<01:53,  4.50it/s]    reward: -4.7407, last reward: -3.3517, gradient norm:  9.354:  18%|#8        | 113/625 [00:25<01:53,  4.50it/s]    reward: -4.7407, last reward: -3.3517, gradient norm:  9.354:  18%|#8        | 114/625 [00:25<01:52,  4.54it/s]    reward: -4.4357, last reward: -5.0914, gradient norm:  3.401:  18%|#8        | 114/625 [00:25<01:52,  4.54it/s]    reward: -4.4357, last reward: -5.0914, gradient norm:  3.401:  18%|#8        | 115/625 [00:25<01:51,  4.57it/s]    reward: -4.7790, last reward: -2.6851, gradient norm:  1.712:  18%|#8        | 115/625 [00:25<01:51,  4.57it/s]    reward: -4.7790, last reward: -2.6851, gradient norm:  1.712:  19%|#8        | 116/625 [00:25<01:50,  4.59it/s]    reward: -5.0181, last reward: -4.6984, gradient norm:  14.06:  19%|#8        | 116/625 [00:26<01:50,  4.59it/s]    reward: -5.0181, last reward: -4.6984, gradient norm:  14.06:  19%|#8        | 117/625 [00:26<01:50,  4.61it/s]    reward: -5.1435, last reward: -6.0245, gradient norm:  57.46:  19%|#8        | 117/625 [00:26<01:50,  4.61it/s]    reward: -5.1435, last reward: -6.0245, gradient norm:  57.46:  19%|#8        | 118/625 [00:26<01:49,  4.62it/s]    reward: -5.1855, last reward: -5.2904, gradient norm:  16.7:  19%|#8        | 118/625 [00:26<01:49,  4.62it/s]     reward: -5.1855, last reward: -5.2904, gradient norm:  16.7:  19%|#9        | 119/625 [00:26<01:49,  4.63it/s]    reward: -5.0618, last reward: -4.9499, gradient norm:  29.53:  19%|#9        | 119/625 [00:26<01:49,  4.63it/s]    reward: -5.0618, last reward: -4.9499, gradient norm:  29.53:  19%|#9        | 120/625 [00:26<01:49,  4.63it/s]    reward: -4.8561, last reward: -5.4175, gradient norm:  14.19:  19%|#9        | 120/625 [00:26<01:49,  4.63it/s]    reward: -4.8561, last reward: -5.4175, gradient norm:  14.19:  19%|#9        | 121/625 [00:26<01:48,  4.63it/s]    reward: -4.1193, last reward: -3.1237, gradient norm:  30.8:  19%|#9        | 121/625 [00:27<01:48,  4.63it/s]     reward: -4.1193, last reward: -3.1237, gradient norm:  30.8:  20%|#9        | 122/625 [00:27<01:48,  4.63it/s]    reward: -4.4464, last reward: -3.2677, gradient norm:  11.72:  20%|#9        | 122/625 [00:27<01:48,  4.63it/s]    reward: -4.4464, last reward: -3.2677, gradient norm:  11.72:  20%|#9        | 123/625 [00:27<01:48,  4.63it/s]    reward: -4.6946, last reward: -4.5547, gradient norm:  11.85:  20%|#9        | 123/625 [00:27<01:48,  4.63it/s]    reward: -4.6946, last reward: -4.5547, gradient norm:  11.85:  20%|#9        | 124/625 [00:27<01:48,  4.64it/s]    reward: -4.8347, last reward: -5.4730, gradient norm:  14.09:  20%|#9        | 124/625 [00:27<01:48,  4.64it/s]    reward: -4.8347, last reward: -5.4730, gradient norm:  14.09:  20%|##        | 125/625 [00:27<01:47,  4.64it/s]    reward: -4.8955, last reward: -4.4996, gradient norm:  11.37:  20%|##        | 125/625 [00:27<01:47,  4.64it/s]    reward: -4.8955, last reward: -4.4996, gradient norm:  11.37:  20%|##        | 126/625 [00:27<01:47,  4.64it/s]    reward: -4.4709, last reward: -3.8433, gradient norm:  11.91:  20%|##        | 126/625 [00:28<01:47,  4.64it/s]    reward: -4.4709, last reward: -3.8433, gradient norm:  11.91:  20%|##        | 127/625 [00:28<01:47,  4.64it/s]    reward: -4.3481, last reward: -3.8304, gradient norm:  16.16:  20%|##        | 127/625 [00:28<01:47,  4.64it/s]    reward: -4.3481, last reward: -3.8304, gradient norm:  16.16:  20%|##        | 128/625 [00:28<01:47,  4.64it/s]    reward: -4.1127, last reward: -2.7206, gradient norm:  21.51:  20%|##        | 128/625 [00:28<01:47,  4.64it/s]    reward: -4.1127, last reward: -2.7206, gradient norm:  21.51:  21%|##        | 129/625 [00:28<01:47,  4.63it/s]    reward: -4.3253, last reward: -5.1534, gradient norm:  118.5:  21%|##        | 129/625 [00:28<01:47,  4.63it/s]    reward: -4.3253, last reward: -5.1534, gradient norm:  118.5:  21%|##        | 130/625 [00:28<01:46,  4.63it/s]    reward: -5.0756, last reward: -5.0648, gradient norm:  16.76:  21%|##        | 130/625 [00:29<01:46,  4.63it/s]    reward: -5.0756, last reward: -5.0648, gradient norm:  16.76:  21%|##        | 131/625 [00:29<01:46,  4.63it/s]    reward: -4.9925, last reward: -4.5363, gradient norm:  347.6:  21%|##        | 131/625 [00:29<01:46,  4.63it/s]    reward: -4.9925, last reward: -4.5363, gradient norm:  347.6:  21%|##1       | 132/625 [00:29<01:46,  4.63it/s]    reward: -5.2872, last reward: -4.2223, gradient norm:  6.69:  21%|##1       | 132/625 [00:29<01:46,  4.63it/s]     reward: -5.2872, last reward: -4.2223, gradient norm:  6.69:  21%|##1       | 133/625 [00:29<01:46,  4.63it/s]    reward: -5.6648, last reward: -6.6645, gradient norm:  37.19:  21%|##1       | 133/625 [00:29<01:46,  4.63it/s]    reward: -5.6648, last reward: -6.6645, gradient norm:  37.19:  21%|##1       | 134/625 [00:29<01:45,  4.63it/s]    reward: -5.7473, last reward: -8.3588, gradient norm:  26.4:  21%|##1       | 134/625 [00:29<01:45,  4.63it/s]     reward: -5.7473, last reward: -8.3588, gradient norm:  26.4:  22%|##1       | 135/625 [00:29<01:45,  4.63it/s]    reward: -5.6368, last reward: -6.9012, gradient norm:  20.41:  22%|##1       | 135/625 [00:30<01:45,  4.63it/s]    reward: -5.6368, last reward: -6.9012, gradient norm:  20.41:  22%|##1       | 136/625 [00:30<01:45,  4.63it/s]    reward: -5.4529, last reward: -5.7480, gradient norm:  15.65:  22%|##1       | 136/625 [00:30<01:45,  4.63it/s]    reward: -5.4529, last reward: -5.7480, gradient norm:  15.65:  22%|##1       | 137/625 [00:30<01:45,  4.63it/s]    reward: -5.3097, last reward: -4.2365, gradient norm:  18.13:  22%|##1       | 137/625 [00:30<01:45,  4.63it/s]    reward: -5.3097, last reward: -4.2365, gradient norm:  18.13:  22%|##2       | 138/625 [00:30<01:45,  4.63it/s]    reward: -4.7727, last reward: -5.8002, gradient norm:  20.8:  22%|##2       | 138/625 [00:30<01:45,  4.63it/s]     reward: -4.7727, last reward: -5.8002, gradient norm:  20.8:  22%|##2       | 139/625 [00:30<01:45,  4.63it/s]    reward: -4.2167, last reward: -3.5346, gradient norm:  12.24:  22%|##2       | 139/625 [00:30<01:45,  4.63it/s]    reward: -4.2167, last reward: -3.5346, gradient norm:  12.24:  22%|##2       | 140/625 [00:30<01:44,  4.63it/s]    reward: -4.4297, last reward: -4.3134, gradient norm:  46.27:  22%|##2       | 140/625 [00:31<01:44,  4.63it/s]    reward: -4.4297, last reward: -4.3134, gradient norm:  46.27:  23%|##2       | 141/625 [00:31<01:45,  4.60it/s]    reward: -5.1542, last reward: -4.4714, gradient norm:  8.625:  23%|##2       | 141/625 [00:31<01:45,  4.60it/s]    reward: -5.1542, last reward: -4.4714, gradient norm:  8.625:  23%|##2       | 142/625 [00:31<01:44,  4.61it/s]    reward: -5.3136, last reward: -4.1843, gradient norm:  7.43:  23%|##2       | 142/625 [00:31<01:44,  4.61it/s]     reward: -5.3136, last reward: -4.1843, gradient norm:  7.43:  23%|##2       | 143/625 [00:31<01:44,  4.62it/s]    reward: -5.1548, last reward: -5.2351, gradient norm:  9.086:  23%|##2       | 143/625 [00:31<01:44,  4.62it/s]    reward: -5.1548, last reward: -5.2351, gradient norm:  9.086:  23%|##3       | 144/625 [00:31<01:44,  4.62it/s]    reward: -5.0157, last reward: -5.1632, gradient norm:  8.966:  23%|##3       | 144/625 [00:32<01:44,  4.62it/s]    reward: -5.0157, last reward: -5.1632, gradient norm:  8.966:  23%|##3       | 145/625 [00:32<01:43,  4.62it/s]    reward: -4.9751, last reward: -4.8632, gradient norm:  9.968:  23%|##3       | 145/625 [00:32<01:43,  4.62it/s]    reward: -4.9751, last reward: -4.8632, gradient norm:  9.968:  23%|##3       | 146/625 [00:32<01:43,  4.62it/s]    reward: -4.7366, last reward: -4.3420, gradient norm:  20.13:  23%|##3       | 146/625 [00:32<01:43,  4.62it/s]    reward: -4.7366, last reward: -4.3420, gradient norm:  20.13:  24%|##3       | 147/625 [00:32<01:43,  4.62it/s]    reward: -4.3489, last reward: -3.7309, gradient norm:  11.04:  24%|##3       | 147/625 [00:32<01:43,  4.62it/s]    reward: -4.3489, last reward: -3.7309, gradient norm:  11.04:  24%|##3       | 148/625 [00:32<01:43,  4.62it/s]    reward: -4.1923, last reward: -2.7081, gradient norm:  2.424:  24%|##3       | 148/625 [00:32<01:43,  4.62it/s]    reward: -4.1923, last reward: -2.7081, gradient norm:  2.424:  24%|##3       | 149/625 [00:32<01:42,  4.62it/s]    reward: -4.3550, last reward: -5.1679, gradient norm:  4.164:  24%|##3       | 149/625 [00:33<01:42,  4.62it/s]    reward: -4.3550, last reward: -5.1679, gradient norm:  4.164:  24%|##4       | 150/625 [00:33<01:42,  4.62it/s]    reward: -4.2149, last reward: -3.5629, gradient norm:  13.34:  24%|##4       | 150/625 [00:33<01:42,  4.62it/s]    reward: -4.2149, last reward: -3.5629, gradient norm:  13.34:  24%|##4       | 151/625 [00:33<01:42,  4.62it/s]    reward: -4.1566, last reward: -3.1028, gradient norm:  102.3:  24%|##4       | 151/625 [00:33<01:42,  4.62it/s]    reward: -4.1566, last reward: -3.1028, gradient norm:  102.3:  24%|##4       | 152/625 [00:33<01:42,  4.62it/s]    reward: -4.3382, last reward: -4.8772, gradient norm:  13.41:  24%|##4       | 152/625 [00:33<01:42,  4.62it/s]    reward: -4.3382, last reward: -4.8772, gradient norm:  13.41:  24%|##4       | 153/625 [00:33<01:42,  4.62it/s]    reward: -4.8698, last reward: -6.7157, gradient norm:  20.54:  24%|##4       | 153/625 [00:33<01:42,  4.62it/s]    reward: -4.8698, last reward: -6.7157, gradient norm:  20.54:  25%|##4       | 154/625 [00:33<01:41,  4.63it/s]    reward: -4.8397, last reward: -6.1144, gradient norm:  13.07:  25%|##4       | 154/625 [00:34<01:41,  4.63it/s]    reward: -4.8397, last reward: -6.1144, gradient norm:  13.07:  25%|##4       | 155/625 [00:34<01:41,  4.63it/s]    reward: -4.7683, last reward: -4.2360, gradient norm:  8.64:  25%|##4       | 155/625 [00:34<01:41,  4.63it/s]     reward: -4.7683, last reward: -4.2360, gradient norm:  8.64:  25%|##4       | 156/625 [00:34<01:41,  4.63it/s]    reward: -4.7235, last reward: -7.4874, gradient norm:  21.01:  25%|##4       | 156/625 [00:34<01:41,  4.63it/s]    reward: -4.7235, last reward: -7.4874, gradient norm:  21.01:  25%|##5       | 157/625 [00:34<01:40,  4.64it/s]    reward: -4.2552, last reward: -4.3192, gradient norm:  292.2:  25%|##5       | 157/625 [00:34<01:40,  4.64it/s]    reward: -4.2552, last reward: -4.3192, gradient norm:  292.2:  25%|##5       | 158/625 [00:34<01:40,  4.63it/s]    reward: -4.4812, last reward: -3.2567, gradient norm:  7.896:  25%|##5       | 158/625 [00:35<01:40,  4.63it/s]    reward: -4.4812, last reward: -3.2567, gradient norm:  7.896:  25%|##5       | 159/625 [00:35<01:40,  4.63it/s]    reward: -4.3104, last reward: -4.4942, gradient norm:  17.39:  25%|##5       | 159/625 [00:35<01:40,  4.63it/s]    reward: -4.3104, last reward: -4.4942, gradient norm:  17.39:  26%|##5       | 160/625 [00:35<01:40,  4.63it/s]    reward: -4.4804, last reward: -4.7292, gradient norm:  17.5:  26%|##5       | 160/625 [00:35<01:40,  4.63it/s]     reward: -4.4804, last reward: -4.7292, gradient norm:  17.5:  26%|##5       | 161/625 [00:35<01:40,  4.63it/s]    reward: -4.5796, last reward: -4.2654, gradient norm:  13.23:  26%|##5       | 161/625 [00:35<01:40,  4.63it/s]    reward: -4.5796, last reward: -4.2654, gradient norm:  13.23:  26%|##5       | 162/625 [00:35<01:39,  4.63it/s]    reward: -4.8805, last reward: -5.2816, gradient norm:  12.83:  26%|##5       | 162/625 [00:35<01:39,  4.63it/s]    reward: -4.8805, last reward: -5.2816, gradient norm:  12.83:  26%|##6       | 163/625 [00:35<01:39,  4.64it/s]    reward: -4.8147, last reward: -4.7800, gradient norm:  13.35:  26%|##6       | 163/625 [00:36<01:39,  4.64it/s]    reward: -4.8147, last reward: -4.7800, gradient norm:  13.35:  26%|##6       | 164/625 [00:36<01:56,  3.94it/s]    reward: -4.6485, last reward: -4.1319, gradient norm:  16.23:  26%|##6       | 164/625 [00:36<01:56,  3.94it/s]    reward: -4.6485, last reward: -4.1319, gradient norm:  16.23:  26%|##6       | 165/625 [00:36<01:51,  4.13it/s]    reward: -4.4244, last reward: -4.0058, gradient norm:  29.53:  26%|##6       | 165/625 [00:36<01:51,  4.13it/s]    reward: -4.4244, last reward: -4.0058, gradient norm:  29.53:  27%|##6       | 166/625 [00:36<01:47,  4.27it/s]    reward: -4.1652, last reward: -3.3788, gradient norm:  4.635:  27%|##6       | 166/625 [00:36<01:47,  4.27it/s]    reward: -4.1652, last reward: -3.3788, gradient norm:  4.635:  27%|##6       | 167/625 [00:36<01:44,  4.37it/s]    reward: -4.4956, last reward: -5.4883, gradient norm:  9.218:  27%|##6       | 167/625 [00:37<01:44,  4.37it/s]    reward: -4.4956, last reward: -5.4883, gradient norm:  9.218:  27%|##6       | 168/625 [00:37<01:43,  4.42it/s]    reward: -4.6229, last reward: -5.3328, gradient norm:  43.43:  27%|##6       | 168/625 [00:37<01:43,  4.42it/s]    reward: -4.6229, last reward: -5.3328, gradient norm:  43.43:  27%|##7       | 169/625 [00:37<01:41,  4.48it/s]    reward: -4.8273, last reward: -3.7098, gradient norm:  4.188:  27%|##7       | 169/625 [00:37<01:41,  4.48it/s]    reward: -4.8273, last reward: -3.7098, gradient norm:  4.188:  27%|##7       | 170/625 [00:37<01:40,  4.53it/s]    reward: -4.9447, last reward: -3.1581, gradient norm:  7.053:  27%|##7       | 170/625 [00:37<01:40,  4.53it/s]    reward: -4.9447, last reward: -3.1581, gradient norm:  7.053:  27%|##7       | 171/625 [00:37<01:39,  4.56it/s]    reward: -5.0189, last reward: -4.6233, gradient norm:  7.668:  27%|##7       | 171/625 [00:38<01:39,  4.56it/s]    reward: -5.0189, last reward: -4.6233, gradient norm:  7.668:  28%|##7       | 172/625 [00:38<01:38,  4.59it/s]    reward: -5.0278, last reward: -4.4442, gradient norm:  12.42:  28%|##7       | 172/625 [00:38<01:38,  4.59it/s]    reward: -5.0278, last reward: -4.4442, gradient norm:  12.42:  28%|##7       | 173/625 [00:38<01:38,  4.61it/s]    reward: -4.7062, last reward: -4.3810, gradient norm:  148.6:  28%|##7       | 173/625 [00:38<01:38,  4.61it/s]    reward: -4.7062, last reward: -4.3810, gradient norm:  148.6:  28%|##7       | 174/625 [00:38<01:37,  4.62it/s]    reward: -4.8141, last reward: -3.1629, gradient norm:  12.6:  28%|##7       | 174/625 [00:38<01:37,  4.62it/s]     reward: -4.8141, last reward: -3.1629, gradient norm:  12.6:  28%|##8       | 175/625 [00:38<01:37,  4.63it/s]    reward: -4.7237, last reward: -6.4486, gradient norm:  17.51:  28%|##8       | 175/625 [00:38<01:37,  4.63it/s]    reward: -4.7237, last reward: -6.4486, gradient norm:  17.51:  28%|##8       | 176/625 [00:38<01:36,  4.63it/s]    reward: -4.3132, last reward: -4.2596, gradient norm:  7.84:  28%|##8       | 176/625 [00:39<01:36,  4.63it/s]     reward: -4.3132, last reward: -4.2596, gradient norm:  7.84:  28%|##8       | 177/625 [00:39<01:36,  4.63it/s]    reward: -3.9584, last reward: -5.0491, gradient norm:  117.1:  28%|##8       | 177/625 [00:39<01:36,  4.63it/s]    reward: -3.9584, last reward: -5.0491, gradient norm:  117.1:  28%|##8       | 178/625 [00:39<01:36,  4.63it/s]    reward: -4.9415, last reward: -4.6082, gradient norm:  9.576:  28%|##8       | 178/625 [00:39<01:36,  4.63it/s]    reward: -4.9415, last reward: -4.6082, gradient norm:  9.576:  29%|##8       | 179/625 [00:39<01:36,  4.64it/s]    reward: -5.1820, last reward: -4.2659, gradient norm:  6.249:  29%|##8       | 179/625 [00:39<01:36,  4.64it/s]    reward: -5.1820, last reward: -4.2659, gradient norm:  6.249:  29%|##8       | 180/625 [00:39<01:35,  4.64it/s]    reward: -5.1256, last reward: -5.0231, gradient norm:  7.383:  29%|##8       | 180/625 [00:39<01:35,  4.64it/s]    reward: -5.1256, last reward: -5.0231, gradient norm:  7.383:  29%|##8       | 181/625 [00:39<01:35,  4.64it/s]    reward: -5.0764, last reward: -5.2045, gradient norm:  14.24:  29%|##8       | 181/625 [00:40<01:35,  4.64it/s]    reward: -5.0764, last reward: -5.2045, gradient norm:  14.24:  29%|##9       | 182/625 [00:40<01:35,  4.64it/s]    reward: -5.1614, last reward: -4.8745, gradient norm:  6.2:  29%|##9       | 182/625 [00:40<01:35,  4.64it/s]      reward: -5.1614, last reward: -4.8745, gradient norm:  6.2:  29%|##9       | 183/625 [00:40<01:35,  4.63it/s]    reward: -5.1001, last reward: -4.4028, gradient norm:  6.897:  29%|##9       | 183/625 [00:40<01:35,  4.63it/s]    reward: -5.1001, last reward: -4.4028, gradient norm:  6.897:  29%|##9       | 184/625 [00:40<01:35,  4.64it/s]    reward: -5.0939, last reward: -4.7421, gradient norm:  7.403:  29%|##9       | 184/625 [00:40<01:35,  4.64it/s]    reward: -5.0939, last reward: -4.7421, gradient norm:  7.403:  30%|##9       | 185/625 [00:40<01:34,  4.64it/s]    reward: -4.5489, last reward: -6.1487, gradient norm:  14.73:  30%|##9       | 185/625 [00:41<01:34,  4.64it/s]    reward: -4.5489, last reward: -6.1487, gradient norm:  14.73:  30%|##9       | 186/625 [00:41<01:34,  4.63it/s]    reward: -4.2853, last reward: -3.8743, gradient norm:  27.03:  30%|##9       | 186/625 [00:41<01:34,  4.63it/s]    reward: -4.2853, last reward: -3.8743, gradient norm:  27.03:  30%|##9       | 187/625 [00:41<01:34,  4.63it/s]    reward: -3.9386, last reward: -2.7690, gradient norm:  27.39:  30%|##9       | 187/625 [00:41<01:34,  4.63it/s]    reward: -3.9386, last reward: -2.7690, gradient norm:  27.39:  30%|###       | 188/625 [00:41<01:34,  4.63it/s]    reward: -4.5363, last reward: -6.5815, gradient norm:  20.31:  30%|###       | 188/625 [00:41<01:34,  4.63it/s]    reward: -4.5363, last reward: -6.5815, gradient norm:  20.31:  30%|###       | 189/625 [00:41<01:34,  4.63it/s]    reward: -4.7150, last reward: -5.2569, gradient norm:  25.93:  30%|###       | 189/625 [00:41<01:34,  4.63it/s]    reward: -4.7150, last reward: -5.2569, gradient norm:  25.93:  30%|###       | 190/625 [00:41<01:33,  4.63it/s]    reward: -4.8699, last reward: -4.2972, gradient norm:  14.85:  30%|###       | 190/625 [00:42<01:33,  4.63it/s]    reward: -4.8699, last reward: -4.2972, gradient norm:  14.85:  31%|###       | 191/625 [00:42<01:33,  4.63it/s]    reward: -4.9955, last reward: -5.1081, gradient norm:  16.16:  31%|###       | 191/625 [00:42<01:33,  4.63it/s]    reward: -4.9955, last reward: -5.1081, gradient norm:  16.16:  31%|###       | 192/625 [00:42<01:33,  4.63it/s]    reward: -5.0650, last reward: -5.2338, gradient norm:  16.49:  31%|###       | 192/625 [00:42<01:33,  4.63it/s]    reward: -5.0650, last reward: -5.2338, gradient norm:  16.49:  31%|###       | 193/625 [00:42<01:33,  4.63it/s]    reward: -4.7617, last reward: -5.9620, gradient norm:  22.74:  31%|###       | 193/625 [00:42<01:33,  4.63it/s]    reward: -4.7617, last reward: -5.9620, gradient norm:  22.74:  31%|###1      | 194/625 [00:42<01:33,  4.63it/s]    reward: -4.5403, last reward: -5.9385, gradient norm:  16.83:  31%|###1      | 194/625 [00:42<01:33,  4.63it/s]    reward: -4.5403, last reward: -5.9385, gradient norm:  16.83:  31%|###1      | 195/625 [00:42<01:33,  4.61it/s]    reward: -4.2710, last reward: -4.6990, gradient norm:  17.84:  31%|###1      | 195/625 [00:43<01:33,  4.61it/s]    reward: -4.2710, last reward: -4.6990, gradient norm:  17.84:  31%|###1      | 196/625 [00:43<01:32,  4.61it/s]    reward: -3.8874, last reward: -2.9997, gradient norm:  196.1:  31%|###1      | 196/625 [00:43<01:32,  4.61it/s]    reward: -3.8874, last reward: -2.9997, gradient norm:  196.1:  32%|###1      | 197/625 [00:43<01:32,  4.62it/s]    reward: -4.0110, last reward: -3.2850, gradient norm:  9.859:  32%|###1      | 197/625 [00:43<01:32,  4.62it/s]    reward: -4.0110, last reward: -3.2850, gradient norm:  9.859:  32%|###1      | 198/625 [00:43<01:32,  4.62it/s]    reward: -4.2655, last reward: -5.2121, gradient norm:  37.56:  32%|###1      | 198/625 [00:43<01:32,  4.62it/s]    reward: -4.2655, last reward: -5.2121, gradient norm:  37.56:  32%|###1      | 199/625 [00:43<01:32,  4.63it/s]    reward: -4.4980, last reward: -5.3311, gradient norm:  71.11:  32%|###1      | 199/625 [00:44<01:32,  4.63it/s]    reward: -4.4980, last reward: -5.3311, gradient norm:  71.11:  32%|###2      | 200/625 [00:44<01:31,  4.63it/s]    reward: -4.3305, last reward: -3.7865, gradient norm:  20.0:  32%|###2      | 200/625 [00:44<01:31,  4.63it/s]     reward: -4.3305, last reward: -3.7865, gradient norm:  20.0:  32%|###2      | 201/625 [00:44<01:31,  4.63it/s]    reward: -3.9042, last reward: -4.4459, gradient norm:  84.17:  32%|###2      | 201/625 [00:44<01:31,  4.63it/s]    reward: -3.9042, last reward: -4.4459, gradient norm:  84.17:  32%|###2      | 202/625 [00:44<01:31,  4.63it/s]    reward: -3.7960, last reward: -4.4873, gradient norm:  95.2:  32%|###2      | 202/625 [00:44<01:31,  4.63it/s]     reward: -3.7960, last reward: -4.4873, gradient norm:  95.2:  32%|###2      | 203/625 [00:44<01:31,  4.63it/s]    reward: -4.1201, last reward: -3.1649, gradient norm:  28.41:  32%|###2      | 203/625 [00:44<01:31,  4.63it/s]    reward: -4.1201, last reward: -3.1649, gradient norm:  28.41:  33%|###2      | 204/625 [00:44<01:31,  4.62it/s]    reward: -3.9688, last reward: -3.0669, gradient norm:  27.83:  33%|###2      | 204/625 [00:45<01:31,  4.62it/s]    reward: -3.9688, last reward: -3.0669, gradient norm:  27.83:  33%|###2      | 205/625 [00:45<01:30,  4.62it/s]    reward: -4.1500, last reward: -2.7162, gradient norm:  19.34:  33%|###2      | 205/625 [00:45<01:30,  4.62it/s]    reward: -4.1500, last reward: -2.7162, gradient norm:  19.34:  33%|###2      | 206/625 [00:45<01:30,  4.62it/s]    reward: -4.1441, last reward: -3.0293, gradient norm:  12.65:  33%|###2      | 206/625 [00:45<01:30,  4.62it/s]    reward: -4.1441, last reward: -3.0293, gradient norm:  12.65:  33%|###3      | 207/625 [00:45<01:30,  4.62it/s]    reward: -4.2186, last reward: -3.3986, gradient norm:  6.374:  33%|###3      | 207/625 [00:45<01:30,  4.62it/s]    reward: -4.2186, last reward: -3.3986, gradient norm:  6.374:  33%|###3      | 208/625 [00:45<01:30,  4.63it/s]    reward: -4.0562, last reward: -3.2101, gradient norm:  15.87:  33%|###3      | 208/625 [00:46<01:30,  4.63it/s]    reward: -4.0562, last reward: -3.2101, gradient norm:  15.87:  33%|###3      | 209/625 [00:46<01:45,  3.93it/s]    reward: -4.0913, last reward: -2.5205, gradient norm:  17.77:  33%|###3      | 209/625 [00:46<01:45,  3.93it/s]    reward: -4.0913, last reward: -2.5205, gradient norm:  17.77:  34%|###3      | 210/625 [00:46<01:40,  4.12it/s]    reward: -3.7103, last reward: -4.3482, gradient norm:  34.15:  34%|###3      | 210/625 [00:46<01:40,  4.12it/s]    reward: -3.7103, last reward: -4.3482, gradient norm:  34.15:  34%|###3      | 211/625 [00:46<01:37,  4.26it/s]    reward: -4.2156, last reward: -4.0283, gradient norm:  27.7:  34%|###3      | 211/625 [00:46<01:37,  4.26it/s]     reward: -4.2156, last reward: -4.0283, gradient norm:  27.7:  34%|###3      | 212/625 [00:46<01:34,  4.37it/s]    reward: -4.3044, last reward: -4.4783, gradient norm:  1.268e+03:  34%|###3      | 212/625 [00:46<01:34,  4.37it/s]    reward: -4.3044, last reward: -4.4783, gradient norm:  1.268e+03:  34%|###4      | 213/625 [00:46<01:33,  4.43it/s]    reward: -4.5168, last reward: -4.5330, gradient norm:  6.255:  34%|###4      | 213/625 [00:47<01:33,  4.43it/s]        reward: -4.5168, last reward: -4.5330, gradient norm:  6.255:  34%|###4      | 214/625 [00:47<01:31,  4.48it/s]    reward: -4.1426, last reward: -4.1175, gradient norm:  7.393:  34%|###4      | 214/625 [00:47<01:31,  4.48it/s]    reward: -4.1426, last reward: -4.1175, gradient norm:  7.393:  34%|###4      | 215/625 [00:47<01:36,  4.26it/s]    reward: -4.3528, last reward: -5.1985, gradient norm:  15.4:  34%|###4      | 215/625 [00:47<01:36,  4.26it/s]     reward: -4.3528, last reward: -5.1985, gradient norm:  15.4:  35%|###4      | 216/625 [00:47<01:39,  4.12it/s]    reward: -4.5549, last reward: -5.0121, gradient norm:  13.75:  35%|###4      | 216/625 [00:47<01:39,  4.12it/s]    reward: -4.5549, last reward: -5.0121, gradient norm:  13.75:  35%|###4      | 217/625 [00:47<01:41,  4.03it/s]    reward: -4.2009, last reward: -5.4451, gradient norm:  7.496:  35%|###4      | 217/625 [00:48<01:41,  4.03it/s]    reward: -4.2009, last reward: -5.4451, gradient norm:  7.496:  35%|###4      | 218/625 [00:48<01:42,  3.97it/s]    reward: -4.4562, last reward: -5.6786, gradient norm:  11.68:  35%|###4      | 218/625 [00:48<01:42,  3.97it/s]    reward: -4.4562, last reward: -5.6786, gradient norm:  11.68:  35%|###5      | 219/625 [00:48<01:43,  3.93it/s]    reward: -4.1657, last reward: -3.2874, gradient norm:  312.0:  35%|###5      | 219/625 [00:48<01:43,  3.93it/s]    reward: -4.1657, last reward: -3.2874, gradient norm:  312.0:  35%|###5      | 220/625 [00:48<01:43,  3.90it/s]    reward: -4.1303, last reward: -3.3616, gradient norm:  25.74:  35%|###5      | 220/625 [00:49<01:43,  3.90it/s]    reward: -4.1303, last reward: -3.3616, gradient norm:  25.74:  35%|###5      | 221/625 [00:49<01:44,  3.88it/s]    reward: -4.0414, last reward: -2.1702, gradient norm:  17.57:  35%|###5      | 221/625 [00:49<01:44,  3.88it/s]    reward: -4.0414, last reward: -2.1702, gradient norm:  17.57:  36%|###5      | 222/625 [00:49<01:44,  3.87it/s]    reward: -3.8627, last reward: -3.9349, gradient norm:  24.65:  36%|###5      | 222/625 [00:49<01:44,  3.87it/s]    reward: -3.8627, last reward: -3.9349, gradient norm:  24.65:  36%|###5      | 223/625 [00:49<01:44,  3.86it/s]    reward: -4.0825, last reward: -2.4257, gradient norm:  23.87:  36%|###5      | 223/625 [00:49<01:44,  3.86it/s]    reward: -4.0825, last reward: -2.4257, gradient norm:  23.87:  36%|###5      | 224/625 [00:49<01:44,  3.85it/s]    reward: -3.9939, last reward: -2.7512, gradient norm:  74.48:  36%|###5      | 224/625 [00:50<01:44,  3.85it/s]    reward: -3.9939, last reward: -2.7512, gradient norm:  74.48:  36%|###6      | 225/625 [00:50<01:44,  3.84it/s]    reward: -3.9160, last reward: -3.4714, gradient norm:  23.56:  36%|###6      | 225/625 [00:50<01:44,  3.84it/s]    reward: -3.9160, last reward: -3.4714, gradient norm:  23.56:  36%|###6      | 226/625 [00:50<01:43,  3.84it/s]    reward: -3.9857, last reward: -3.3414, gradient norm:  6.983:  36%|###6      | 226/625 [00:50<01:43,  3.84it/s]    reward: -3.9857, last reward: -3.3414, gradient norm:  6.983:  36%|###6      | 227/625 [00:50<01:43,  3.84it/s]    reward: -4.1241, last reward: -1.8157, gradient norm:  8.666:  36%|###6      | 227/625 [00:50<01:43,  3.84it/s]    reward: -4.1241, last reward: -1.8157, gradient norm:  8.666:  36%|###6      | 228/625 [00:50<01:43,  3.84it/s]    reward: -4.0705, last reward: -1.7081, gradient norm:  5.443:  36%|###6      | 228/625 [00:51<01:43,  3.84it/s]    reward: -4.0705, last reward: -1.7081, gradient norm:  5.443:  37%|###6      | 229/625 [00:51<01:41,  3.89it/s]    reward: -3.9893, last reward: -2.3101, gradient norm:  13.24:  37%|###6      | 229/625 [00:51<01:41,  3.89it/s]    reward: -3.9893, last reward: -2.3101, gradient norm:  13.24:  37%|###6      | 230/625 [00:51<01:36,  4.07it/s]    reward: -3.8796, last reward: -4.2426, gradient norm:  24.38:  37%|###6      | 230/625 [00:51<01:36,  4.07it/s]    reward: -3.8796, last reward: -4.2426, gradient norm:  24.38:  37%|###6      | 231/625 [00:51<01:33,  4.21it/s]    reward: -3.5865, last reward: -3.2280, gradient norm:  12.99:  37%|###6      | 231/625 [00:51<01:33,  4.21it/s]    reward: -3.5865, last reward: -3.2280, gradient norm:  12.99:  37%|###7      | 232/625 [00:51<01:30,  4.33it/s]    reward: -3.8679, last reward: -2.8119, gradient norm:  26.7:  37%|###7      | 232/625 [00:51<01:30,  4.33it/s]     reward: -3.8679, last reward: -2.8119, gradient norm:  26.7:  37%|###7      | 233/625 [00:51<01:28,  4.41it/s]    reward: -4.0144, last reward: -3.7201, gradient norm:  12.97:  37%|###7      | 233/625 [00:52<01:28,  4.41it/s]    reward: -4.0144, last reward: -3.7201, gradient norm:  12.97:  37%|###7      | 234/625 [00:52<01:27,  4.48it/s]    reward: -4.0515, last reward: -2.9970, gradient norm:  15.46:  37%|###7      | 234/625 [00:52<01:27,  4.48it/s]    reward: -4.0515, last reward: -2.9970, gradient norm:  15.46:  38%|###7      | 235/625 [00:52<01:26,  4.52it/s]    reward: -3.8066, last reward: -4.3375, gradient norm:  44.35:  38%|###7      | 235/625 [00:52<01:26,  4.52it/s]    reward: -3.8066, last reward: -4.3375, gradient norm:  44.35:  38%|###7      | 236/625 [00:52<01:25,  4.55it/s]    reward: -3.9807, last reward: -3.7684, gradient norm:  28.14:  38%|###7      | 236/625 [00:52<01:25,  4.55it/s]    reward: -3.9807, last reward: -3.7684, gradient norm:  28.14:  38%|###7      | 237/625 [00:52<01:24,  4.57it/s]    reward: -3.5995, last reward: -3.2259, gradient norm:  91.23:  38%|###7      | 237/625 [00:53<01:24,  4.57it/s]    reward: -3.5995, last reward: -3.2259, gradient norm:  91.23:  38%|###8      | 238/625 [00:53<01:24,  4.59it/s]    reward: -3.7022, last reward: -3.3968, gradient norm:  13.81:  38%|###8      | 238/625 [00:53<01:24,  4.59it/s]    reward: -3.7022, last reward: -3.3968, gradient norm:  13.81:  38%|###8      | 239/625 [00:53<01:23,  4.60it/s]    reward: -3.3982, last reward: -3.1269, gradient norm:  33.92:  38%|###8      | 239/625 [00:53<01:23,  4.60it/s]    reward: -3.3982, last reward: -3.1269, gradient norm:  33.92:  38%|###8      | 240/625 [00:53<01:23,  4.60it/s]    reward: -4.0225, last reward: -3.4321, gradient norm:  41.56:  38%|###8      | 240/625 [00:53<01:23,  4.60it/s]    reward: -4.0225, last reward: -3.4321, gradient norm:  41.56:  39%|###8      | 241/625 [00:53<01:23,  4.61it/s]    reward: -3.9207, last reward: -3.0471, gradient norm:  35.52:  39%|###8      | 241/625 [00:53<01:23,  4.61it/s]    reward: -3.9207, last reward: -3.0471, gradient norm:  35.52:  39%|###8      | 242/625 [00:53<01:23,  4.61it/s]    reward: -3.6985, last reward: -2.6172, gradient norm:  9.884:  39%|###8      | 242/625 [00:54<01:23,  4.61it/s]    reward: -3.6985, last reward: -2.6172, gradient norm:  9.884:  39%|###8      | 243/625 [00:54<01:22,  4.61it/s]    reward: -3.5946, last reward: -3.6090, gradient norm:  22.19:  39%|###8      | 243/625 [00:54<01:22,  4.61it/s]    reward: -3.5946, last reward: -3.6090, gradient norm:  22.19:  39%|###9      | 244/625 [00:54<01:22,  4.62it/s]    reward: -4.1029, last reward: -2.4233, gradient norm:  9.145:  39%|###9      | 244/625 [00:54<01:22,  4.62it/s]    reward: -4.1029, last reward: -2.4233, gradient norm:  9.145:  39%|###9      | 245/625 [00:54<01:22,  4.62it/s]    reward: -4.0060, last reward: -3.8929, gradient norm:  12.35:  39%|###9      | 245/625 [00:54<01:22,  4.62it/s]    reward: -4.0060, last reward: -3.8929, gradient norm:  12.35:  39%|###9      | 246/625 [00:54<01:22,  4.62it/s]    reward: -4.2404, last reward: -3.3984, gradient norm:  11.38:  39%|###9      | 246/625 [00:55<01:22,  4.62it/s]    reward: -4.2404, last reward: -3.3984, gradient norm:  11.38:  40%|###9      | 247/625 [00:55<01:39,  3.81it/s]    reward: -4.4397, last reward: -4.2382, gradient norm:  8.645:  40%|###9      | 247/625 [00:55<01:39,  3.81it/s]    reward: -4.4397, last reward: -4.2382, gradient norm:  8.645:  40%|###9      | 248/625 [00:55<01:35,  3.93it/s]    reward: -4.5041, last reward: -5.9562, gradient norm:  26.47:  40%|###9      | 248/625 [00:55<01:35,  3.93it/s]    reward: -4.5041, last reward: -5.9562, gradient norm:  26.47:  40%|###9      | 249/625 [00:55<01:31,  4.12it/s]    reward: -4.5235, last reward: -4.9321, gradient norm:  180.8:  40%|###9      | 249/625 [00:55<01:31,  4.12it/s]    reward: -4.5235, last reward: -4.9321, gradient norm:  180.8:  40%|####      | 250/625 [00:55<01:27,  4.26it/s]    reward: -4.4387, last reward: -4.8357, gradient norm:  14.83:  40%|####      | 250/625 [00:56<01:27,  4.26it/s]    reward: -4.4387, last reward: -4.8357, gradient norm:  14.83:  40%|####      | 251/625 [00:56<01:25,  4.37it/s]    reward: -4.4153, last reward: -6.1393, gradient norm:  37.31:  40%|####      | 251/625 [00:56<01:25,  4.37it/s]    reward: -4.4153, last reward: -6.1393, gradient norm:  37.31:  40%|####      | 252/625 [00:56<01:23,  4.45it/s]    reward: -4.2288, last reward: -5.6305, gradient norm:  27.34:  40%|####      | 252/625 [00:56<01:23,  4.45it/s]    reward: -4.2288, last reward: -5.6305, gradient norm:  27.34:  40%|####      | 253/625 [00:56<01:22,  4.50it/s]    reward: -4.3803, last reward: -4.8206, gradient norm:  11.86:  40%|####      | 253/625 [00:56<01:22,  4.50it/s]    reward: -4.3803, last reward: -4.8206, gradient norm:  11.86:  41%|####      | 254/625 [00:56<01:21,  4.54it/s]    reward: -4.0299, last reward: -2.8761, gradient norm:  6.048:  41%|####      | 254/625 [00:56<01:21,  4.54it/s]    reward: -4.0299, last reward: -2.8761, gradient norm:  6.048:  41%|####      | 255/625 [00:56<01:20,  4.57it/s]    reward: -4.1744, last reward: -2.8613, gradient norm:  13.57:  41%|####      | 255/625 [00:57<01:20,  4.57it/s]    reward: -4.1744, last reward: -2.8613, gradient norm:  13.57:  41%|####      | 256/625 [00:57<01:20,  4.59it/s]    reward: -4.2204, last reward: -3.0925, gradient norm:  24.72:  41%|####      | 256/625 [00:57<01:20,  4.59it/s]    reward: -4.2204, last reward: -3.0925, gradient norm:  24.72:  41%|####1     | 257/625 [00:57<01:19,  4.60it/s]    reward: -4.2255, last reward: -2.7406, gradient norm:  11.17:  41%|####1     | 257/625 [00:57<01:19,  4.60it/s]    reward: -4.2255, last reward: -2.7406, gradient norm:  11.17:  41%|####1     | 258/625 [00:57<01:19,  4.62it/s]    reward: -4.1357, last reward: -3.6814, gradient norm:  21.42:  41%|####1     | 258/625 [00:57<01:19,  4.62it/s]    reward: -4.1357, last reward: -3.6814, gradient norm:  21.42:  41%|####1     | 259/625 [00:57<01:19,  4.63it/s]    reward: -3.8567, last reward: -1.6356, gradient norm:  60.84:  41%|####1     | 259/625 [00:57<01:19,  4.63it/s]    reward: -3.8567, last reward: -1.6356, gradient norm:  60.84:  42%|####1     | 260/625 [00:57<01:18,  4.63it/s]    reward: -3.5807, last reward: -4.9107, gradient norm:  33.33:  42%|####1     | 260/625 [00:58<01:18,  4.63it/s]    reward: -3.5807, last reward: -4.9107, gradient norm:  33.33:  42%|####1     | 261/625 [00:58<01:18,  4.62it/s]    reward: -3.9391, last reward: -3.0113, gradient norm:  110.3:  42%|####1     | 261/625 [00:58<01:18,  4.62it/s]    reward: -3.9391, last reward: -3.0113, gradient norm:  110.3:  42%|####1     | 262/625 [00:58<01:18,  4.63it/s]    reward: -4.1248, last reward: -2.4668, gradient norm:  8.087:  42%|####1     | 262/625 [00:58<01:18,  4.63it/s]    reward: -4.1248, last reward: -2.4668, gradient norm:  8.087:  42%|####2     | 263/625 [00:58<01:18,  4.63it/s]    reward: -4.0668, last reward: -2.8278, gradient norm:  7.039:  42%|####2     | 263/625 [00:58<01:18,  4.63it/s]    reward: -4.0668, last reward: -2.8278, gradient norm:  7.039:  42%|####2     | 264/625 [00:58<01:17,  4.63it/s]    reward: -4.1988, last reward: -4.0673, gradient norm:  14.25:  42%|####2     | 264/625 [00:59<01:17,  4.63it/s]    reward: -4.1988, last reward: -4.0673, gradient norm:  14.25:  42%|####2     | 265/625 [00:59<01:17,  4.63it/s]    reward: -4.0040, last reward: -3.7700, gradient norm:  10.11:  42%|####2     | 265/625 [00:59<01:17,  4.63it/s]    reward: -4.0040, last reward: -3.7700, gradient norm:  10.11:  43%|####2     | 266/625 [00:59<01:17,  4.64it/s]    reward: -4.0511, last reward: -3.4896, gradient norm:  7.601:  43%|####2     | 266/625 [00:59<01:17,  4.64it/s]    reward: -4.0511, last reward: -3.4896, gradient norm:  7.601:  43%|####2     | 267/625 [00:59<01:17,  4.64it/s]    reward: -3.9192, last reward: -2.1827, gradient norm:  12.35:  43%|####2     | 267/625 [00:59<01:17,  4.64it/s]    reward: -3.9192, last reward: -2.1827, gradient norm:  12.35:  43%|####2     | 268/625 [00:59<01:16,  4.64it/s]    reward: -3.5280, last reward: -4.0873, gradient norm:  103.8:  43%|####2     | 268/625 [00:59<01:16,  4.64it/s]    reward: -3.5280, last reward: -4.0873, gradient norm:  103.8:  43%|####3     | 269/625 [00:59<01:16,  4.63it/s]    reward: -4.3014, last reward: -3.5507, gradient norm:  22.97:  43%|####3     | 269/625 [01:00<01:16,  4.63it/s]    reward: -4.3014, last reward: -3.5507, gradient norm:  22.97:  43%|####3     | 270/625 [01:00<01:16,  4.62it/s]    reward: -4.5069, last reward: -4.2757, gradient norm:  13.92:  43%|####3     | 270/625 [01:00<01:16,  4.62it/s]    reward: -4.5069, last reward: -4.2757, gradient norm:  13.92:  43%|####3     | 271/625 [01:00<01:16,  4.63it/s]    reward: -4.4602, last reward: -4.7364, gradient norm:  14.59:  43%|####3     | 271/625 [01:00<01:16,  4.63it/s]    reward: -4.4602, last reward: -4.7364, gradient norm:  14.59:  44%|####3     | 272/625 [01:00<01:16,  4.62it/s]    reward: -4.5137, last reward: -4.7152, gradient norm:  13.86:  44%|####3     | 272/625 [01:00<01:16,  4.62it/s]    reward: -4.5137, last reward: -4.7152, gradient norm:  13.86:  44%|####3     | 273/625 [01:00<01:16,  4.62it/s]    reward: -4.2609, last reward: -3.5740, gradient norm:  15.65:  44%|####3     | 273/625 [01:01<01:16,  4.62it/s]    reward: -4.2609, last reward: -3.5740, gradient norm:  15.65:  44%|####3     | 274/625 [01:01<01:15,  4.63it/s]    reward: -3.9472, last reward: -2.5040, gradient norm:  13.34:  44%|####3     | 274/625 [01:01<01:15,  4.63it/s]    reward: -3.9472, last reward: -2.5040, gradient norm:  13.34:  44%|####4     | 275/625 [01:01<01:15,  4.62it/s]    reward: -3.6028, last reward: -3.9918, gradient norm:  17.11:  44%|####4     | 275/625 [01:01<01:15,  4.62it/s]    reward: -3.6028, last reward: -3.9918, gradient norm:  17.11:  44%|####4     | 276/625 [01:01<01:15,  4.63it/s]    reward: -3.9059, last reward: -4.3914, gradient norm:  95.49:  44%|####4     | 276/625 [01:01<01:15,  4.63it/s]    reward: -3.9059, last reward: -4.3914, gradient norm:  95.49:  44%|####4     | 277/625 [01:01<01:15,  4.63it/s]    reward: -4.0026, last reward: -2.2842, gradient norm:  26.62:  44%|####4     | 277/625 [01:01<01:15,  4.63it/s]    reward: -4.0026, last reward: -2.2842, gradient norm:  26.62:  44%|####4     | 278/625 [01:01<01:15,  4.62it/s]    reward: -4.0321, last reward: -2.1832, gradient norm:  27.1:  44%|####4     | 278/625 [01:02<01:15,  4.62it/s]     reward: -4.0321, last reward: -2.1832, gradient norm:  27.1:  45%|####4     | 279/625 [01:02<01:14,  4.63it/s]    reward: -4.2598, last reward: -4.9834, gradient norm:  30.28:  45%|####4     | 279/625 [01:02<01:14,  4.63it/s]    reward: -4.2598, last reward: -4.9834, gradient norm:  30.28:  45%|####4     | 280/625 [01:02<01:14,  4.63it/s]    reward: -4.4768, last reward: -5.4292, gradient norm:  17.73:  45%|####4     | 280/625 [01:02<01:14,  4.63it/s]    reward: -4.4768, last reward: -5.4292, gradient norm:  17.73:  45%|####4     | 281/625 [01:02<01:14,  4.63it/s]    reward: -4.3962, last reward: -4.3840, gradient norm:  16.0:  45%|####4     | 281/625 [01:02<01:14,  4.63it/s]     reward: -4.3962, last reward: -4.3840, gradient norm:  16.0:  45%|####5     | 282/625 [01:02<01:14,  4.63it/s]    reward: -4.2931, last reward: -4.6942, gradient norm:  17.02:  45%|####5     | 282/625 [01:02<01:14,  4.63it/s]    reward: -4.2931, last reward: -4.6942, gradient norm:  17.02:  45%|####5     | 283/625 [01:02<01:13,  4.63it/s]    reward: -4.1488, last reward: -1.3838, gradient norm:  3.266:  45%|####5     | 283/625 [01:03<01:13,  4.63it/s]    reward: -4.1488, last reward: -1.3838, gradient norm:  3.266:  45%|####5     | 284/625 [01:03<01:13,  4.63it/s]    reward: -4.0074, last reward: -2.9390, gradient norm:  10.3:  45%|####5     | 284/625 [01:03<01:13,  4.63it/s]     reward: -4.0074, last reward: -2.9390, gradient norm:  10.3:  46%|####5     | 285/625 [01:03<01:13,  4.63it/s]    reward: -3.9666, last reward: -4.0514, gradient norm:  17.01:  46%|####5     | 285/625 [01:03<01:13,  4.63it/s]    reward: -3.9666, last reward: -4.0514, gradient norm:  17.01:  46%|####5     | 286/625 [01:03<01:13,  4.64it/s]    reward: -3.3936, last reward: -3.3241, gradient norm:  63.51:  46%|####5     | 286/625 [01:03<01:13,  4.64it/s]    reward: -3.3936, last reward: -3.3241, gradient norm:  63.51:  46%|####5     | 287/625 [01:03<01:12,  4.63it/s]    reward: -4.0752, last reward: -3.2862, gradient norm:  28.89:  46%|####5     | 287/625 [01:04<01:12,  4.63it/s]    reward: -4.0752, last reward: -3.2862, gradient norm:  28.89:  46%|####6     | 288/625 [01:04<01:12,  4.63it/s]    reward: -4.2323, last reward: -3.4057, gradient norm:  15.14:  46%|####6     | 288/625 [01:04<01:12,  4.63it/s]    reward: -4.2323, last reward: -3.4057, gradient norm:  15.14:  46%|####6     | 289/625 [01:04<01:12,  4.64it/s]    reward: -4.2797, last reward: -3.3964, gradient norm:  12.41:  46%|####6     | 289/625 [01:04<01:12,  4.64it/s]    reward: -4.2797, last reward: -3.3964, gradient norm:  12.41:  46%|####6     | 290/625 [01:04<01:12,  4.64it/s]    reward: -4.4318, last reward: -4.0231, gradient norm:  15.51:  46%|####6     | 290/625 [01:04<01:12,  4.64it/s]    reward: -4.4318, last reward: -4.0231, gradient norm:  15.51:  47%|####6     | 291/625 [01:04<01:11,  4.64it/s]    reward: -4.3599, last reward: -2.9775, gradient norm:  18.12:  47%|####6     | 291/625 [01:04<01:11,  4.64it/s]    reward: -4.3599, last reward: -2.9775, gradient norm:  18.12:  47%|####6     | 292/625 [01:04<01:11,  4.64it/s]    reward: -3.7158, last reward: -2.3675, gradient norm:  24.02:  47%|####6     | 292/625 [01:05<01:11,  4.64it/s]    reward: -3.7158, last reward: -2.3675, gradient norm:  24.02:  47%|####6     | 293/625 [01:05<01:11,  4.64it/s]    reward: -3.6441, last reward: -4.9933, gradient norm:  19.52:  47%|####6     | 293/625 [01:05<01:11,  4.64it/s]    reward: -3.6441, last reward: -4.9933, gradient norm:  19.52:  47%|####7     | 294/625 [01:05<01:11,  4.64it/s]    reward: -3.8600, last reward: -3.4586, gradient norm:  35.54:  47%|####7     | 294/625 [01:05<01:11,  4.64it/s]    reward: -3.8600, last reward: -3.4586, gradient norm:  35.54:  47%|####7     | 295/625 [01:05<01:11,  4.63it/s]    reward: -3.9585, last reward: -1.9932, gradient norm:  7.098:  47%|####7     | 295/625 [01:05<01:11,  4.63it/s]    reward: -3.9585, last reward: -1.9932, gradient norm:  7.098:  47%|####7     | 296/625 [01:05<01:11,  4.63it/s]    reward: -4.1756, last reward: -2.4249, gradient norm:  9.875:  47%|####7     | 296/625 [01:05<01:11,  4.63it/s]    reward: -4.1756, last reward: -2.4249, gradient norm:  9.875:  48%|####7     | 297/625 [01:05<01:10,  4.63it/s]    reward: -4.0147, last reward: -2.3067, gradient norm:  6.121:  48%|####7     | 297/625 [01:06<01:10,  4.63it/s]    reward: -4.0147, last reward: -2.3067, gradient norm:  6.121:  48%|####7     | 298/625 [01:06<01:10,  4.64it/s]    reward: -3.9180, last reward: -2.6733, gradient norm:  8.422:  48%|####7     | 298/625 [01:06<01:10,  4.64it/s]    reward: -3.9180, last reward: -2.6733, gradient norm:  8.422:  48%|####7     | 299/625 [01:06<01:10,  4.64it/s]    reward: -3.8126, last reward: -3.6604, gradient norm:  8.579:  48%|####7     | 299/625 [01:06<01:10,  4.64it/s]    reward: -3.8126, last reward: -3.6604, gradient norm:  8.579:  48%|####8     | 300/625 [01:06<01:10,  4.64it/s]    reward: -3.8080, last reward: -3.6994, gradient norm:  28.01:  48%|####8     | 300/625 [01:06<01:10,  4.64it/s]    reward: -3.8080, last reward: -3.6994, gradient norm:  28.01:  48%|####8     | 301/625 [01:06<01:09,  4.64it/s]    reward: -3.4560, last reward: -3.9351, gradient norm:  35.26:  48%|####8     | 301/625 [01:07<01:09,  4.64it/s]    reward: -3.4560, last reward: -3.9351, gradient norm:  35.26:  48%|####8     | 302/625 [01:07<01:09,  4.62it/s]    reward: -3.5834, last reward: -3.3132, gradient norm:  12.85:  48%|####8     | 302/625 [01:07<01:09,  4.62it/s]    reward: -3.5834, last reward: -3.3132, gradient norm:  12.85:  48%|####8     | 303/625 [01:07<01:09,  4.62it/s]    reward: -3.7041, last reward: -5.3072, gradient norm:  22.77:  48%|####8     | 303/625 [01:07<01:09,  4.62it/s]    reward: -3.7041, last reward: -5.3072, gradient norm:  22.77:  49%|####8     | 304/625 [01:07<01:09,  4.62it/s]    reward: -3.2861, last reward: -2.3868, gradient norm:  62.61:  49%|####8     | 304/625 [01:07<01:09,  4.62it/s]    reward: -3.2861, last reward: -2.3868, gradient norm:  62.61:  49%|####8     | 305/625 [01:07<01:09,  4.61it/s]    reward: -3.6433, last reward: -4.0882, gradient norm:  20.36:  49%|####8     | 305/625 [01:07<01:09,  4.61it/s]    reward: -3.6433, last reward: -4.0882, gradient norm:  20.36:  49%|####8     | 306/625 [01:07<01:09,  4.62it/s]    reward: -4.0045, last reward: -3.6813, gradient norm:  22.76:  49%|####8     | 306/625 [01:08<01:09,  4.62it/s]    reward: -4.0045, last reward: -3.6813, gradient norm:  22.76:  49%|####9     | 307/625 [01:08<01:08,  4.62it/s]    reward: -4.0191, last reward: -3.3033, gradient norm:  63.89:  49%|####9     | 307/625 [01:08<01:08,  4.62it/s]    reward: -4.0191, last reward: -3.3033, gradient norm:  63.89:  49%|####9     | 308/625 [01:08<01:08,  4.62it/s]    reward: -4.2015, last reward: -2.9612, gradient norm:  8.071:  49%|####9     | 308/625 [01:08<01:08,  4.62it/s]    reward: -4.2015, last reward: -2.9612, gradient norm:  8.071:  49%|####9     | 309/625 [01:08<01:08,  4.62it/s]    reward: -3.9705, last reward: -2.9102, gradient norm:  26.83:  49%|####9     | 309/625 [01:08<01:08,  4.62it/s]    reward: -3.9705, last reward: -2.9102, gradient norm:  26.83:  50%|####9     | 310/625 [01:08<01:08,  4.62it/s]    reward: -3.8991, last reward: -3.0725, gradient norm:  17.26:  50%|####9     | 310/625 [01:09<01:08,  4.62it/s]    reward: -3.8991, last reward: -3.0725, gradient norm:  17.26:  50%|####9     | 311/625 [01:09<01:08,  4.62it/s]    reward: -4.2188, last reward: -3.0618, gradient norm:  7.3:  50%|####9     | 311/625 [01:09<01:08,  4.62it/s]      reward: -4.2188, last reward: -3.0618, gradient norm:  7.3:  50%|####9     | 312/625 [01:09<01:07,  4.61it/s]    reward: -4.0159, last reward: -4.1930, gradient norm:  9.159:  50%|####9     | 312/625 [01:09<01:07,  4.61it/s]    reward: -4.0159, last reward: -4.1930, gradient norm:  9.159:  50%|#####     | 313/625 [01:09<01:07,  4.62it/s]    reward: -4.0002, last reward: -2.8525, gradient norm:  11.44:  50%|#####     | 313/625 [01:09<01:07,  4.62it/s]    reward: -4.0002, last reward: -2.8525, gradient norm:  11.44:  50%|#####     | 314/625 [01:09<01:07,  4.61it/s]    reward: -4.1452, last reward: -2.5926, gradient norm:  6.708:  50%|#####     | 314/625 [01:09<01:07,  4.61it/s]    reward: -4.1452, last reward: -2.5926, gradient norm:  6.708:  50%|#####     | 315/625 [01:09<01:07,  4.62it/s]    reward: -3.8479, last reward: -2.9037, gradient norm:  13.78:  50%|#####     | 315/625 [01:10<01:07,  4.62it/s]    reward: -3.8479, last reward: -2.9037, gradient norm:  13.78:  51%|#####     | 316/625 [01:10<01:06,  4.62it/s]    reward: -3.5351, last reward: -4.4159, gradient norm:  34.96:  51%|#####     | 316/625 [01:10<01:06,  4.62it/s]    reward: -3.5351, last reward: -4.4159, gradient norm:  34.96:  51%|#####     | 317/625 [01:10<01:18,  3.91it/s]    reward: -3.2926, last reward: -2.7374, gradient norm:  70.74:  51%|#####     | 317/625 [01:10<01:18,  3.91it/s]    reward: -3.2926, last reward: -2.7374, gradient norm:  70.74:  51%|#####     | 318/625 [01:10<01:14,  4.10it/s]    reward: -3.9609, last reward: -3.0911, gradient norm:  21.29:  51%|#####     | 318/625 [01:10<01:14,  4.10it/s]    reward: -3.9609, last reward: -3.0911, gradient norm:  21.29:  51%|#####1    | 319/625 [01:10<01:12,  4.24it/s]    reward: -4.1442, last reward: -2.7381, gradient norm:  15.31:  51%|#####1    | 319/625 [01:11<01:12,  4.24it/s]    reward: -4.1442, last reward: -2.7381, gradient norm:  15.31:  51%|#####1    | 320/625 [01:11<01:10,  4.34it/s]    reward: -4.0751, last reward: -2.4052, gradient norm:  57.12:  51%|#####1    | 320/625 [01:11<01:10,  4.34it/s]    reward: -4.0751, last reward: -2.4052, gradient norm:  57.12:  51%|#####1    | 321/625 [01:11<01:08,  4.42it/s]    reward: -3.9347, last reward: -4.9890, gradient norm:  1.483e+03:  51%|#####1    | 321/625 [01:11<01:08,  4.42it/s]    reward: -3.9347, last reward: -4.9890, gradient norm:  1.483e+03:  52%|#####1    | 322/625 [01:11<01:07,  4.48it/s]    reward: -4.2569, last reward: -5.5460, gradient norm:  28.96:  52%|#####1    | 322/625 [01:11<01:07,  4.48it/s]        reward: -4.2569, last reward: -5.5460, gradient norm:  28.96:  52%|#####1    | 323/625 [01:11<01:06,  4.52it/s]    reward: -4.5463, last reward: -3.7730, gradient norm:  16.33:  52%|#####1    | 323/625 [01:11<01:06,  4.52it/s]    reward: -4.5463, last reward: -3.7730, gradient norm:  16.33:  52%|#####1    | 324/625 [01:11<01:06,  4.55it/s]    reward: -4.3693, last reward: -4.3198, gradient norm:  374.4:  52%|#####1    | 324/625 [01:12<01:06,  4.55it/s]    reward: -4.3693, last reward: -4.3198, gradient norm:  374.4:  52%|#####2    | 325/625 [01:12<01:05,  4.57it/s]    reward: -3.5941, last reward: -4.1420, gradient norm:  255.0:  52%|#####2    | 325/625 [01:12<01:05,  4.57it/s]    reward: -3.5941, last reward: -4.1420, gradient norm:  255.0:  52%|#####2    | 326/625 [01:12<01:05,  4.58it/s]    reward: -3.8283, last reward: -3.7905, gradient norm:  21.34:  52%|#####2    | 326/625 [01:12<01:05,  4.58it/s]    reward: -3.8283, last reward: -3.7905, gradient norm:  21.34:  52%|#####2    | 327/625 [01:12<01:04,  4.59it/s]    reward: -3.6564, last reward: -3.9658, gradient norm:  169.5:  52%|#####2    | 327/625 [01:12<01:04,  4.59it/s]    reward: -3.6564, last reward: -3.9658, gradient norm:  169.5:  52%|#####2    | 328/625 [01:12<01:04,  4.60it/s]    reward: -4.3260, last reward: -3.5651, gradient norm:  20.36:  52%|#####2    | 328/625 [01:13<01:04,  4.60it/s]    reward: -4.3260, last reward: -3.5651, gradient norm:  20.36:  53%|#####2    | 329/625 [01:13<01:04,  4.60it/s]    reward: -4.2164, last reward: -4.2481, gradient norm:  37.64:  53%|#####2    | 329/625 [01:13<01:04,  4.60it/s]    reward: -4.2164, last reward: -4.2481, gradient norm:  37.64:  53%|#####2    | 330/625 [01:13<01:04,  4.60it/s]    reward: -4.2250, last reward: -2.9772, gradient norm:  23.19:  53%|#####2    | 330/625 [01:13<01:04,  4.60it/s]    reward: -4.2250, last reward: -2.9772, gradient norm:  23.19:  53%|#####2    | 331/625 [01:13<01:03,  4.60it/s]    reward: -3.8438, last reward: -5.6115, gradient norm:  58.15:  53%|#####2    | 331/625 [01:13<01:03,  4.60it/s]    reward: -3.8438, last reward: -5.6115, gradient norm:  58.15:  53%|#####3    | 332/625 [01:13<01:03,  4.60it/s]    reward: -3.9620, last reward: -3.7462, gradient norm:  43.89:  53%|#####3    | 332/625 [01:13<01:03,  4.60it/s]    reward: -3.9620, last reward: -3.7462, gradient norm:  43.89:  53%|#####3    | 333/625 [01:13<01:03,  4.61it/s]    reward: -4.1774, last reward: -4.4915, gradient norm:  43.11:  53%|#####3    | 333/625 [01:14<01:03,  4.61it/s]    reward: -4.1774, last reward: -4.4915, gradient norm:  43.11:  53%|#####3    | 334/625 [01:14<01:03,  4.61it/s]    reward: -3.8624, last reward: -2.9831, gradient norm:  37.75:  53%|#####3    | 334/625 [01:14<01:03,  4.61it/s]    reward: -3.8624, last reward: -2.9831, gradient norm:  37.75:  54%|#####3    | 335/625 [01:14<01:02,  4.61it/s]    reward: -4.2100, last reward: -5.0000, gradient norm:  24.82:  54%|#####3    | 335/625 [01:14<01:02,  4.61it/s]    reward: -4.2100, last reward: -5.0000, gradient norm:  24.82:  54%|#####3    | 336/625 [01:14<01:02,  4.62it/s]    reward: -4.1456, last reward: -2.8095, gradient norm:  201.4:  54%|#####3    | 336/625 [01:14<01:02,  4.62it/s]    reward: -4.1456, last reward: -2.8095, gradient norm:  201.4:  54%|#####3    | 337/625 [01:14<01:02,  4.62it/s]    reward: -3.9707, last reward: -3.3552, gradient norm:  21.08:  54%|#####3    | 337/625 [01:14<01:02,  4.62it/s]    reward: -3.9707, last reward: -3.3552, gradient norm:  21.08:  54%|#####4    | 338/625 [01:14<01:02,  4.62it/s]    reward: -4.2000, last reward: -3.8271, gradient norm:  23.31:  54%|#####4    | 338/625 [01:15<01:02,  4.62it/s]    reward: -4.2000, last reward: -3.8271, gradient norm:  23.31:  54%|#####4    | 339/625 [01:15<01:01,  4.63it/s]    reward: -3.8380, last reward: -3.2761, gradient norm:  33.57:  54%|#####4    | 339/625 [01:15<01:01,  4.63it/s]    reward: -3.8380, last reward: -3.2761, gradient norm:  33.57:  54%|#####4    | 340/625 [01:15<01:01,  4.63it/s]    reward: -3.9326, last reward: -2.8235, gradient norm:  156.6:  54%|#####4    | 340/625 [01:15<01:01,  4.63it/s]    reward: -3.9326, last reward: -2.8235, gradient norm:  156.6:  55%|#####4    | 341/625 [01:15<01:01,  4.63it/s]    reward: -3.8955, last reward: -2.2052, gradient norm:  13.8:  55%|#####4    | 341/625 [01:15<01:01,  4.63it/s]     reward: -3.8955, last reward: -2.2052, gradient norm:  13.8:  55%|#####4    | 342/625 [01:15<01:01,  4.63it/s]    reward: -3.5594, last reward: -2.9462, gradient norm:  132.4:  55%|#####4    | 342/625 [01:16<01:01,  4.63it/s]    reward: -3.5594, last reward: -2.9462, gradient norm:  132.4:  55%|#####4    | 343/625 [01:16<01:00,  4.64it/s]    reward: -3.7638, last reward: -2.8639, gradient norm:  77.21:  55%|#####4    | 343/625 [01:16<01:00,  4.64it/s]    reward: -3.7638, last reward: -2.8639, gradient norm:  77.21:  55%|#####5    | 344/625 [01:16<01:00,  4.63it/s]    reward: -3.6126, last reward: -3.1713, gradient norm:  75.81:  55%|#####5    | 344/625 [01:16<01:00,  4.63it/s]    reward: -3.6126, last reward: -3.1713, gradient norm:  75.81:  55%|#####5    | 345/625 [01:16<01:00,  4.64it/s]    reward: -3.8285, last reward: -3.3792, gradient norm:  53.2:  55%|#####5    | 345/625 [01:16<01:00,  4.64it/s]     reward: -3.8285, last reward: -3.3792, gradient norm:  53.2:  55%|#####5    | 346/625 [01:16<01:00,  4.64it/s]    reward: -3.7427, last reward: -3.8262, gradient norm:  47.53:  55%|#####5    | 346/625 [01:16<01:00,  4.64it/s]    reward: -3.7427, last reward: -3.8262, gradient norm:  47.53:  56%|#####5    | 347/625 [01:16<01:00,  4.63it/s]    reward: -3.8490, last reward: -3.1490, gradient norm:  58.65:  56%|#####5    | 347/625 [01:17<01:00,  4.63it/s]    reward: -3.8490, last reward: -3.1490, gradient norm:  58.65:  56%|#####5    | 348/625 [01:17<00:59,  4.64it/s]    reward: -3.8405, last reward: -3.5136, gradient norm:  55.5:  56%|#####5    | 348/625 [01:17<00:59,  4.64it/s]     reward: -3.8405, last reward: -3.5136, gradient norm:  55.5:  56%|#####5    | 349/625 [01:17<00:59,  4.64it/s]    reward: -3.9772, last reward: -2.6007, gradient norm:  31.0:  56%|#####5    | 349/625 [01:17<00:59,  4.64it/s]    reward: -3.9772, last reward: -2.6007, gradient norm:  31.0:  56%|#####6    | 350/625 [01:17<00:59,  4.63it/s]    reward: -4.2552, last reward: -3.2067, gradient norm:  10.33:  56%|#####6    | 350/625 [01:17<00:59,  4.63it/s]    reward: -4.2552, last reward: -3.2067, gradient norm:  10.33:  56%|#####6    | 351/625 [01:17<00:59,  4.64it/s]    reward: -4.4539, last reward: -2.5870, gradient norm:  10.02:  56%|#####6    | 351/625 [01:18<00:59,  4.64it/s]    reward: -4.4539, last reward: -2.5870, gradient norm:  10.02:  56%|#####6    | 352/625 [01:18<00:58,  4.64it/s]    reward: -4.1299, last reward: -3.1656, gradient norm:  10.92:  56%|#####6    | 352/625 [01:18<00:58,  4.64it/s]    reward: -4.1299, last reward: -3.1656, gradient norm:  10.92:  56%|#####6    | 353/625 [01:18<00:58,  4.64it/s]    reward: -4.1129, last reward: -1.5336, gradient norm:  5.446:  56%|#####6    | 353/625 [01:18<00:58,  4.64it/s]    reward: -4.1129, last reward: -1.5336, gradient norm:  5.446:  57%|#####6    | 354/625 [01:18<00:58,  4.64it/s]    reward: -3.8256, last reward: -2.4978, gradient norm:  12.05:  57%|#####6    | 354/625 [01:18<00:58,  4.64it/s]    reward: -3.8256, last reward: -2.4978, gradient norm:  12.05:  57%|#####6    | 355/625 [01:18<00:58,  4.65it/s]    reward: -3.4813, last reward: -2.7872, gradient norm:  9.356:  57%|#####6    | 355/625 [01:18<00:58,  4.65it/s]    reward: -3.4813, last reward: -2.7872, gradient norm:  9.356:  57%|#####6    | 356/625 [01:18<00:57,  4.64it/s]    reward: -4.1024, last reward: -5.2716, gradient norm:  16.42:  57%|#####6    | 356/625 [01:19<00:57,  4.64it/s]    reward: -4.1024, last reward: -5.2716, gradient norm:  16.42:  57%|#####7    | 357/625 [01:19<00:57,  4.64it/s]    reward: -4.0829, last reward: -5.2118, gradient norm:  14.12:  57%|#####7    | 357/625 [01:19<00:57,  4.64it/s]    reward: -4.0829, last reward: -5.2118, gradient norm:  14.12:  57%|#####7    | 358/625 [01:19<00:57,  4.64it/s]    reward: -4.7679, last reward: -6.2976, gradient norm:  13.49:  57%|#####7    | 358/625 [01:19<00:57,  4.64it/s]    reward: -4.7679, last reward: -6.2976, gradient norm:  13.49:  57%|#####7    | 359/625 [01:19<00:57,  4.63it/s]    reward: -4.5480, last reward: -4.9900, gradient norm:  7.716:  57%|#####7    | 359/625 [01:19<00:57,  4.63it/s]    reward: -4.5480, last reward: -4.9900, gradient norm:  7.716:  58%|#####7    | 360/625 [01:19<00:57,  4.63it/s]    reward: -4.9307, last reward: -7.1472, gradient norm:  12.83:  58%|#####7    | 360/625 [01:19<00:57,  4.63it/s]    reward: -4.9307, last reward: -7.1472, gradient norm:  12.83:  58%|#####7    | 361/625 [01:19<00:56,  4.63it/s]    reward: -4.4880, last reward: -5.1299, gradient norm:  12.65:  58%|#####7    | 361/625 [01:20<00:56,  4.63it/s]    reward: -4.4880, last reward: -5.1299, gradient norm:  12.65:  58%|#####7    | 362/625 [01:20<00:56,  4.63it/s]    reward: -4.2448, last reward: -5.2286, gradient norm:  18.76:  58%|#####7    | 362/625 [01:20<00:56,  4.63it/s]    reward: -4.2448, last reward: -5.2286, gradient norm:  18.76:  58%|#####8    | 363/625 [01:20<00:56,  4.63it/s]    reward: -3.6911, last reward: -2.1006, gradient norm:  103.0:  58%|#####8    | 363/625 [01:20<00:56,  4.63it/s]    reward: -3.6911, last reward: -2.1006, gradient norm:  103.0:  58%|#####8    | 364/625 [01:20<00:56,  4.63it/s]    reward: -3.6835, last reward: -4.3896, gradient norm:  68.1:  58%|#####8    | 364/625 [01:20<00:56,  4.63it/s]     reward: -3.6835, last reward: -4.3896, gradient norm:  68.1:  58%|#####8    | 365/625 [01:20<00:56,  4.63it/s]    reward: -3.7777, last reward: -2.1966, gradient norm:  201.7:  58%|#####8    | 365/625 [01:21<00:56,  4.63it/s]    reward: -3.7777, last reward: -2.1966, gradient norm:  201.7:  59%|#####8    | 366/625 [01:21<00:55,  4.63it/s]    reward: -3.8924, last reward: -4.2317, gradient norm:  18.15:  59%|#####8    | 366/625 [01:21<00:55,  4.63it/s]    reward: -3.8924, last reward: -4.2317, gradient norm:  18.15:  59%|#####8    | 367/625 [01:21<00:55,  4.62it/s]    reward: -4.1241, last reward: -4.3337, gradient norm:  12.28:  59%|#####8    | 367/625 [01:21<00:55,  4.62it/s]    reward: -4.1241, last reward: -4.3337, gradient norm:  12.28:  59%|#####8    | 368/625 [01:21<00:55,  4.62it/s]    reward: -4.1632, last reward: -4.2781, gradient norm:  14.91:  59%|#####8    | 368/625 [01:21<00:55,  4.62it/s]    reward: -4.1632, last reward: -4.2781, gradient norm:  14.91:  59%|#####9    | 369/625 [01:21<00:55,  4.62it/s]    reward: -3.9493, last reward: -4.7175, gradient norm:  25.03:  59%|#####9    | 369/625 [01:21<00:55,  4.62it/s]    reward: -3.9493, last reward: -4.7175, gradient norm:  25.03:  59%|#####9    | 370/625 [01:21<00:55,  4.62it/s]    reward: -4.1068, last reward: -3.5553, gradient norm:  8.384:  59%|#####9    | 370/625 [01:22<00:55,  4.62it/s]    reward: -4.1068, last reward: -3.5553, gradient norm:  8.384:  59%|#####9    | 371/625 [01:22<00:54,  4.62it/s]    reward: -3.7952, last reward: -2.7853, gradient norm:  11.57:  59%|#####9    | 371/625 [01:22<00:54,  4.62it/s]    reward: -3.7952, last reward: -2.7853, gradient norm:  11.57:  60%|#####9    | 372/625 [01:22<00:54,  4.62it/s]    reward: -3.4168, last reward: -2.6806, gradient norm:  7.557:  60%|#####9    | 372/625 [01:22<00:54,  4.62it/s]    reward: -3.4168, last reward: -2.6806, gradient norm:  7.557:  60%|#####9    | 373/625 [01:22<01:04,  3.92it/s]    reward: -3.6027, last reward: -4.1973, gradient norm:  14.46:  60%|#####9    | 373/625 [01:22<01:04,  3.92it/s]    reward: -3.6027, last reward: -4.1973, gradient norm:  14.46:  60%|#####9    | 374/625 [01:22<01:01,  4.11it/s]    reward: -3.7414, last reward: -4.2277, gradient norm:  15.86:  60%|#####9    | 374/625 [01:23<01:01,  4.11it/s]    reward: -3.7414, last reward: -4.2277, gradient norm:  15.86:  60%|######    | 375/625 [01:23<00:58,  4.26it/s]    reward: -3.8725, last reward: -3.8405, gradient norm:  22.08:  60%|######    | 375/625 [01:23<00:58,  4.26it/s]    reward: -3.8725, last reward: -3.8405, gradient norm:  22.08:  60%|######    | 376/625 [01:23<00:57,  4.36it/s]    reward: -3.3431, last reward: -3.5582, gradient norm:  42.58:  60%|######    | 376/625 [01:23<00:57,  4.36it/s]    reward: -3.3431, last reward: -3.5582, gradient norm:  42.58:  60%|######    | 377/625 [01:23<00:55,  4.44it/s]    reward: -3.2882, last reward: -6.1221, gradient norm:  10.87:  60%|######    | 377/625 [01:23<00:55,  4.44it/s]    reward: -3.2882, last reward: -6.1221, gradient norm:  10.87:  60%|######    | 378/625 [01:23<00:54,  4.50it/s]    reward: -3.7185, last reward: -5.5716, gradient norm:  35.1:  60%|######    | 378/625 [01:23<00:54,  4.50it/s]     reward: -3.7185, last reward: -5.5716, gradient norm:  35.1:  61%|######    | 379/625 [01:23<00:54,  4.54it/s]    reward: -3.6549, last reward: -5.1545, gradient norm:  19.96:  61%|######    | 379/625 [01:24<00:54,  4.54it/s]    reward: -3.6549, last reward: -5.1545, gradient norm:  19.96:  61%|######    | 380/625 [01:24<00:53,  4.57it/s]    reward: -3.3890, last reward: -4.4029, gradient norm:  32.75:  61%|######    | 380/625 [01:24<00:53,  4.57it/s]    reward: -3.3890, last reward: -4.4029, gradient norm:  32.75:  61%|######    | 381/625 [01:24<00:53,  4.59it/s]    reward: -3.1570, last reward: -1.6254, gradient norm:  13.47:  61%|######    | 381/625 [01:24<00:53,  4.59it/s]    reward: -3.1570, last reward: -1.6254, gradient norm:  13.47:  61%|######1   | 382/625 [01:24<00:52,  4.60it/s]    reward: -3.1548, last reward: -2.1731, gradient norm:  61.91:  61%|######1   | 382/625 [01:24<00:52,  4.60it/s]    reward: -3.1548, last reward: -2.1731, gradient norm:  61.91:  61%|######1   | 383/625 [01:24<00:52,  4.61it/s]    reward: -3.1671, last reward: -1.7785, gradient norm:  109.1:  61%|######1   | 383/625 [01:25<00:52,  4.61it/s]    reward: -3.1671, last reward: -1.7785, gradient norm:  109.1:  61%|######1   | 384/625 [01:25<00:52,  4.62it/s]    reward: -2.9914, last reward: -1.7940, gradient norm:  276.4:  61%|######1   | 384/625 [01:25<00:52,  4.62it/s]    reward: -2.9914, last reward: -1.7940, gradient norm:  276.4:  62%|######1   | 385/625 [01:25<00:51,  4.62it/s]    reward: -3.5071, last reward: -2.7138, gradient norm:  21.14:  62%|######1   | 385/625 [01:25<00:51,  4.62it/s]    reward: -3.5071, last reward: -2.7138, gradient norm:  21.14:  62%|######1   | 386/625 [01:25<00:51,  4.63it/s]    reward: -3.2596, last reward: -2.7983, gradient norm:  48.02:  62%|######1   | 386/625 [01:25<00:51,  4.63it/s]    reward: -3.2596, last reward: -2.7983, gradient norm:  48.02:  62%|######1   | 387/625 [01:25<00:51,  4.63it/s]    reward: -3.3659, last reward: -3.2849, gradient norm:  20.57:  62%|######1   | 387/625 [01:25<00:51,  4.63it/s]    reward: -3.3659, last reward: -3.2849, gradient norm:  20.57:  62%|######2   | 388/625 [01:25<00:51,  4.64it/s]    reward: -3.8605, last reward: -2.6753, gradient norm:  35.04:  62%|######2   | 388/625 [01:26<00:51,  4.64it/s]    reward: -3.8605, last reward: -2.6753, gradient norm:  35.04:  62%|######2   | 389/625 [01:26<00:50,  4.64it/s]    reward: -3.7983, last reward: -2.6005, gradient norm:  114.9:  62%|######2   | 389/625 [01:26<00:50,  4.64it/s]    reward: -3.7983, last reward: -2.6005, gradient norm:  114.9:  62%|######2   | 390/625 [01:26<00:50,  4.64it/s]    reward: -3.7873, last reward: -1.9913, gradient norm:  62.52:  62%|######2   | 390/625 [01:26<00:50,  4.64it/s]    reward: -3.7873, last reward: -1.9913, gradient norm:  62.52:  63%|######2   | 391/625 [01:26<00:50,  4.64it/s]    reward: -3.3542, last reward: -1.3406, gradient norm:  10.36:  63%|######2   | 391/625 [01:26<00:50,  4.64it/s]    reward: -3.3542, last reward: -1.3406, gradient norm:  10.36:  63%|######2   | 392/625 [01:26<00:50,  4.64it/s]    reward: -3.4591, last reward: -2.4027, gradient norm:  9.574:  63%|######2   | 392/625 [01:26<00:50,  4.64it/s]    reward: -3.4591, last reward: -2.4027, gradient norm:  9.574:  63%|######2   | 393/625 [01:26<00:49,  4.64it/s]    reward: -3.5674, last reward: -2.0402, gradient norm:  13.38:  63%|######2   | 393/625 [01:27<00:49,  4.64it/s]    reward: -3.5674, last reward: -2.0402, gradient norm:  13.38:  63%|######3   | 394/625 [01:27<00:49,  4.64it/s]    reward: -3.6186, last reward: -3.0033, gradient norm:  259.1:  63%|######3   | 394/625 [01:27<00:49,  4.64it/s]    reward: -3.6186, last reward: -3.0033, gradient norm:  259.1:  63%|######3   | 395/625 [01:27<00:49,  4.61it/s]    reward: -3.4185, last reward: -2.3325, gradient norm:  136.9:  63%|######3   | 395/625 [01:27<00:49,  4.61it/s]    reward: -3.4185, last reward: -2.3325, gradient norm:  136.9:  63%|######3   | 396/625 [01:27<00:49,  4.62it/s]    reward: -2.5159, last reward: -0.6041, gradient norm:  5.681:  63%|######3   | 396/625 [01:27<00:49,  4.62it/s]    reward: -2.5159, last reward: -0.6041, gradient norm:  5.681:  64%|######3   | 397/625 [01:27<00:49,  4.63it/s]    reward: -2.7954, last reward: -0.2275, gradient norm:  5.815:  64%|######3   | 397/625 [01:28<00:49,  4.63it/s]    reward: -2.7954, last reward: -0.2275, gradient norm:  5.815:  64%|######3   | 398/625 [01:28<00:48,  4.64it/s]    reward: -2.5413, last reward: -0.0069, gradient norm:  0.7672:  64%|######3   | 398/625 [01:28<00:48,  4.64it/s]    reward: -2.5413, last reward: -0.0069, gradient norm:  0.7672:  64%|######3   | 399/625 [01:28<00:48,  4.64it/s]    reward: -2.5722, last reward: -0.0592, gradient norm:  1.364:  64%|######3   | 399/625 [01:28<00:48,  4.64it/s]     reward: -2.5722, last reward: -0.0592, gradient norm:  1.364:  64%|######4   | 400/625 [01:28<00:48,  4.64it/s]    reward: -2.6850, last reward: -0.4255, gradient norm:  50.43:  64%|######4   | 400/625 [01:28<00:48,  4.64it/s]    reward: -2.6850, last reward: -0.4255, gradient norm:  50.43:  64%|######4   | 401/625 [01:28<00:48,  4.64it/s]    reward: -2.6123, last reward: -0.1269, gradient norm:  1.542:  64%|######4   | 401/625 [01:28<00:48,  4.64it/s]    reward: -2.6123, last reward: -0.1269, gradient norm:  1.542:  64%|######4   | 402/625 [01:28<00:47,  4.65it/s]    reward: -2.0049, last reward: -0.0202, gradient norm:  0.3814:  64%|######4   | 402/625 [01:29<00:47,  4.65it/s]    reward: -2.0049, last reward: -0.0202, gradient norm:  0.3814:  64%|######4   | 403/625 [01:29<00:47,  4.65it/s]    reward: -2.5955, last reward: -0.0309, gradient norm:  0.6388:  64%|######4   | 403/625 [01:29<00:47,  4.65it/s]    reward: -2.5955, last reward: -0.0309, gradient norm:  0.6388:  65%|######4   | 404/625 [01:29<00:47,  4.64it/s]    reward: -2.5150, last reward: -0.0265, gradient norm:  1.198:  65%|######4   | 404/625 [01:29<00:47,  4.64it/s]     reward: -2.5150, last reward: -0.0265, gradient norm:  1.198:  65%|######4   | 405/625 [01:29<00:47,  4.64it/s]    reward: -2.4821, last reward: -0.0065, gradient norm:  0.8755:  65%|######4   | 405/625 [01:29<00:47,  4.64it/s]    reward: -2.4821, last reward: -0.0065, gradient norm:  0.8755:  65%|######4   | 406/625 [01:29<00:47,  4.64it/s]    reward: -2.6165, last reward: -0.8638, gradient norm:  1.236:  65%|######4   | 406/625 [01:30<00:47,  4.64it/s]     reward: -2.6165, last reward: -0.8638, gradient norm:  1.236:  65%|######5   | 407/625 [01:30<00:46,  4.64it/s]    reward: -1.7435, last reward: -0.0147, gradient norm:  0.6713:  65%|######5   | 407/625 [01:30<00:46,  4.64it/s]    reward: -1.7435, last reward: -0.0147, gradient norm:  0.6713:  65%|######5   | 408/625 [01:30<00:46,  4.65it/s]    reward: -2.1156, last reward: -0.0243, gradient norm:  0.8155:  65%|######5   | 408/625 [01:30<00:46,  4.65it/s]    reward: -2.1156, last reward: -0.0243, gradient norm:  0.8155:  65%|######5   | 409/625 [01:30<00:46,  4.65it/s]    reward: -2.4659, last reward: -0.0141, gradient norm:  0.4149:  65%|######5   | 409/625 [01:30<00:46,  4.65it/s]    reward: -2.4659, last reward: -0.0141, gradient norm:  0.4149:  66%|######5   | 410/625 [01:30<00:46,  4.64it/s]    reward: -1.9123, last reward: -0.0025, gradient norm:  0.4418:  66%|######5   | 410/625 [01:30<00:46,  4.64it/s]    reward: -1.9123, last reward: -0.0025, gradient norm:  0.4418:  66%|######5   | 411/625 [01:30<00:46,  4.65it/s]    reward: -2.1559, last reward: -0.0176, gradient norm:  1.415:  66%|######5   | 411/625 [01:31<00:46,  4.65it/s]     reward: -2.1559, last reward: -0.0176, gradient norm:  1.415:  66%|######5   | 412/625 [01:31<00:45,  4.64it/s]    reward: -2.3635, last reward: -0.0182, gradient norm:  1.352:  66%|######5   | 412/625 [01:31<00:45,  4.64it/s]    reward: -2.3635, last reward: -0.0182, gradient norm:  1.352:  66%|######6   | 413/625 [01:31<00:45,  4.63it/s]    reward: -2.0932, last reward: -0.0056, gradient norm:  0.738:  66%|######6   | 413/625 [01:31<00:45,  4.63it/s]    reward: -2.0932, last reward: -0.0056, gradient norm:  0.738:  66%|######6   | 414/625 [01:31<00:45,  4.63it/s]    reward: -2.0281, last reward: -0.6352, gradient norm:  69.39:  66%|######6   | 414/625 [01:31<00:45,  4.63it/s]    reward: -2.0281, last reward: -0.6352, gradient norm:  69.39:  66%|######6   | 415/625 [01:31<00:45,  4.62it/s]    reward: -1.9092, last reward: -0.0020, gradient norm:  0.7145:  66%|######6   | 415/625 [01:31<00:45,  4.62it/s]    reward: -1.9092, last reward: -0.0020, gradient norm:  0.7145:  67%|######6   | 416/625 [01:31<00:45,  4.62it/s]    reward: -2.0269, last reward: -0.3579, gradient norm:  26.29:  67%|######6   | 416/625 [01:32<00:45,  4.62it/s]     reward: -2.0269, last reward: -0.3579, gradient norm:  26.29:  67%|######6   | 417/625 [01:32<00:45,  4.62it/s]    reward: -1.9136, last reward: -1.3377, gradient norm:  60.53:  67%|######6   | 417/625 [01:32<00:45,  4.62it/s]    reward: -1.9136, last reward: -1.3377, gradient norm:  60.53:  67%|######6   | 418/625 [01:32<00:52,  3.92it/s]    reward: -2.0781, last reward: -0.4073, gradient norm:  15.87:  67%|######6   | 418/625 [01:32<00:52,  3.92it/s]    reward: -2.0781, last reward: -0.4073, gradient norm:  15.87:  67%|######7   | 419/625 [01:32<00:50,  4.11it/s]    reward: -2.3373, last reward: -0.3578, gradient norm:  3.983:  67%|######7   | 419/625 [01:32<00:50,  4.11it/s]    reward: -2.3373, last reward: -0.3578, gradient norm:  3.983:  67%|######7   | 420/625 [01:32<00:48,  4.25it/s]    reward: -2.1113, last reward: -0.4225, gradient norm:  28.91:  67%|######7   | 420/625 [01:33<00:48,  4.25it/s]    reward: -2.1113, last reward: -0.4225, gradient norm:  28.91:  67%|######7   | 421/625 [01:33<00:46,  4.36it/s]    reward: -1.7475, last reward: -0.0359, gradient norm:  6.446:  67%|######7   | 421/625 [01:33<00:46,  4.36it/s]    reward: -1.7475, last reward: -0.0359, gradient norm:  6.446:  68%|######7   | 422/625 [01:33<00:45,  4.44it/s]    reward: -2.2852, last reward: -2.5245, gradient norm:  68.79:  68%|######7   | 422/625 [01:33<00:45,  4.44it/s]    reward: -2.2852, last reward: -2.5245, gradient norm:  68.79:  68%|######7   | 423/625 [01:33<00:44,  4.50it/s]    reward: -2.4992, last reward: -0.9388, gradient norm:  34.24:  68%|######7   | 423/625 [01:33<00:44,  4.50it/s]    reward: -2.4992, last reward: -0.9388, gradient norm:  34.24:  68%|######7   | 424/625 [01:33<00:44,  4.54it/s]    reward: -2.2857, last reward: -0.7211, gradient norm:  22.31:  68%|######7   | 424/625 [01:34<00:44,  4.54it/s]    reward: -2.2857, last reward: -0.7211, gradient norm:  22.31:  68%|######8   | 425/625 [01:34<00:43,  4.56it/s]    reward: -1.9163, last reward: -0.5431, gradient norm:  48.41:  68%|######8   | 425/625 [01:34<00:43,  4.56it/s]    reward: -1.9163, last reward: -0.5431, gradient norm:  48.41:  68%|######8   | 426/625 [01:34<00:43,  4.58it/s]    reward: -1.4721, last reward: -0.0830, gradient norm:  3.594:  68%|######8   | 426/625 [01:34<00:43,  4.58it/s]    reward: -1.4721, last reward: -0.0830, gradient norm:  3.594:  68%|######8   | 427/625 [01:34<00:43,  4.59it/s]    reward: -1.9386, last reward: -0.0353, gradient norm:  1.882:  68%|######8   | 427/625 [01:34<00:43,  4.59it/s]    reward: -1.9386, last reward: -0.0353, gradient norm:  1.882:  68%|######8   | 428/625 [01:34<00:42,  4.60it/s]    reward: -1.6649, last reward: -0.0023, gradient norm:  0.5471:  68%|######8   | 428/625 [01:34<00:42,  4.60it/s]    reward: -1.6649, last reward: -0.0023, gradient norm:  0.5471:  69%|######8   | 429/625 [01:34<00:42,  4.61it/s]    reward: -1.5594, last reward: -0.0303, gradient norm:  2.86:  69%|######8   | 429/625 [01:35<00:42,  4.61it/s]      reward: -1.5594, last reward: -0.0303, gradient norm:  2.86:  69%|######8   | 430/625 [01:35<00:42,  4.61it/s]    reward: -1.8240, last reward: -0.0136, gradient norm:  1.624:  69%|######8   | 430/625 [01:35<00:42,  4.61it/s]    reward: -1.8240, last reward: -0.0136, gradient norm:  1.624:  69%|######8   | 431/625 [01:35<00:42,  4.61it/s]    reward: -1.4656, last reward: -0.0013, gradient norm:  0.5372:  69%|######8   | 431/625 [01:35<00:42,  4.61it/s]    reward: -1.4656, last reward: -0.0013, gradient norm:  0.5372:  69%|######9   | 432/625 [01:35<00:41,  4.62it/s]    reward: -1.4186, last reward: -0.0110, gradient norm:  0.8879:  69%|######9   | 432/625 [01:35<00:41,  4.62it/s]    reward: -1.4186, last reward: -0.0110, gradient norm:  0.8879:  69%|######9   | 433/625 [01:35<00:41,  4.63it/s]    reward: -1.6537, last reward: -0.0077, gradient norm:  0.7151:  69%|######9   | 433/625 [01:35<00:41,  4.63it/s]    reward: -1.6537, last reward: -0.0077, gradient norm:  0.7151:  69%|######9   | 434/625 [01:35<00:41,  4.63it/s]    reward: -1.4349, last reward: -0.0004, gradient norm:  0.1692:  69%|######9   | 434/625 [01:36<00:41,  4.63it/s]    reward: -1.4349, last reward: -0.0004, gradient norm:  0.1692:  70%|######9   | 435/625 [01:36<00:41,  4.63it/s]    reward: -1.5612, last reward: -0.0062, gradient norm:  3.061:  70%|######9   | 435/625 [01:36<00:41,  4.63it/s]     reward: -1.5612, last reward: -0.0062, gradient norm:  3.061:  70%|######9   | 436/625 [01:36<00:40,  4.63it/s]    reward: -1.2758, last reward: -0.0020, gradient norm:  1.879:  70%|######9   | 436/625 [01:36<00:40,  4.63it/s]    reward: -1.2758, last reward: -0.0020, gradient norm:  1.879:  70%|######9   | 437/625 [01:36<00:40,  4.63it/s]    reward: -1.3622, last reward: -0.0052, gradient norm:  1.082:  70%|######9   | 437/625 [01:36<00:40,  4.63it/s]    reward: -1.3622, last reward: -0.0052, gradient norm:  1.082:  70%|#######   | 438/625 [01:36<00:40,  4.63it/s]    reward: -1.9641, last reward: -0.0053, gradient norm:  0.5533:  70%|#######   | 438/625 [01:37<00:40,  4.63it/s]    reward: -1.9641, last reward: -0.0053, gradient norm:  0.5533:  70%|#######   | 439/625 [01:37<00:40,  4.63it/s]    reward: -1.6092, last reward: -0.0022, gradient norm:  0.3178:  70%|#######   | 439/625 [01:37<00:40,  4.63it/s]    reward: -1.6092, last reward: -0.0022, gradient norm:  0.3178:  70%|#######   | 440/625 [01:37<00:40,  4.62it/s]    reward: -1.6514, last reward: -0.0001, gradient norm:  0.5912:  70%|#######   | 440/625 [01:37<00:40,  4.62it/s]    reward: -1.6514, last reward: -0.0001, gradient norm:  0.5912:  71%|#######   | 441/625 [01:37<00:39,  4.62it/s]    reward: -1.5670, last reward: -0.0005, gradient norm:  0.4096:  71%|#######   | 441/625 [01:37<00:39,  4.62it/s]    reward: -1.5670, last reward: -0.0005, gradient norm:  0.4096:  71%|#######   | 442/625 [01:37<00:39,  4.62it/s]    reward: -1.7481, last reward: -0.0049, gradient norm:  0.6967:  71%|#######   | 442/625 [01:37<00:39,  4.62it/s]    reward: -1.7481, last reward: -0.0049, gradient norm:  0.6967:  71%|#######   | 443/625 [01:37<00:39,  4.62it/s]    reward: -1.6597, last reward: -0.0224, gradient norm:  1.206:  71%|#######   | 443/625 [01:38<00:39,  4.62it/s]     reward: -1.6597, last reward: -0.0224, gradient norm:  1.206:  71%|#######1  | 444/625 [01:38<00:39,  4.62it/s]    reward: -1.6124, last reward: -0.0106, gradient norm:  3.425:  71%|#######1  | 444/625 [01:38<00:39,  4.62it/s]    reward: -1.6124, last reward: -0.0106, gradient norm:  3.425:  71%|#######1  | 445/625 [01:38<00:38,  4.62it/s]    reward: -1.6434, last reward: -0.0147, gradient norm:  0.8651:  71%|#######1  | 445/625 [01:38<00:38,  4.62it/s]    reward: -1.6434, last reward: -0.0147, gradient norm:  0.8651:  71%|#######1  | 446/625 [01:38<00:38,  4.62it/s]    reward: -1.8931, last reward: -0.0365, gradient norm:  1.593:  71%|#######1  | 446/625 [01:38<00:38,  4.62it/s]     reward: -1.8931, last reward: -0.0365, gradient norm:  1.593:  72%|#######1  | 447/625 [01:38<00:38,  4.63it/s]    reward: -1.6992, last reward: -0.0322, gradient norm:  2.085:  72%|#######1  | 447/625 [01:38<00:38,  4.63it/s]    reward: -1.6992, last reward: -0.0322, gradient norm:  2.085:  72%|#######1  | 448/625 [01:38<00:38,  4.62it/s]    reward: -1.8942, last reward: -0.2957, gradient norm:  46.21:  72%|#######1  | 448/625 [01:39<00:38,  4.62it/s]    reward: -1.8942, last reward: -0.2957, gradient norm:  46.21:  72%|#######1  | 449/625 [01:39<00:38,  4.62it/s]    reward: -1.7127, last reward: -0.0115, gradient norm:  3.884:  72%|#######1  | 449/625 [01:39<00:38,  4.62it/s]    reward: -1.7127, last reward: -0.0115, gradient norm:  3.884:  72%|#######2  | 450/625 [01:39<00:37,  4.63it/s]    reward: -1.7995, last reward: -0.0123, gradient norm:  1.083:  72%|#######2  | 450/625 [01:39<00:37,  4.63it/s]    reward: -1.7995, last reward: -0.0123, gradient norm:  1.083:  72%|#######2  | 451/625 [01:39<00:37,  4.62it/s]    reward: -1.6254, last reward: -0.0316, gradient norm:  1.975:  72%|#######2  | 451/625 [01:39<00:37,  4.62it/s]    reward: -1.6254, last reward: -0.0316, gradient norm:  1.975:  72%|#######2  | 452/625 [01:39<00:37,  4.62it/s]    reward: -1.6283, last reward: -0.0117, gradient norm:  0.9908:  72%|#######2  | 452/625 [01:40<00:37,  4.62it/s]    reward: -1.6283, last reward: -0.0117, gradient norm:  0.9908:  72%|#######2  | 453/625 [01:40<00:37,  4.63it/s]    reward: -1.5989, last reward: -0.0121, gradient norm:  2.507:  72%|#######2  | 453/625 [01:40<00:37,  4.63it/s]     reward: -1.5989, last reward: -0.0121, gradient norm:  2.507:  73%|#######2  | 454/625 [01:40<00:36,  4.63it/s]    reward: -1.4934, last reward: -0.0170, gradient norm:  1.981:  73%|#######2  | 454/625 [01:40<00:36,  4.63it/s]    reward: -1.4934, last reward: -0.0170, gradient norm:  1.981:  73%|#######2  | 455/625 [01:40<00:36,  4.62it/s]    reward: -1.7330, last reward: -0.0007, gradient norm:  0.3675:  73%|#######2  | 455/625 [01:40<00:36,  4.62it/s]    reward: -1.7330, last reward: -0.0007, gradient norm:  0.3675:  73%|#######2  | 456/625 [01:40<00:43,  3.92it/s]    reward: -1.3200, last reward: -0.0037, gradient norm:  0.7489:  73%|#######2  | 456/625 [01:41<00:43,  3.92it/s]    reward: -1.3200, last reward: -0.0037, gradient norm:  0.7489:  73%|#######3  | 457/625 [01:41<00:41,  4.09it/s]    reward: -1.2597, last reward: -0.0055, gradient norm:  0.6815:  73%|#######3  | 457/625 [01:41<00:41,  4.09it/s]    reward: -1.2597, last reward: -0.0055, gradient norm:  0.6815:  73%|#######3  | 458/625 [01:41<00:39,  4.24it/s]    reward: -1.5925, last reward: -0.0007, gradient norm:  0.3302:  73%|#######3  | 458/625 [01:41<00:39,  4.24it/s]    reward: -1.5925, last reward: -0.0007, gradient norm:  0.3302:  73%|#######3  | 459/625 [01:41<00:38,  4.35it/s]    reward: -1.4868, last reward: -0.0023, gradient norm:  0.4718:  73%|#######3  | 459/625 [01:41<00:38,  4.35it/s]    reward: -1.4868, last reward: -0.0023, gradient norm:  0.4718:  74%|#######3  | 460/625 [01:41<00:37,  4.43it/s]    reward: -1.5454, last reward: -0.0056, gradient norm:  0.6822:  74%|#######3  | 460/625 [01:41<00:37,  4.43it/s]    reward: -1.5454, last reward: -0.0056, gradient norm:  0.6822:  74%|#######3  | 461/625 [01:41<00:36,  4.49it/s]    reward: -1.2850, last reward: -0.0008, gradient norm:  0.3434:  74%|#######3  | 461/625 [01:42<00:36,  4.49it/s]    reward: -1.2850, last reward: -0.0008, gradient norm:  0.3434:  74%|#######3  | 462/625 [01:42<00:35,  4.53it/s]    reward: -1.2985, last reward: -0.0016, gradient norm:  0.4229:  74%|#######3  | 462/625 [01:42<00:35,  4.53it/s]    reward: -1.2985, last reward: -0.0016, gradient norm:  0.4229:  74%|#######4  | 463/625 [01:42<00:35,  4.56it/s]    reward: -1.6575, last reward: -0.0040, gradient norm:  2.829:  74%|#######4  | 463/625 [01:42<00:35,  4.56it/s]     reward: -1.6575, last reward: -0.0040, gradient norm:  2.829:  74%|#######4  | 464/625 [01:42<00:35,  4.58it/s]    reward: -1.5289, last reward: -0.0036, gradient norm:  0.5989:  74%|#######4  | 464/625 [01:42<00:35,  4.58it/s]    reward: -1.5289, last reward: -0.0036, gradient norm:  0.5989:  74%|#######4  | 465/625 [01:42<00:34,  4.60it/s]    reward: -1.3969, last reward: -0.0000, gradient norm:  0.1283:  74%|#######4  | 465/625 [01:43<00:34,  4.60it/s]    reward: -1.3969, last reward: -0.0000, gradient norm:  0.1283:  75%|#######4  | 466/625 [01:43<00:34,  4.60it/s]    reward: -1.6184, last reward: -0.0055, gradient norm:  1.4:  75%|#######4  | 466/625 [01:43<00:34,  4.60it/s]       reward: -1.6184, last reward: -0.0055, gradient norm:  1.4:  75%|#######4  | 467/625 [01:43<00:34,  4.61it/s]    reward: -1.6687, last reward: -0.0064, gradient norm:  4.008:  75%|#######4  | 467/625 [01:43<00:34,  4.61it/s]    reward: -1.6687, last reward: -0.0064, gradient norm:  4.008:  75%|#######4  | 468/625 [01:43<00:33,  4.62it/s]    reward: -1.4158, last reward: -0.0403, gradient norm:  4.025:  75%|#######4  | 468/625 [01:43<00:33,  4.62it/s]    reward: -1.4158, last reward: -0.0403, gradient norm:  4.025:  75%|#######5  | 469/625 [01:43<00:33,  4.62it/s]    reward: -1.6001, last reward: -0.0480, gradient norm:  60.92:  75%|#######5  | 469/625 [01:43<00:33,  4.62it/s]    reward: -1.6001, last reward: -0.0480, gradient norm:  60.92:  75%|#######5  | 470/625 [01:43<00:33,  4.62it/s]    reward: -1.6841, last reward: -0.0256, gradient norm:  1.598:  75%|#######5  | 470/625 [01:44<00:33,  4.62it/s]    reward: -1.6841, last reward: -0.0256, gradient norm:  1.598:  75%|#######5  | 471/625 [01:44<00:33,  4.62it/s]    reward: -1.3022, last reward: -0.0522, gradient norm:  2.971:  75%|#######5  | 471/625 [01:44<00:33,  4.62it/s]    reward: -1.3022, last reward: -0.0522, gradient norm:  2.971:  76%|#######5  | 472/625 [01:44<00:33,  4.62it/s]    reward: -1.5669, last reward: -0.0150, gradient norm:  1.43:  76%|#######5  | 472/625 [01:44<00:33,  4.62it/s]     reward: -1.5669, last reward: -0.0150, gradient norm:  1.43:  76%|#######5  | 473/625 [01:44<00:32,  4.63it/s]    reward: -1.6943, last reward: -0.0806, gradient norm:  18.19:  76%|#######5  | 473/625 [01:44<00:32,  4.63it/s]    reward: -1.6943, last reward: -0.0806, gradient norm:  18.19:  76%|#######5  | 474/625 [01:44<00:32,  4.63it/s]    reward: -1.4634, last reward: -0.5638, gradient norm:  62.72:  76%|#######5  | 474/625 [01:44<00:32,  4.63it/s]    reward: -1.4634, last reward: -0.5638, gradient norm:  62.72:  76%|#######6  | 475/625 [01:44<00:32,  4.62it/s]    reward: -1.5903, last reward: -0.0081, gradient norm:  1.497:  76%|#######6  | 475/625 [01:45<00:32,  4.62it/s]    reward: -1.5903, last reward: -0.0081, gradient norm:  1.497:  76%|#######6  | 476/625 [01:45<00:32,  4.62it/s]    reward: -1.5456, last reward: -0.0166, gradient norm:  2.212:  76%|#######6  | 476/625 [01:45<00:32,  4.62it/s]    reward: -1.5456, last reward: -0.0166, gradient norm:  2.212:  76%|#######6  | 477/625 [01:45<00:32,  4.62it/s]    reward: -1.3739, last reward: -0.0373, gradient norm:  2.901:  76%|#######6  | 477/625 [01:45<00:32,  4.62it/s]    reward: -1.3739, last reward: -0.0373, gradient norm:  2.901:  76%|#######6  | 478/625 [01:45<00:31,  4.62it/s]    reward: -1.4191, last reward: -0.0209, gradient norm:  2.153:  76%|#######6  | 478/625 [01:45<00:31,  4.62it/s]    reward: -1.4191, last reward: -0.0209, gradient norm:  2.153:  77%|#######6  | 479/625 [01:45<00:31,  4.63it/s]    reward: -1.5538, last reward: -0.0005, gradient norm:  0.5469:  77%|#######6  | 479/625 [01:46<00:31,  4.63it/s]    reward: -1.5538, last reward: -0.0005, gradient norm:  0.5469:  77%|#######6  | 480/625 [01:46<00:31,  4.63it/s]    reward: -1.3807, last reward: -0.0169, gradient norm:  1.511:  77%|#######6  | 480/625 [01:46<00:31,  4.63it/s]     reward: -1.3807, last reward: -0.0169, gradient norm:  1.511:  77%|#######6  | 481/625 [01:46<00:31,  4.63it/s]    reward: -1.3327, last reward: -0.0238, gradient norm:  1.949:  77%|#######6  | 481/625 [01:46<00:31,  4.63it/s]    reward: -1.3327, last reward: -0.0238, gradient norm:  1.949:  77%|#######7  | 482/625 [01:46<00:30,  4.64it/s]    reward: -1.5574, last reward: -0.0042, gradient norm:  0.5868:  77%|#######7  | 482/625 [01:46<00:30,  4.64it/s]    reward: -1.5574, last reward: -0.0042, gradient norm:  0.5868:  77%|#######7  | 483/625 [01:46<00:30,  4.64it/s]    reward: -1.4748, last reward: -0.0015, gradient norm:  0.3529:  77%|#######7  | 483/625 [01:46<00:30,  4.64it/s]    reward: -1.4748, last reward: -0.0015, gradient norm:  0.3529:  77%|#######7  | 484/625 [01:46<00:30,  4.63it/s]    reward: -1.8558, last reward: -0.0086, gradient norm:  0.7138:  77%|#######7  | 484/625 [01:47<00:30,  4.63it/s]    reward: -1.8558, last reward: -0.0086, gradient norm:  0.7138:  78%|#######7  | 485/625 [01:47<00:30,  4.63it/s]    reward: -1.4408, last reward: -0.0081, gradient norm:  0.6879:  78%|#######7  | 485/625 [01:47<00:30,  4.63it/s]    reward: -1.4408, last reward: -0.0081, gradient norm:  0.6879:  78%|#######7  | 486/625 [01:47<00:29,  4.64it/s]    reward: -1.5582, last reward: -0.0016, gradient norm:  0.4199:  78%|#######7  | 486/625 [01:47<00:29,  4.64it/s]    reward: -1.5582, last reward: -0.0016, gradient norm:  0.4199:  78%|#######7  | 487/625 [01:47<00:29,  4.63it/s]    reward: -1.6868, last reward: -0.0011, gradient norm:  0.4794:  78%|#######7  | 487/625 [01:47<00:29,  4.63it/s]    reward: -1.6868, last reward: -0.0011, gradient norm:  0.4794:  78%|#######8  | 488/625 [01:47<00:29,  4.63it/s]    reward: -1.3112, last reward: -0.0052, gradient norm:  0.691:  78%|#######8  | 488/625 [01:47<00:29,  4.63it/s]     reward: -1.3112, last reward: -0.0052, gradient norm:  0.691:  78%|#######8  | 489/625 [01:47<00:29,  4.64it/s]    reward: -1.4264, last reward: -0.0027, gradient norm:  0.5186:  78%|#######8  | 489/625 [01:48<00:29,  4.64it/s]    reward: -1.4264, last reward: -0.0027, gradient norm:  0.5186:  78%|#######8  | 490/625 [01:48<00:29,  4.63it/s]    reward: -1.3766, last reward: -0.0000, gradient norm:  0.1398:  78%|#######8  | 490/625 [01:48<00:29,  4.63it/s]    reward: -1.3766, last reward: -0.0000, gradient norm:  0.1398:  79%|#######8  | 491/625 [01:48<00:28,  4.63it/s]    reward: -1.7910, last reward: -0.0027, gradient norm:  0.4244:  79%|#######8  | 491/625 [01:48<00:28,  4.63it/s]    reward: -1.7910, last reward: -0.0027, gradient norm:  0.4244:  79%|#######8  | 492/625 [01:48<00:28,  4.63it/s]    reward: -1.4063, last reward: -0.0047, gradient norm:  0.5459:  79%|#######8  | 492/625 [01:48<00:28,  4.63it/s]    reward: -1.4063, last reward: -0.0047, gradient norm:  0.5459:  79%|#######8  | 493/625 [01:48<00:28,  4.62it/s]    reward: -1.7694, last reward: -0.0022, gradient norm:  0.3254:  79%|#######8  | 493/625 [01:49<00:28,  4.62it/s]    reward: -1.7694, last reward: -0.0022, gradient norm:  0.3254:  79%|#######9  | 494/625 [01:49<00:28,  4.62it/s]    reward: -1.4348, last reward: -0.0000, gradient norm:  0.1834:  79%|#######9  | 494/625 [01:49<00:28,  4.62it/s]    reward: -1.4348, last reward: -0.0000, gradient norm:  0.1834:  79%|#######9  | 495/625 [01:49<00:28,  4.62it/s]    reward: -1.2418, last reward: -0.0014, gradient norm:  0.7178:  79%|#######9  | 495/625 [01:49<00:28,  4.62it/s]    reward: -1.2418, last reward: -0.0014, gradient norm:  0.7178:  79%|#######9  | 496/625 [01:49<00:27,  4.62it/s]    reward: -1.5223, last reward: -0.0001, gradient norm:  0.4077:  79%|#######9  | 496/625 [01:49<00:27,  4.62it/s]    reward: -1.5223, last reward: -0.0001, gradient norm:  0.4077:  80%|#######9  | 497/625 [01:49<00:27,  4.62it/s]    reward: -1.3807, last reward: -0.0019, gradient norm:  0.6349:  80%|#######9  | 497/625 [01:49<00:27,  4.62it/s]    reward: -1.3807, last reward: -0.0019, gradient norm:  0.6349:  80%|#######9  | 498/625 [01:49<00:27,  4.62it/s]    reward: -1.3537, last reward: -0.0033, gradient norm:  0.36:  80%|#######9  | 498/625 [01:50<00:27,  4.62it/s]      reward: -1.3537, last reward: -0.0033, gradient norm:  0.36:  80%|#######9  | 499/625 [01:50<00:27,  4.61it/s]    reward: -1.5482, last reward: -0.0015, gradient norm:  0.282:  80%|#######9  | 499/625 [01:50<00:27,  4.61it/s]    reward: -1.5482, last reward: -0.0015, gradient norm:  0.282:  80%|########  | 500/625 [01:50<00:27,  4.62it/s]    reward: -1.4965, last reward: -0.0002, gradient norm:  0.4215:  80%|########  | 500/625 [01:50<00:27,  4.62it/s]    reward: -1.4965, last reward: -0.0002, gradient norm:  0.4215:  80%|########  | 501/625 [01:50<00:26,  4.61it/s]    reward: -1.4687, last reward: -0.0002, gradient norm:  0.3084:  80%|########  | 501/625 [01:50<00:26,  4.61it/s]    reward: -1.4687, last reward: -0.0002, gradient norm:  0.3084:  80%|########  | 502/625 [01:50<00:26,  4.61it/s]    reward: -1.2781, last reward: -0.0000, gradient norm:  0.1967:  80%|########  | 502/625 [01:51<00:26,  4.61it/s]    reward: -1.2781, last reward: -0.0000, gradient norm:  0.1967:  80%|########  | 503/625 [01:51<00:26,  4.61it/s]    reward: -1.3800, last reward: -0.0000, gradient norm:  0.5958:  80%|########  | 503/625 [01:51<00:26,  4.61it/s]    reward: -1.3800, last reward: -0.0000, gradient norm:  0.5958:  81%|########  | 504/625 [01:51<00:26,  4.59it/s]    reward: -1.5623, last reward: -0.0008, gradient norm:  2.626:  81%|########  | 504/625 [01:51<00:26,  4.59it/s]     reward: -1.5623, last reward: -0.0008, gradient norm:  2.626:  81%|########  | 505/625 [01:51<00:26,  4.58it/s]    reward: -1.5930, last reward: -0.0109, gradient norm:  0.8169:  81%|########  | 505/625 [01:51<00:26,  4.58it/s]    reward: -1.5930, last reward: -0.0109, gradient norm:  0.8169:  81%|########  | 506/625 [01:51<00:25,  4.59it/s]    reward: -1.6399, last reward: -0.0049, gradient norm:  24.37:  81%|########  | 506/625 [01:51<00:25,  4.59it/s]     reward: -1.6399, last reward: -0.0049, gradient norm:  24.37:  81%|########1 | 507/625 [01:51<00:25,  4.60it/s]    reward: -1.4313, last reward: -0.0015, gradient norm:  1.666:  81%|########1 | 507/625 [01:52<00:25,  4.60it/s]    reward: -1.4313, last reward: -0.0015, gradient norm:  1.666:  81%|########1 | 508/625 [01:52<00:25,  4.60it/s]    reward: -1.8404, last reward: -0.0004, gradient norm:  3.843:  81%|########1 | 508/625 [01:52<00:25,  4.60it/s]    reward: -1.8404, last reward: -0.0004, gradient norm:  3.843:  81%|########1 | 509/625 [01:52<00:25,  4.60it/s]    reward: -1.5611, last reward: -0.0193, gradient norm:  1.523:  81%|########1 | 509/625 [01:52<00:25,  4.60it/s]    reward: -1.5611, last reward: -0.0193, gradient norm:  1.523:  82%|########1 | 510/625 [01:52<00:24,  4.60it/s]    reward: -1.6002, last reward: -0.0110, gradient norm:  1.353:  82%|########1 | 510/625 [01:52<00:24,  4.60it/s]    reward: -1.6002, last reward: -0.0110, gradient norm:  1.353:  82%|########1 | 511/625 [01:52<00:24,  4.60it/s]    reward: -1.6186, last reward: -0.0085, gradient norm:  3.659:  82%|########1 | 511/625 [01:52<00:24,  4.60it/s]    reward: -1.6186, last reward: -0.0085, gradient norm:  3.659:  82%|########1 | 512/625 [01:52<00:24,  4.61it/s]    reward: -1.5150, last reward: -0.0029, gradient norm:  1.417:  82%|########1 | 512/625 [01:53<00:24,  4.61it/s]    reward: -1.5150, last reward: -0.0029, gradient norm:  1.417:  82%|########2 | 513/625 [01:53<00:24,  4.61it/s]    reward: -1.5302, last reward: -0.0080, gradient norm:  0.9369:  82%|########2 | 513/625 [01:53<00:24,  4.61it/s]    reward: -1.5302, last reward: -0.0080, gradient norm:  0.9369:  82%|########2 | 514/625 [01:53<00:24,  4.61it/s]    reward: -1.5740, last reward: -0.0066, gradient norm:  0.6998:  82%|########2 | 514/625 [01:53<00:24,  4.61it/s]    reward: -1.5740, last reward: -0.0066, gradient norm:  0.6998:  82%|########2 | 515/625 [01:53<00:23,  4.62it/s]    reward: -1.6370, last reward: -0.0000, gradient norm:  0.5099:  82%|########2 | 515/625 [01:53<00:23,  4.62it/s]    reward: -1.6370, last reward: -0.0000, gradient norm:  0.5099:  83%|########2 | 516/625 [01:53<00:23,  4.62it/s]    reward: -1.4817, last reward: -0.0019, gradient norm:  1.603:  83%|########2 | 516/625 [01:54<00:23,  4.62it/s]     reward: -1.4817, last reward: -0.0019, gradient norm:  1.603:  83%|########2 | 517/625 [01:54<00:23,  4.62it/s]    reward: -1.4089, last reward: -0.0013, gradient norm:  0.2707:  83%|########2 | 517/625 [01:54<00:23,  4.62it/s]    reward: -1.4089, last reward: -0.0013, gradient norm:  0.2707:  83%|########2 | 518/625 [01:54<00:23,  4.62it/s]    reward: -1.7110, last reward: -0.0042, gradient norm:  0.8602:  83%|########2 | 518/625 [01:54<00:23,  4.62it/s]    reward: -1.7110, last reward: -0.0042, gradient norm:  0.8602:  83%|########3 | 519/625 [01:54<00:22,  4.62it/s]    reward: -1.6731, last reward: -0.0003, gradient norm:  1.187:  83%|########3 | 519/625 [01:54<00:22,  4.62it/s]     reward: -1.6731, last reward: -0.0003, gradient norm:  1.187:  83%|########3 | 520/625 [01:54<00:22,  4.62it/s]    reward: -1.4991, last reward: -0.0002, gradient norm:  0.3643:  83%|########3 | 520/625 [01:54<00:22,  4.62it/s]    reward: -1.4991, last reward: -0.0002, gradient norm:  0.3643:  83%|########3 | 521/625 [01:54<00:22,  4.62it/s]    reward: -1.4262, last reward: -0.0003, gradient norm:  1.021:  83%|########3 | 521/625 [01:55<00:22,  4.62it/s]     reward: -1.4262, last reward: -0.0003, gradient norm:  1.021:  84%|########3 | 522/625 [01:55<00:22,  4.62it/s]    reward: -1.5164, last reward: -0.0027, gradient norm:  0.7196:  84%|########3 | 522/625 [01:55<00:22,  4.62it/s]    reward: -1.5164, last reward: -0.0027, gradient norm:  0.7196:  84%|########3 | 523/625 [01:55<00:22,  4.62it/s]    reward: -1.5381, last reward: -0.0000, gradient norm:  0.09353:  84%|########3 | 523/625 [01:55<00:22,  4.62it/s]    reward: -1.5381, last reward: -0.0000, gradient norm:  0.09353:  84%|########3 | 524/625 [01:55<00:21,  4.62it/s]    reward: -1.4574, last reward: -0.0103, gradient norm:  1.66:  84%|########3 | 524/625 [01:55<00:21,  4.62it/s]       reward: -1.4574, last reward: -0.0103, gradient norm:  1.66:  84%|########4 | 525/625 [01:55<00:21,  4.62it/s]    reward: -1.5063, last reward: -0.0124, gradient norm:  1.492:  84%|########4 | 525/625 [01:56<00:21,  4.62it/s]    reward: -1.5063, last reward: -0.0124, gradient norm:  1.492:  84%|########4 | 526/625 [01:56<00:25,  3.94it/s]    reward: -1.3328, last reward: -0.0053, gradient norm:  1.148:  84%|########4 | 526/625 [01:56<00:25,  3.94it/s]    reward: -1.3328, last reward: -0.0053, gradient norm:  1.148:  84%|########4 | 527/625 [01:56<00:23,  4.12it/s]    reward: -1.5620, last reward: -0.0047, gradient norm:  0.9034:  84%|########4 | 527/625 [01:56<00:23,  4.12it/s]    reward: -1.5620, last reward: -0.0047, gradient norm:  0.9034:  84%|########4 | 528/625 [01:56<00:22,  4.27it/s]    reward: -1.5060, last reward: -0.0098, gradient norm:  1.106:  84%|########4 | 528/625 [01:56<00:22,  4.27it/s]     reward: -1.5060, last reward: -0.0098, gradient norm:  1.106:  85%|########4 | 529/625 [01:56<00:21,  4.37it/s]    reward: -1.5730, last reward: -0.0000, gradient norm:  0.09915:  85%|########4 | 529/625 [01:56<00:21,  4.37it/s]    reward: -1.5730, last reward: -0.0000, gradient norm:  0.09915:  85%|########4 | 530/625 [01:56<00:21,  4.45it/s]    reward: -1.6462, last reward: -0.0048, gradient norm:  0.8528:  85%|########4 | 530/625 [01:57<00:21,  4.45it/s]     reward: -1.6462, last reward: -0.0048, gradient norm:  0.8528:  85%|########4 | 531/625 [01:57<00:20,  4.50it/s]    reward: -1.4146, last reward: -0.0030, gradient norm:  0.6585:  85%|########4 | 531/625 [01:57<00:20,  4.50it/s]    reward: -1.4146, last reward: -0.0030, gradient norm:  0.6585:  85%|########5 | 532/625 [01:57<00:20,  4.54it/s]    reward: -1.6769, last reward: -0.0004, gradient norm:  0.3161:  85%|########5 | 532/625 [01:57<00:20,  4.54it/s]    reward: -1.6769, last reward: -0.0004, gradient norm:  0.3161:  85%|########5 | 533/625 [01:57<00:20,  4.57it/s]    reward: -1.7595, last reward: -0.0088, gradient norm:  0.9262:  85%|########5 | 533/625 [01:57<00:20,  4.57it/s]    reward: -1.7595, last reward: -0.0088, gradient norm:  0.9262:  85%|########5 | 534/625 [01:57<00:19,  4.60it/s]    reward: -1.3997, last reward: -0.0028, gradient norm:  0.6147:  85%|########5 | 534/625 [01:58<00:19,  4.60it/s]    reward: -1.3997, last reward: -0.0028, gradient norm:  0.6147:  86%|########5 | 535/625 [01:58<00:19,  4.61it/s]    reward: -1.3009, last reward: -0.0021, gradient norm:  0.5009:  86%|########5 | 535/625 [01:58<00:19,  4.61it/s]    reward: -1.3009, last reward: -0.0021, gradient norm:  0.5009:  86%|########5 | 536/625 [01:58<00:19,  4.62it/s]    reward: -1.5789, last reward: -0.0067, gradient norm:  0.9273:  86%|########5 | 536/625 [01:58<00:19,  4.62it/s]    reward: -1.5789, last reward: -0.0067, gradient norm:  0.9273:  86%|########5 | 537/625 [01:58<00:19,  4.63it/s]    reward: -1.5459, last reward: -0.0002, gradient norm:  0.2331:  86%|########5 | 537/625 [01:58<00:19,  4.63it/s]    reward: -1.5459, last reward: -0.0002, gradient norm:  0.2331:  86%|########6 | 538/625 [01:58<00:18,  4.63it/s]    reward: -1.3397, last reward: -0.0061, gradient norm:  0.8454:  86%|########6 | 538/625 [01:58<00:18,  4.63it/s]    reward: -1.3397, last reward: -0.0061, gradient norm:  0.8454:  86%|########6 | 539/625 [01:58<00:18,  4.63it/s]    reward: -1.5125, last reward: -0.0034, gradient norm:  0.7267:  86%|########6 | 539/625 [01:59<00:18,  4.63it/s]    reward: -1.5125, last reward: -0.0034, gradient norm:  0.7267:  86%|########6 | 540/625 [01:59<00:18,  4.63it/s]    reward: -1.5344, last reward: -0.0000, gradient norm:  0.1777:  86%|########6 | 540/625 [01:59<00:18,  4.63it/s]    reward: -1.5344, last reward: -0.0000, gradient norm:  0.1777:  87%|########6 | 541/625 [01:59<00:18,  4.63it/s]    reward: -1.4060, last reward: -0.0015, gradient norm:  0.6126:  87%|########6 | 541/625 [01:59<00:18,  4.63it/s]    reward: -1.4060, last reward: -0.0015, gradient norm:  0.6126:  87%|########6 | 542/625 [01:59<00:17,  4.63it/s]    reward: -1.5194, last reward: -0.0001, gradient norm:  5.273:  87%|########6 | 542/625 [01:59<00:17,  4.63it/s]     reward: -1.5194, last reward: -0.0001, gradient norm:  5.273:  87%|########6 | 543/625 [01:59<00:17,  4.63it/s]    reward: -1.2428, last reward: -0.0018, gradient norm:  0.7637:  87%|########6 | 543/625 [02:00<00:17,  4.63it/s]    reward: -1.2428, last reward: -0.0018, gradient norm:  0.7637:  87%|########7 | 544/625 [02:00<00:17,  4.63it/s]    reward: -1.4377, last reward: -0.0011, gradient norm:  1.96:  87%|########7 | 544/625 [02:00<00:17,  4.63it/s]      reward: -1.4377, last reward: -0.0011, gradient norm:  1.96:  87%|########7 | 545/625 [02:00<00:17,  4.63it/s]    reward: -1.7458, last reward: -0.1639, gradient norm:  15.18:  87%|########7 | 545/625 [02:00<00:17,  4.63it/s]    reward: -1.7458, last reward: -0.1639, gradient norm:  15.18:  87%|########7 | 546/625 [02:00<00:17,  4.63it/s]    reward: -1.4933, last reward: -0.0148, gradient norm:  2.942:  87%|########7 | 546/625 [02:00<00:17,  4.63it/s]    reward: -1.4933, last reward: -0.0148, gradient norm:  2.942:  88%|########7 | 547/625 [02:00<00:16,  4.63it/s]    reward: -1.6040, last reward: -0.0095, gradient norm:  1.472:  88%|########7 | 547/625 [02:00<00:16,  4.63it/s]    reward: -1.6040, last reward: -0.0095, gradient norm:  1.472:  88%|########7 | 548/625 [02:00<00:16,  4.62it/s]    reward: -1.8440, last reward: -0.0142, gradient norm:  1.938:  88%|########7 | 548/625 [02:01<00:16,  4.62it/s]    reward: -1.8440, last reward: -0.0142, gradient norm:  1.938:  88%|########7 | 549/625 [02:01<00:16,  4.60it/s]    reward: -1.4537, last reward: -0.0000, gradient norm:  0.5649:  88%|########7 | 549/625 [02:01<00:16,  4.60it/s]    reward: -1.4537, last reward: -0.0000, gradient norm:  0.5649:  88%|########8 | 550/625 [02:01<00:16,  4.59it/s]    reward: -1.6866, last reward: -0.2054, gradient norm:  69.51:  88%|########8 | 550/625 [02:01<00:16,  4.59it/s]     reward: -1.6866, last reward: -0.2054, gradient norm:  69.51:  88%|########8 | 551/625 [02:01<00:16,  4.60it/s]    reward: -2.0001, last reward: -0.0300, gradient norm:  8.11:  88%|########8 | 551/625 [02:01<00:16,  4.60it/s]     reward: -2.0001, last reward: -0.0300, gradient norm:  8.11:  88%|########8 | 552/625 [02:01<00:15,  4.61it/s]    reward: -1.7342, last reward: -0.0044, gradient norm:  13.5:  88%|########8 | 552/625 [02:01<00:15,  4.61it/s]    reward: -1.7342, last reward: -0.0044, gradient norm:  13.5:  88%|########8 | 553/625 [02:01<00:15,  4.61it/s]    reward: -1.6614, last reward: -0.0134, gradient norm:  5.897:  88%|########8 | 553/625 [02:02<00:15,  4.61it/s]    reward: -1.6614, last reward: -0.0134, gradient norm:  5.897:  89%|########8 | 554/625 [02:02<00:15,  4.61it/s]    reward: -2.3916, last reward: -0.0883, gradient norm:  6.243:  89%|########8 | 554/625 [02:02<00:15,  4.61it/s]    reward: -2.3916, last reward: -0.0883, gradient norm:  6.243:  89%|########8 | 555/625 [02:02<00:15,  4.61it/s]    reward: -2.0259, last reward: -0.0387, gradient norm:  15.33:  89%|########8 | 555/625 [02:02<00:15,  4.61it/s]    reward: -2.0259, last reward: -0.0387, gradient norm:  15.33:  89%|########8 | 556/625 [02:02<00:14,  4.61it/s]    reward: -1.5540, last reward: -0.0272, gradient norm:  2.735:  89%|########8 | 556/625 [02:02<00:14,  4.61it/s]    reward: -1.5540, last reward: -0.0272, gradient norm:  2.735:  89%|########9 | 557/625 [02:02<00:14,  4.62it/s]    reward: -2.4142, last reward: -0.3006, gradient norm:  17.59:  89%|########9 | 557/625 [02:03<00:14,  4.62it/s]    reward: -2.4142, last reward: -0.3006, gradient norm:  17.59:  89%|########9 | 558/625 [02:03<00:14,  4.62it/s]    reward: -1.6493, last reward: -0.0001, gradient norm:  0.3002:  89%|########9 | 558/625 [02:03<00:14,  4.62it/s]    reward: -1.6493, last reward: -0.0001, gradient norm:  0.3002:  89%|########9 | 559/625 [02:03<00:14,  4.62it/s]    reward: -1.8739, last reward: -0.0019, gradient norm:  0.5192:  89%|########9 | 559/625 [02:03<00:14,  4.62it/s]    reward: -1.8739, last reward: -0.0019, gradient norm:  0.5192:  90%|########9 | 560/625 [02:03<00:14,  4.62it/s]    reward: -1.5129, last reward: -0.0005, gradient norm:  0.8113:  90%|########9 | 560/625 [02:03<00:14,  4.62it/s]    reward: -1.5129, last reward: -0.0005, gradient norm:  0.8113:  90%|########9 | 561/625 [02:03<00:13,  4.62it/s]    reward: -1.8092, last reward: -0.0000, gradient norm:  0.3621:  90%|########9 | 561/625 [02:03<00:13,  4.62it/s]    reward: -1.8092, last reward: -0.0000, gradient norm:  0.3621:  90%|########9 | 562/625 [02:03<00:13,  4.61it/s]    reward: -1.3960, last reward: -0.0005, gradient norm:  1.13:  90%|########9 | 562/625 [02:04<00:13,  4.61it/s]      reward: -1.3960, last reward: -0.0005, gradient norm:  1.13:  90%|######### | 563/625 [02:04<00:13,  4.62it/s]    reward: -1.5756, last reward: -0.0022, gradient norm:  3.178:  90%|######### | 563/625 [02:04<00:13,  4.62it/s]    reward: -1.5756, last reward: -0.0022, gradient norm:  3.178:  90%|######### | 564/625 [02:04<00:13,  4.62it/s]    reward: -1.4223, last reward: -0.0078, gradient norm:  0.8371:  90%|######### | 564/625 [02:04<00:13,  4.62it/s]    reward: -1.4223, last reward: -0.0078, gradient norm:  0.8371:  90%|######### | 565/625 [02:04<00:12,  4.62it/s]    reward: -1.5067, last reward: -0.0033, gradient norm:  0.4395:  90%|######### | 565/625 [02:04<00:12,  4.62it/s]    reward: -1.5067, last reward: -0.0033, gradient norm:  0.4395:  91%|######### | 566/625 [02:04<00:12,  4.62it/s]    reward: -1.4185, last reward: -0.0001, gradient norm:  0.2474:  91%|######### | 566/625 [02:04<00:12,  4.62it/s]    reward: -1.4185, last reward: -0.0001, gradient norm:  0.2474:  91%|######### | 567/625 [02:04<00:12,  4.62it/s]    reward: -1.6147, last reward: -0.0024, gradient norm:  0.8578:  91%|######### | 567/625 [02:05<00:12,  4.62it/s]    reward: -1.6147, last reward: -0.0024, gradient norm:  0.8578:  91%|######### | 568/625 [02:05<00:12,  4.61it/s]    reward: -1.5831, last reward: -0.0012, gradient norm:  0.6581:  91%|######### | 568/625 [02:05<00:12,  4.61it/s]    reward: -1.5831, last reward: -0.0012, gradient norm:  0.6581:  91%|#########1| 569/625 [02:05<00:12,  4.50it/s]    reward: -1.3662, last reward: -0.0010, gradient norm:  0.5693:  91%|#########1| 569/625 [02:05<00:12,  4.50it/s]    reward: -1.3662, last reward: -0.0010, gradient norm:  0.5693:  91%|#########1| 570/625 [02:05<00:12,  4.54it/s]    reward: -1.4515, last reward: -0.0036, gradient norm:  0.62:  91%|#########1| 570/625 [02:05<00:12,  4.54it/s]      reward: -1.4515, last reward: -0.0036, gradient norm:  0.62:  91%|#########1| 571/625 [02:05<00:11,  4.57it/s]    reward: -1.5741, last reward: -0.0012, gradient norm:  0.2417:  91%|#########1| 571/625 [02:06<00:11,  4.57it/s]    reward: -1.5741, last reward: -0.0012, gradient norm:  0.2417:  92%|#########1| 572/625 [02:06<00:11,  4.59it/s]    reward: -1.6877, last reward: -0.0001, gradient norm:  1.519:  92%|#########1| 572/625 [02:06<00:11,  4.59it/s]     reward: -1.6877, last reward: -0.0001, gradient norm:  1.519:  92%|#########1| 573/625 [02:06<00:11,  4.60it/s]    reward: -1.4525, last reward: -0.0000, gradient norm:  2.69:  92%|#########1| 573/625 [02:06<00:11,  4.60it/s]     reward: -1.4525, last reward: -0.0000, gradient norm:  2.69:  92%|#########1| 574/625 [02:06<00:11,  4.61it/s]    reward: -1.2384, last reward: -0.0014, gradient norm:  1.931:  92%|#########1| 574/625 [02:06<00:11,  4.61it/s]    reward: -1.2384, last reward: -0.0014, gradient norm:  1.931:  92%|#########2| 575/625 [02:06<00:10,  4.62it/s]    reward: -1.6353, last reward: -0.0016, gradient norm:  0.9342:  92%|#########2| 575/625 [02:06<00:10,  4.62it/s]    reward: -1.6353, last reward: -0.0016, gradient norm:  0.9342:  92%|#########2| 576/625 [02:06<00:10,  4.63it/s]    reward: -1.7127, last reward: -0.0103, gradient norm:  2.147:  92%|#########2| 576/625 [02:07<00:10,  4.63it/s]     reward: -1.7127, last reward: -0.0103, gradient norm:  2.147:  92%|#########2| 577/625 [02:07<00:10,  4.60it/s]    reward: -1.5638, last reward: -0.0158, gradient norm:  1.805:  92%|#########2| 577/625 [02:07<00:10,  4.60it/s]    reward: -1.5638, last reward: -0.0158, gradient norm:  1.805:  92%|#########2| 578/625 [02:07<00:10,  4.62it/s]    reward: -1.5378, last reward: -0.0109, gradient norm:  1.015:  92%|#########2| 578/625 [02:07<00:10,  4.62it/s]    reward: -1.5378, last reward: -0.0109, gradient norm:  1.015:  93%|#########2| 579/625 [02:07<00:09,  4.63it/s]    reward: -1.4552, last reward: -0.0000, gradient norm:  1.069:  93%|#########2| 579/625 [02:07<00:09,  4.63it/s]    reward: -1.4552, last reward: -0.0000, gradient norm:  1.069:  93%|#########2| 580/625 [02:07<00:09,  4.63it/s]    reward: -1.3430, last reward: -0.0176, gradient norm:  2.5:  93%|#########2| 580/625 [02:08<00:09,  4.63it/s]      reward: -1.3430, last reward: -0.0176, gradient norm:  2.5:  93%|#########2| 581/625 [02:08<00:09,  4.64it/s]    reward: -1.5089, last reward: -0.0315, gradient norm:  3.384:  93%|#########2| 581/625 [02:08<00:09,  4.64it/s]    reward: -1.5089, last reward: -0.0315, gradient norm:  3.384:  93%|#########3| 582/625 [02:08<00:10,  3.93it/s]    reward: -1.5640, last reward: -0.0072, gradient norm:  2.31:  93%|#########3| 582/625 [02:08<00:10,  3.93it/s]     reward: -1.5640, last reward: -0.0072, gradient norm:  2.31:  93%|#########3| 583/625 [02:08<00:10,  4.12it/s]    reward: -1.1114, last reward: -0.0002, gradient norm:  0.5908:  93%|#########3| 583/625 [02:08<00:10,  4.12it/s]    reward: -1.1114, last reward: -0.0002, gradient norm:  0.5908:  93%|#########3| 584/625 [02:08<00:09,  4.27it/s]    reward: -1.4767, last reward: -0.0048, gradient norm:  16.35:  93%|#########3| 584/625 [02:09<00:09,  4.27it/s]     reward: -1.4767, last reward: -0.0048, gradient norm:  16.35:  94%|#########3| 585/625 [02:09<00:09,  4.37it/s]    reward: -1.4403, last reward: -0.0091, gradient norm:  1.692:  94%|#########3| 585/625 [02:09<00:09,  4.37it/s]    reward: -1.4403, last reward: -0.0091, gradient norm:  1.692:  94%|#########3| 586/625 [02:09<00:08,  4.45it/s]    reward: -1.3886, last reward: -0.0125, gradient norm:  0.9933:  94%|#########3| 586/625 [02:09<00:08,  4.45it/s]    reward: -1.3886, last reward: -0.0125, gradient norm:  0.9933:  94%|#########3| 587/625 [02:09<00:08,  4.51it/s]    reward: -1.6368, last reward: -0.0062, gradient norm:  0.6599:  94%|#########3| 587/625 [02:09<00:08,  4.51it/s]    reward: -1.6368, last reward: -0.0062, gradient norm:  0.6599:  94%|#########4| 588/625 [02:09<00:08,  4.55it/s]    reward: -1.3784, last reward: -0.0007, gradient norm:  0.3128:  94%|#########4| 588/625 [02:09<00:08,  4.55it/s]    reward: -1.3784, last reward: -0.0007, gradient norm:  0.3128:  94%|#########4| 589/625 [02:09<00:07,  4.58it/s]    reward: -1.6548, last reward: -0.0010, gradient norm:  1.493:  94%|#########4| 589/625 [02:10<00:07,  4.58it/s]     reward: -1.6548, last reward: -0.0010, gradient norm:  1.493:  94%|#########4| 590/625 [02:10<00:07,  4.60it/s]    reward: -1.6893, last reward: -0.0090, gradient norm:  61.66:  94%|#########4| 590/625 [02:10<00:07,  4.60it/s]    reward: -1.6893, last reward: -0.0090, gradient norm:  61.66:  95%|#########4| 591/625 [02:10<00:07,  4.61it/s]    reward: -2.1896, last reward: -0.0222, gradient norm:  0.7727:  95%|#########4| 591/625 [02:10<00:07,  4.61it/s]    reward: -2.1896, last reward: -0.0222, gradient norm:  0.7727:  95%|#########4| 592/625 [02:10<00:07,  4.62it/s]    reward: -2.2106, last reward: -0.0207, gradient norm:  1.203:  95%|#########4| 592/625 [02:10<00:07,  4.62it/s]     reward: -2.2106, last reward: -0.0207, gradient norm:  1.203:  95%|#########4| 593/625 [02:10<00:06,  4.63it/s]    reward: -1.7451, last reward: -0.0166, gradient norm:  0.9996:  95%|#########4| 593/625 [02:10<00:06,  4.63it/s]    reward: -1.7451, last reward: -0.0166, gradient norm:  0.9996:  95%|#########5| 594/625 [02:10<00:06,  4.63it/s]    reward: -2.2112, last reward: -0.0073, gradient norm:  0.7498:  95%|#########5| 594/625 [02:11<00:06,  4.63it/s]    reward: -2.2112, last reward: -0.0073, gradient norm:  0.7498:  95%|#########5| 595/625 [02:11<00:06,  4.62it/s]    reward: -4.0536, last reward: -2.8849, gradient norm:  60.86:  95%|#########5| 595/625 [02:11<00:06,  4.62it/s]     reward: -4.0536, last reward: -2.8849, gradient norm:  60.86:  95%|#########5| 596/625 [02:11<00:06,  4.62it/s]    reward: -4.2815, last reward: -2.7195, gradient norm:  28.21:  95%|#########5| 596/625 [02:11<00:06,  4.62it/s]    reward: -4.2815, last reward: -2.7195, gradient norm:  28.21:  96%|#########5| 597/625 [02:11<00:06,  4.62it/s]    reward: -3.5419, last reward: -4.0450, gradient norm:  48.7:  96%|#########5| 597/625 [02:11<00:06,  4.62it/s]     reward: -3.5419, last reward: -4.0450, gradient norm:  48.7:  96%|#########5| 598/625 [02:11<00:05,  4.62it/s]    reward: -2.1132, last reward: -0.0058, gradient norm:  1.541:  96%|#########5| 598/625 [02:12<00:05,  4.62it/s]    reward: -2.1132, last reward: -0.0058, gradient norm:  1.541:  96%|#########5| 599/625 [02:12<00:05,  4.62it/s]    reward: -1.6940, last reward: -0.0142, gradient norm:  0.6006:  96%|#########5| 599/625 [02:12<00:05,  4.62it/s]    reward: -1.6940, last reward: -0.0142, gradient norm:  0.6006:  96%|#########6| 600/625 [02:12<00:05,  4.62it/s]    reward: -1.9512, last reward: -0.0125, gradient norm:  0.5803:  96%|#########6| 600/625 [02:12<00:05,  4.62it/s]    reward: -1.9512, last reward: -0.0125, gradient norm:  0.5803:  96%|#########6| 601/625 [02:12<00:05,  4.61it/s]    reward: -1.4805, last reward: -0.0076, gradient norm:  0.3985:  96%|#########6| 601/625 [02:12<00:05,  4.61it/s]    reward: -1.4805, last reward: -0.0076, gradient norm:  0.3985:  96%|#########6| 602/625 [02:12<00:04,  4.62it/s]    reward: -1.5613, last reward: -0.0021, gradient norm:  0.2526:  96%|#########6| 602/625 [02:12<00:04,  4.62it/s]    reward: -1.5613, last reward: -0.0021, gradient norm:  0.2526:  96%|#########6| 603/625 [02:12<00:04,  4.62it/s]    reward: -2.2808, last reward: -0.0001, gradient norm:  0.4755:  96%|#########6| 603/625 [02:13<00:04,  4.62it/s]    reward: -2.2808, last reward: -0.0001, gradient norm:  0.4755:  97%|#########6| 604/625 [02:13<00:04,  4.61it/s]    reward: -1.8463, last reward: -0.0007, gradient norm:  0.2842:  97%|#########6| 604/625 [02:13<00:04,  4.61it/s]    reward: -1.8463, last reward: -0.0007, gradient norm:  0.2842:  97%|#########6| 605/625 [02:13<00:04,  4.62it/s]    reward: -2.2937, last reward: -0.0027, gradient norm:  0.3396:  97%|#########6| 605/625 [02:13<00:04,  4.62it/s]    reward: -2.2937, last reward: -0.0027, gradient norm:  0.3396:  97%|#########6| 606/625 [02:13<00:04,  4.61it/s]    reward: -1.9633, last reward: -0.0046, gradient norm:  0.2741:  97%|#########6| 606/625 [02:13<00:04,  4.61it/s]    reward: -1.9633, last reward: -0.0046, gradient norm:  0.2741:  97%|#########7| 607/625 [02:13<00:03,  4.61it/s]    reward: -1.9763, last reward: -0.0061, gradient norm:  0.3327:  97%|#########7| 607/625 [02:13<00:03,  4.61it/s]    reward: -1.9763, last reward: -0.0061, gradient norm:  0.3327:  97%|#########7| 608/625 [02:13<00:03,  4.62it/s]    reward: -2.3215, last reward: -0.0055, gradient norm:  0.4658:  97%|#########7| 608/625 [02:14<00:03,  4.62it/s]    reward: -2.3215, last reward: -0.0055, gradient norm:  0.4658:  97%|#########7| 609/625 [02:14<00:03,  4.62it/s]    reward: -1.6501, last reward: -0.0080, gradient norm:  0.2626:  97%|#########7| 609/625 [02:14<00:03,  4.62it/s]    reward: -1.6501, last reward: -0.0080, gradient norm:  0.2626:  98%|#########7| 610/625 [02:14<00:03,  4.61it/s]    reward: -1.9696, last reward: -0.0017, gradient norm:  0.5173:  98%|#########7| 610/625 [02:14<00:03,  4.61it/s]    reward: -1.9696, last reward: -0.0017, gradient norm:  0.5173:  98%|#########7| 611/625 [02:14<00:03,  4.61it/s]    reward: -1.9731, last reward: -0.0005, gradient norm:  0.5793:  98%|#########7| 611/625 [02:14<00:03,  4.61it/s]    reward: -1.9731, last reward: -0.0005, gradient norm:  0.5793:  98%|#########7| 612/625 [02:14<00:02,  4.61it/s]    reward: -2.4196, last reward: -0.6237, gradient norm:  25.43:  98%|#########7| 612/625 [02:15<00:02,  4.61it/s]     reward: -2.4196, last reward: -0.6237, gradient norm:  25.43:  98%|#########8| 613/625 [02:15<00:02,  4.61it/s]    reward: -1.9212, last reward: -0.0141, gradient norm:  0.2519:  98%|#########8| 613/625 [02:15<00:02,  4.61it/s]    reward: -1.9212, last reward: -0.0141, gradient norm:  0.2519:  98%|#########8| 614/625 [02:15<00:02,  4.61it/s]    reward: -2.1131, last reward: -0.0090, gradient norm:  0.392:  98%|#########8| 614/625 [02:15<00:02,  4.61it/s]     reward: -2.1131, last reward: -0.0090, gradient norm:  0.392:  98%|#########8| 615/625 [02:15<00:02,  4.62it/s]    reward: -1.6106, last reward: -0.0080, gradient norm:  0.4802:  98%|#########8| 615/625 [02:15<00:02,  4.62it/s]    reward: -1.6106, last reward: -0.0080, gradient norm:  0.4802:  99%|#########8| 616/625 [02:15<00:01,  4.62it/s]    reward: -1.8127, last reward: -0.0049, gradient norm:  0.2904:  99%|#########8| 616/625 [02:15<00:01,  4.62it/s]    reward: -1.8127, last reward: -0.0049, gradient norm:  0.2904:  99%|#########8| 617/625 [02:15<00:01,  4.63it/s]    reward: -2.1906, last reward: -0.0015, gradient norm:  0.3571:  99%|#########8| 617/625 [02:16<00:01,  4.63it/s]    reward: -2.1906, last reward: -0.0015, gradient norm:  0.3571:  99%|#########8| 618/625 [02:16<00:01,  4.64it/s]    reward: -2.2302, last reward: -0.0530, gradient norm:  0.9727:  99%|#########8| 618/625 [02:16<00:01,  4.64it/s]    reward: -2.2302, last reward: -0.0530, gradient norm:  0.9727:  99%|#########9| 619/625 [02:16<00:01,  4.63it/s]    reward: -1.9984, last reward: -0.0009, gradient norm:  0.3939:  99%|#########9| 619/625 [02:16<00:01,  4.63it/s]    reward: -1.9984, last reward: -0.0009, gradient norm:  0.3939:  99%|#########9| 620/625 [02:16<00:01,  4.63it/s]    reward: -2.1301, last reward: -0.0037, gradient norm:  2.24:  99%|#########9| 620/625 [02:16<00:01,  4.63it/s]      reward: -2.1301, last reward: -0.0037, gradient norm:  2.24:  99%|#########9| 621/625 [02:16<00:00,  4.62it/s]    reward: -2.5129, last reward: -0.0066, gradient norm:  1.197:  99%|#########9| 621/625 [02:17<00:00,  4.62it/s]    reward: -2.5129, last reward: -0.0066, gradient norm:  1.197: 100%|#########9| 622/625 [02:17<00:00,  4.62it/s]    reward: -1.9250, last reward: -0.0005, gradient norm:  0.6589: 100%|#########9| 622/625 [02:17<00:00,  4.62it/s]    reward: -1.9250, last reward: -0.0005, gradient norm:  0.6589: 100%|#########9| 623/625 [02:17<00:00,  4.62it/s]    reward: -1.9425, last reward: -0.0007, gradient norm:  0.2548: 100%|#########9| 623/625 [02:17<00:00,  4.62it/s]    reward: -1.9425, last reward: -0.0007, gradient norm:  0.2548: 100%|#########9| 624/625 [02:17<00:00,  4.63it/s]    reward: -1.8481, last reward: -0.0033, gradient norm:  0.3058: 100%|#########9| 624/625 [02:17<00:00,  4.63it/s]    reward: -1.8481, last reward: -0.0033, gradient norm:  0.3058: 100%|##########| 625/625 [02:17<00:00,  4.63it/s]    reward: -1.8481, last reward: -0.0033, gradient norm:  0.3058: 100%|##########| 625/625 [02:17<00:00,  4.54it/s]




.. GENERATED FROM PYTHON SOURCE LINES 691-705

Conclusion
----------

In this tutorial, we have learned how to code a stateless environment from
scratch. We touched the subjects of:

* the four essential components that need to be taken care of when coding
  an environment (step, reset, seeding and building specs). We saw how these
  methods and classes interact with the :class:`tensordict.TensorDict` class;
* how to test that an environment is properly coded using
  :func:`torchrl.envs.utils.check_env_specs`;
* How to code transforms in the context of stateless environments;
* How to train a policy on a fully differentiable simulator.



.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 3 minutes  32.248 seconds)

**Estimated memory usage:**  10 MB


.. _sphx_glr_download_tutorials_pendulum.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: pendulum.py <pendulum.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: pendulum.ipynb <pendulum.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
