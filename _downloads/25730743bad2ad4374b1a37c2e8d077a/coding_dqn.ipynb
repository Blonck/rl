{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Coding a pixel-based DQN using TorchRL\n**Author**: [Vincent Moens](https://github.com/vmoens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This tutorial will guide you through the steps to code DQN to solve the\nCartPole task from scratch. DQN\n([Deep Q-Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)) was\nthe founding work in deep reinforcement learning.\nOn a high level, the algorithm is quite simple: Q-learning consists in learning a table of\nstate-action values in such a way that, when encountering any particular state,\nwe know which action to pick just by searching for the action with the\nhighest value. This simple setting requires the actions and states to be\ndiscrete, otherwise a lookup table cannot be built.\n\nDQN uses a neural network that encodes a map from the state-action space to\na value (scalar) space, which amortizes the cost of storing and exploring all\nthe possible state-action combinations: if a state has not been seen in the\npast, we can still pass it in conjunction with the various actions available\nthrough our neural network and get an interpolated value for each of the\nactions available.\n\nWe will solve the classic control problem of the cart pole. From the\nGymnasium doc from where this environment is retrieved:\n\n| A pole is attached by an un-actuated joint to a cart, which moves along a\n| frictionless track. The pendulum is placed upright on the cart and the goal\n| is to balance the pole by applying forces in the left and right direction\n| on the cart.\n\n.. figure:: /_static/img/cart_pole.gif\n   :alt: Cart Pole\n\n**Prerequisites**: We encourage you to get familiar with torchrl through the\n`PPO tutorial <https://pytorch.org/rl/tutorials/coding_ppo.html>` first.\nThis tutorial is more complex and full-fleshed, but it may be .\n\nIn this tutorial, you will learn:\n\n- how to build an environment in TorchRL, including transforms (e.g. data\n  normalization, frame concatenation, resizing and turning to grayscale)\n  and parallel execution. Unlike what we did in the\n  [DDPG tutorial](https://pytorch.org/rl/tutorials/coding_ddpg.html), we\n  will normalize the pixels and not the state vector.\n- how to design a QValue actor, i.e. an actor that estimates the action\n  values and picks up the action with the highest estimated return;\n- how to collect data from your environment efficiently and store them\n  in a replay buffer;\n- how to store trajectories (and not transitions) in your replay buffer),\n  and how to estimate returns using TD(lambda);\n- how to make a module functional and use ;\n- and finally how to evaluate your model.\n\nThis tutorial assumes the reader is familiar with some of TorchRL\nprimitives, such as :class:`tensordict.TensorDict` and\n:class:`tensordict.TensorDictModules`, although it\nshould be sufficiently transparent to be understood without a deep\nunderstanding of these classes.\n\nWe do not aim at giving a SOTA implementation of the algorithm, but rather\nto provide a high-level illustration of TorchRL features in the context\nof this algorithm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport tqdm\nfrom functorch import vmap\nfrom IPython import display\nfrom matplotlib import pyplot as plt\nfrom tensordict import TensorDict\nfrom tensordict.nn import get_functional\nfrom torch import nn\nfrom torchrl.collectors import MultiaSyncDataCollector\nfrom torchrl.data import LazyMemmapStorage, TensorDictReplayBuffer\nfrom torchrl.envs import EnvCreator, ParallelEnv\nfrom torchrl.envs.libs.gym import GymEnv\nfrom torchrl.envs.transforms import (\n    CatFrames,\n    CatTensors,\n    Compose,\n    GrayScale,\n    ObservationNorm,\n    Resize,\n    ToTensorImage,\n    TransformedEnv,\n)\nfrom torchrl.envs.utils import set_exploration_mode, step_mdp\nfrom torchrl.modules import DuelingCnnDQNet, EGreedyWrapper, QValueActor\n\n\ndef is_notebook() -> bool:\n    try:\n        shell = get_ipython().__class__.__name__\n        if shell == \"ZMQInteractiveShell\":\n            return True  # Jupyter notebook or qtconsole\n        elif shell == \"TerminalInteractiveShell\":\n            return False  # Terminal running IPython\n        else:\n            return False  # Other type (?)\n    except NameError:\n        return False  # Probably standard Python interpreter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters\n\nLet's start with our hyperparameters. The following setting should work well\nin practice, and the performance of the algorithm should hopefully not be\ntoo sensitive to slight variations of these.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = \"cuda:0\" if torch.cuda.device_count() > 0 else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimizer\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# the learning rate of the optimizer\nlr = 2e-3\n# the beta parameters of Adam\nbetas = (0.9, 0.999)\n# Optimization steps per batch collected (aka UPD or updates per data)\nn_optim = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DQN parameters\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# gamma decay factor\ngamma = 0.99\n# lambda decay factor (see second the part with TD(lambda)\nlmbda = 0.95\n\n# Smooth target network update decay parameter. This loosely corresponds to a 1/(1-tau) interval with hard target network update\ntau = 0.005"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data collection and replay buffer\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# total frames collected in the environment. In other implementations, the user defines a maximum number of episodes.\n# This is harder to do with our data collectors since they return batches of N collected frames, where N is a constant.\n# However, one can easily get the same restriction on number of episodes by breaking the training loop when a certain number\n# episodes has been collected.\ntotal_frames = 500\n# Random frames used to initialize the replay buffer.\ninit_random_frames = 100\n# Frames in each batch collected.\nframes_per_batch = 32\n# Frames sampled from the replay buffer at each optimization step\nbatch_size = 32\n# Size of the replay buffer in terms of frames\nbuffer_size = min(total_frames, 100000)\n# Number of environments run in parallel in each data collector\nn_workers = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment and exploration\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Initial and final value of the epsilon factor in Epsilon-greedy exploration (notice that since our policy is deterministic exploration is crucial)\neps_greedy_val = 0.1\neps_greedy_val_env = 0.05\n\n# To speed up learning, we set the bias of the last layer of our value network\n# to a predefined value (this is not mandatory)\ninit_bias = 20.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note**: for fast rendering of the tutorial ``total_frames`` hyperparameter\nwas set to a very low number. To get a reasonable performance, use a greater\nvalue e.g. 500000\n\n## Building the environment\n\nOur environment builder has two arguments:\n\n- ``parallel``: determines whether multiple environments have to be run in\n  parallel. We stack the transforms after the\n  :class:`torchrl.envs.ParallelEnv` to take advantage\n  of vectorization of the operations on device, although this would\n  technically work with every single environment attached to its own set of\n  transforms.\n- ``observation_norm_state_dict`` will contain the normalizing constants for\n  the :class:`torchrl.envs.ObservationNorm` tranform.\n\nWe will be using five transforms:\n\n- :class:`torchrl.envs.ToTensorImage` will convert a ``[W, H, C]`` uint8\n  tensor in a floating point tensor in the ``[0, 1]`` space with shape\n  ``[C, W, H]``;\n- :class:`torchrl.envs.GrayScale` will turn our image into grayscale;\n- :class:`torchrl.envs.Resize` will resize the image in a 64x64 format;\n- :class:`torchrl.envs.CatFrames` will concatenate an arbitrary number of\n  successive frames (``N=4``) in a single tensor along the channel dimension.\n  This is useful as a single image does not carry information about the\n  motion of the cartpole. Some memory about past observations and actions\n  is needed, either via a recurrent neural network or using a stack of\n  frames.\n- :class:`torchrl.envs.ObservationNorm` which will normalize our observations\n  given some custom summary statistics.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_env(parallel=False, observation_norm_state_dict=None):\n    if observation_norm_state_dict is None:\n        observation_norm_state_dict = {\"standard_normal\": True}\n    if parallel:\n        base_env = ParallelEnv(\n            n_workers,\n            EnvCreator(\n                lambda: GymEnv(\n                    \"CartPole-v1\", from_pixels=True, pixels_only=True, device=device\n                )\n            ),\n        )\n    else:\n        base_env = GymEnv(\n            \"CartPole-v1\", from_pixels=True, pixels_only=True, device=device\n        )\n\n    env = TransformedEnv(\n        base_env,\n        Compose(\n            ToTensorImage(),\n            GrayScale(),\n            Resize(64, 64),\n            CatFrames(4, in_keys=[\"pixels\"], dim=-3),\n            ObservationNorm(in_keys=[\"pixels\"], **observation_norm_state_dict),\n        ),\n    )\n    return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compute normalizing constants\n\nTo normalize images, we don't want to normalize each pixel independently\nwith a full ``[C, W, H]`` normalizing mask, but with simpler ``[C, 1, 1]``\nshaped loc and scale parameters. We will be using the ``reduce_dim`` argument\nof :func:`torchrl.envs.ObservationNorm.init_stats` to instruct which\ndimensions must be reduced, and the ``keep_dims`` parameter to ensure that\nnot all dimensions disappear in the process:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dummy_env = make_env()\ndummy_env.transform[-1].init_stats(num_iter=1000, cat_dim=0, reduce_dim=[-1, -2, -4], keep_dims=(-1, -2))\nobservation_norm_state_dict = dummy_env.transform[-1].state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "let's check that normalizing constants have a size of ``[C, 1, 1]`` where\n``C=4`` (because of :class:`torchrl.envs.CatFrames`).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(observation_norm_state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building the model (Deep Q-network)\n\nThe following function builds a :class:`torchrl.modules.DuelingCnnDQNet`\nobject which is a simple CNN followed by a two-layer MLP. The only trick used\nhere is that the action values (i.e. left and right action value) are\ncomputed using\n\n\\begin{align}val = b(obs) + v(obs) - E[v(obs)]\\end{align}\n\nwhere $b$ is a $num_obs \\rightarrow 1$ function and $v$ is a\n$num_obs \\rightarrow num_actions$ function.\n\nOur network is wrapped in a :class:`torchrl.modules.QValueActor`, which will read the state-action\nvalues, pick up the one with the maximum value and write all those results\nin the input :class:`tensordict.TensorDict`.\n\n### Target parameters\n\nMany off-policy RL algorithms use the concept of \"target parameters\" when it\ncomes to estimate the value of the ``t+1`` state or state-action pair.\nThe target parameters are lagged copies of the model parameters. Because\ntheir predictions mismatch those of the current model configuration, they\nhelp learning by putting a pessimistic bound on the value being estimated.\nThis is a powerful trick (known as \"Double Q-Learning\") that is ubiquitous\nin similar algorithms.\n\n### Functionalizing modules\n\nOne of the features of torchrl is its usage of functional modules: as the\nsame architecture is often used with multiple sets of parameters (e.g.\ntrainable and target parameters), we functionalize the modules and isolate\nthe various sets of parameters in separate tensordicts.\n\nTo this aim, we use :func:`tensordict.nn.get_functional`, which augments\nour modules with some extra feature that make them compatible with parameters\npassed in the ``TensorDict`` format.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_model(dummy_env):\n    cnn_kwargs = {\n        \"num_cells\": [32, 64, 64],\n        \"kernel_sizes\": [6, 4, 3],\n        \"strides\": [2, 2, 1],\n        \"activation_class\": nn.ELU,\n        \"squeeze_output\": True,\n        \"aggregator_class\": nn.AdaptiveAvgPool2d,\n        \"aggregator_kwargs\": {\"output_size\": (1, 1)},\n    }\n    mlp_kwargs = {\n        \"depth\": 2,\n        \"num_cells\": [\n            64,\n            64,\n        ],\n        \"activation_class\": nn.ELU,\n    }\n    net = DuelingCnnDQNet(\n        dummy_env.action_spec.shape[-1], 1, cnn_kwargs, mlp_kwargs\n    ).to(device)\n    net.value[-1].bias.data.fill_(init_bias)\n\n    actor = QValueActor(net, in_keys=[\"pixels\"], spec=dummy_env.action_spec).to(device)\n    # init actor: because the model is composed of lazy conv/linear layers,\n    # we must pass a fake batch of data through it to instantiate them.\n    tensordict = dummy_env.fake_tensordict()\n    actor(tensordict)\n\n    # Make functional:\n    # here's an explicit way of creating the parameters and buffer tensordict.\n    # Alternatively, we could have used `params = make_functional(actor)` from\n    # tensordict.nn\n    params = TensorDict({k: v for k, v in actor.named_parameters()}, [])\n    buffers = TensorDict({k: v for k, v in actor.named_buffers()}, [])\n    params = params.update(buffers)\n    params = params.unflatten_keys(\".\")  # creates a nested TensorDict\n    factor = get_functional(actor)\n\n    # creating the target parameters is fairly easy with tensordict:\n    params_target = params.clone().detach()\n\n    # we wrap our actor in an EGreedyWrapper for data collection\n    actor_explore = EGreedyWrapper(\n        actor,\n        annealing_num_steps=total_frames,\n        eps_init=eps_greedy_val,\n        eps_end=eps_greedy_val_env,\n    )\n\n    return factor, actor, actor_explore, params, params_target\n\n\n(\n    factor,\n    actor,\n    actor_explore,\n    params,\n    params_target,\n) = make_model(dummy_env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We represent the parameters and targets as flat structures, but unflattening\nthem is quite easy:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "params_flat = params.flatten_keys(\".\")\nparams_target_flat = params_target.flatten_keys(\".\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regular DQN\n\nWe'll start with a simple implementation of DQN where the returns are\ncomputed without bootstrapping, i.e.\n\n  return = reward + gamma * value_next_step * not_terminated\n\nWe start with the *replay buffer*. We'll use a regular replay buffer,\nalthough a prioritized RB could improve the performance significantly.\nWe place the storage on disk using ``LazyMemmapStorage``. The only requirement\nof this storage is that the data given to it must always have the same\nshape. This storage will be instantiated later.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "replay_buffer = TensorDictReplayBuffer(\n    storage=LazyMemmapStorage(buffer_size),\n    prefetch=n_optim,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our *data collector* will run two parallel environments in parallel, and\ndeliver the collected tensordicts once at a time to the main process. We'll\nuse the ``MultiaSyncDataCollector`` collector, which will collect data while\nthe optimization is taking place.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_collector = MultiaSyncDataCollector(\n    [\n        make_env(parallel=True, observation_norm_state_dict=observation_norm_state_dict),\n        make_env(parallel=True, observation_norm_state_dict=observation_norm_state_dict),\n    ],  # 2 collectors, each with an set of `num_workers` environments being run in parallel\n    policy=actor_explore,\n    frames_per_batch=frames_per_batch,\n    total_frames=total_frames,\n    exploration_mode=\"random\",  # this is the default behaviour: the collector runs in `\"random\"` (or explorative) mode\n    devices=[device, device],  # each collector can sit on a different device\n    storing_devices=[device, device],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our *optimizer* and the env used for evaluation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optim = torch.optim.Adam(list(params_flat.values()), lr)\ndummy_env = make_env(parallel=False, observation_norm_state_dict=observation_norm_state_dict)\nprint(actor_explore(dummy_env.reset()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Various lists that will contain the values recorded for evaluation:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "evals = []\ntraj_lengths_eval = []\nlosses = []\nframes = []\nvalues = []\ngrad_vals = []\ntraj_lengths = []\nmavgs = []\ntraj_count = []\nprev_traj_count = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training loop\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pbar = tqdm.tqdm(total=total_frames)\nfor j, data in enumerate(data_collector):\n    # trajectories are padded to be stored in the same tensordict: since we do not care about consecutive step, we'll just mask the tensordict and get the flattened representation instead.\n    mask = data[\"collector\", \"mask\"]\n    current_frames = mask.sum().cpu().item()\n    pbar.update(current_frames)\n\n    # We store the values on the replay buffer, after placing them on CPU. When called for the first time, this will instantiate our storage object which will print its content.\n    replay_buffer.extend(data[mask].cpu())\n\n    # some logging\n    if len(frames):\n        frames.append(current_frames + frames[-1])\n    else:\n        frames.append(current_frames)\n\n    if data[\"done\"].any():\n        done = data[\"done\"].squeeze(-1)\n        traj_lengths.append(data[\"collector\", \"step_count\"][done].float().mean().item())\n\n    # check that we have enough data to start training\n    if sum(frames) > init_random_frames:\n        for _ in range(n_optim):\n            # sample from the RB and send to device\n            sampled_data = replay_buffer.sample(batch_size)\n            sampled_data = sampled_data.to(device, non_blocking=True)\n\n            # collect data from RB\n            reward = sampled_data[\"reward\"].squeeze(-1)\n            done = sampled_data[\"done\"].squeeze(-1).to(reward.dtype)\n            action = sampled_data[\"action\"].clone()\n\n            # Compute action value (of the action actually taken) at time t\n            sampled_data_out = sampled_data.select(*actor.in_keys)\n            sampled_data_out = factor(sampled_data_out, params=params)\n            action_value = sampled_data_out[\"action_value\"]\n            action_value = (action_value * action.to(action_value.dtype)).sum(-1)\n            with torch.no_grad():\n                # compute best action value for the next step, using target parameters\n                tdstep = step_mdp(sampled_data)\n                next_value = factor(\n                    tdstep.select(*actor.in_keys),\n                    params=params_target,\n                )[\"chosen_action_value\"].squeeze(-1)\n                exp_value = reward + gamma * next_value * (1 - done)\n            assert exp_value.shape == action_value.shape\n            # we use MSE loss but L1 or smooth L1 should also work\n            error = nn.functional.mse_loss(exp_value, action_value).mean()\n            error.backward()\n\n            gv = sum([p.grad.pow(2).sum() for p in params_flat.values()]).sqrt()\n            nn.utils.clip_grad_value_(list(params_flat.values()), 1)\n\n            optim.step()\n            optim.zero_grad()\n\n            # update of the target parameters\n            for (key, p1) in params_flat.items():\n                p2 = params_target_flat[key]\n                params_target_flat.set_(key, tau * p1.data + (1 - tau) * p2.data)\n\n        pbar.set_description(\n            f\"error: {error: 4.4f}, value: {action_value.mean(): 4.4f}\"\n        )\n        actor_explore.step(current_frames)\n\n        # logs\n        with set_exploration_mode(\"mode\"), torch.no_grad():\n            # execute a rollout. The `set_exploration_mode(\"mode\")` has no effect here since the policy is deterministic, but we add it for completeness\n            eval_rollout = dummy_env.rollout(max_steps=10000, policy=actor).cpu()\n        grad_vals.append(float(gv))\n        traj_lengths_eval.append(eval_rollout.shape[-1])\n        evals.append(eval_rollout[\"reward\"].squeeze(-1).sum(-1).item())\n        if len(mavgs):\n            mavgs.append(evals[-1] * 0.05 + mavgs[-1] * 0.95)\n        else:\n            mavgs.append(evals[-1])\n        losses.append(error.item())\n        values.append(action_value.mean().item())\n        traj_count.append(prev_traj_count + data[\"done\"].sum().item())\n        prev_traj_count = traj_count[-1]\n        # plots\n        if j % 10 == 0:\n            if is_notebook():\n                display.clear_output(wait=True)\n                display.display(plt.gcf())\n            else:\n                plt.clf()\n            plt.figure(figsize=(15, 15))\n            plt.subplot(3, 2, 1)\n            plt.plot(frames[-len(evals) :], evals, label=\"return\")\n            plt.plot(frames[-len(mavgs) :], mavgs, label=\"mavg\")\n            plt.xlabel(\"frames collected\")\n            plt.ylabel(\"trajectory length (= return)\")\n            plt.subplot(3, 2, 2)\n            plt.plot(traj_count[-len(evals) :], evals, label=\"return\")\n            plt.plot(traj_count[-len(mavgs) :], mavgs, label=\"mavg\")\n            plt.xlabel(\"trajectories collected\")\n            plt.legend()\n            plt.subplot(3, 2, 3)\n            plt.plot(frames[-len(losses) :], losses)\n            plt.xlabel(\"frames collected\")\n            plt.title(\"loss\")\n            plt.subplot(3, 2, 4)\n            plt.plot(frames[-len(values) :], values)\n            plt.xlabel(\"frames collected\")\n            plt.title(\"value\")\n            plt.subplot(3, 2, 5)\n            plt.plot(frames[-len(grad_vals) :], grad_vals)\n            plt.xlabel(\"frames collected\")\n            plt.title(\"grad norm\")\n            if len(traj_lengths):\n                plt.subplot(3, 2, 6)\n                plt.plot(traj_lengths)\n                plt.xlabel(\"batches\")\n                plt.title(\"traj length (training)\")\n        plt.savefig(\"dqn_td0.png\")\n        if is_notebook():\n            plt.show()\n\n    # update policy weights\n    data_collector.update_policy_weights_()\n\nprint(\"shutting down\")\ndata_collector.shutdown()\ndel data_collector\n\nif is_notebook():\n    display.clear_output(wait=True)\n    display.display(plt.gcf())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note**: As already mentioned above, to get a more reasonable performance,\nuse a greater value for ``total_frames`` e.g. 500000.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 15))\nplt.imshow(plt.imread(\"dqn_td0.png\"))\nplt.tight_layout()\nplt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# save results\ntorch.save(\n    {\n        \"frames\": frames,\n        \"evals\": evals,\n        \"mavgs\": mavgs,\n        \"losses\": losses,\n        \"values\": values,\n        \"grad_vals\": grad_vals,\n        \"traj_lengths_training\": traj_lengths,\n        \"traj_count\": traj_count,\n        \"weights\": (params,),\n    },\n    \"saved_results_td0.pt\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TD-lambda\nWe can improve the above algorithm by getting a better estimate of the\nreturn, using not only the next state value but the whole sequence of rewards\nand values that follow a particular step.\n\nTorchRL provides a vectorized version of TD(lambda) named\n``vec_td_lambda_advantage_estimate``. We'll use this to obtain a target\nvalue that the value network will be trained to match.\n\nThe big difference in this implementation is that we'll store entire\ntrajectories and not single steps in the replay buffer. This will be done\nautomatically as long as we're not \"flattening\" the tensordict collected\nusing its mask: by keeping a shape ``[Batch x timesteps]`` and giving this\nto the RB, we'll be creating a replay buffer of size\n``[Capacity x timesteps]``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensordict.tensordict import pad\nfrom torchrl.objectives.value.functional import vec_td_lambda_advantage_estimate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We reset the actor, the RB and the collector\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "(\n    factor,\n    actor,\n    actor_explore,\n    params,\n    params_target,\n) = make_model(dummy_env)\nparams_flat = params.flatten_keys(\".\")\nparams_target_flat = params_target.flatten_keys(\".\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "max_size = frames_per_batch // n_workers\n\nreplay_buffer = TensorDictReplayBuffer(\n    storage=LazyMemmapStorage(-(-buffer_size // max_size)),\n    prefetch=n_optim,\n)\n\ndata_collector = MultiaSyncDataCollector(\n    [make_env(parallel=True, observation_norm_state_dict=observation_norm_state_dict), make_env(parallel=True, observation_norm_state_dict=observation_norm_state_dict)],\n    policy=actor_explore,\n    frames_per_batch=frames_per_batch,\n    total_frames=total_frames,\n    exploration_mode=\"random\",\n    devices=[device, device],\n    storing_devices=[device, device],\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optim = torch.optim.Adam(list(params_flat.values()), lr)\ndummy_env = make_env(parallel=False, observation_norm_state_dict=observation_norm_state_dict)\nprint(actor_explore(dummy_env.reset()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "evals = []\ntraj_lengths_eval = []\nlosses = []\nframes = []\nvalues = []\ngrad_vals = []\ntraj_lengths = []\nmavgs = []\ntraj_count = []\nprev_traj_count = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training loop\nThere are very few differences with the training loop above:\n\n- The tensordict received by the collector is not masked but padded to the\n  desired shape (such that all tensordicts have the same shape of\n  ``[Batch x max_size]``), and sent directly to the RB.\n- We use ``vec_td_lambda_advantage_estimate`` to compute the target value.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pbar = tqdm.tqdm(total=total_frames)\nfor j, data in enumerate(data_collector):\n    mask = data[\"collector\", \"mask\"]\n    data = pad(data, [0, 0, 0, max_size - data.shape[1]])\n    current_frames = mask.sum().cpu().item()\n    pbar.update(current_frames)\n\n    replay_buffer.extend(data.cpu())\n    if len(frames):\n        frames.append(current_frames + frames[-1])\n    else:\n        frames.append(current_frames)\n\n    if data[\"done\"].any():\n        done = data[\"done\"].squeeze(-1)\n        traj_lengths.append(data[\"collector\", \"step_count\"][done].float().mean().item())\n\n    if sum(frames) > init_random_frames:\n        for _ in range(n_optim):\n            sampled_data = replay_buffer.sample(batch_size // max_size)\n            sampled_data = sampled_data.clone().to(device, non_blocking=True)\n\n            reward = sampled_data[\"reward\"]\n            done = sampled_data[\"done\"].to(reward.dtype)\n            action = sampled_data[\"action\"].clone()\n\n            sampled_data_out = sampled_data.select(*actor.in_keys)\n            sampled_data_out = vmap(factor, (0, None))(sampled_data_out, params)\n            action_value = sampled_data_out[\"action_value\"]\n            action_value = (action_value * action.to(action_value.dtype)).sum(-1, True)\n            with torch.no_grad():\n                tdstep = step_mdp(sampled_data)\n                next_value = vmap(factor, (0, None))(\n                    tdstep.select(*actor.in_keys), params\n                )\n                next_value = next_value[\"chosen_action_value\"]\n            error = vec_td_lambda_advantage_estimate(\n                gamma,\n                lmbda,\n                action_value,\n                next_value,\n                reward,\n                done,\n            ).pow(2)\n            # reward + gamma * next_value * (1 - done)\n            mask = sampled_data[\"collector\", \"mask\"]\n            error = error[mask].mean()\n            # assert exp_value.shape == action_value.shape\n            # error = nn.functional.smooth_l1_loss(exp_value, action_value).mean()\n            # error = nn.functional.mse_loss(exp_value, action_value)[mask].mean()\n            error.backward()\n\n            # gv = sum([p.grad.pow(2).sum() for p in params_flat.values()]).sqrt()\n            # nn.utils.clip_grad_value_(list(params_flat.values()), 1)\n            gv = nn.utils.clip_grad_norm_(list(params_flat.values()), 100)\n\n            optim.step()\n            optim.zero_grad()\n\n            for (key, p1) in params_flat.items():\n                p2 = params_target_flat[key]\n                params_target_flat.set_(key, tau * p1.data + (1 - tau) * p2.data)\n\n        pbar.set_description(\n            f\"error: {error: 4.4f}, value: {action_value.mean(): 4.4f}\"\n        )\n        actor_explore.step(current_frames)\n\n        # logs\n        with set_exploration_mode(\"random\"), torch.no_grad():\n            #         eval_rollout = dummy_env.rollout(max_steps=1000, policy=actor_explore, auto_reset=True).cpu()\n            eval_rollout = dummy_env.rollout(\n                max_steps=10000, policy=actor, auto_reset=True\n            ).cpu()\n        grad_vals.append(float(gv))\n        traj_lengths_eval.append(eval_rollout.shape[-1])\n        evals.append(eval_rollout[\"reward\"].squeeze(-1).sum(-1).item())\n        if len(mavgs):\n            mavgs.append(evals[-1] * 0.05 + mavgs[-1] * 0.95)\n        else:\n            mavgs.append(evals[-1])\n        losses.append(error.item())\n        values.append(action_value[mask].mean().item())\n        traj_count.append(prev_traj_count + data[\"done\"].sum().item())\n        prev_traj_count = traj_count[-1]\n        # plots\n        if j % 10 == 0:\n            if is_notebook():\n                display.clear_output(wait=True)\n                display.display(plt.gcf())\n            else:\n                plt.clf()\n            plt.figure(figsize=(15, 15))\n            plt.subplot(3, 2, 1)\n            plt.plot(frames[-len(evals) :], evals, label=\"return\")\n            plt.plot(frames[-len(mavgs) :], mavgs, label=\"mavg\")\n            plt.xlabel(\"frames collected\")\n            plt.ylabel(\"trajectory length (= return)\")\n            plt.subplot(3, 2, 2)\n            plt.plot(traj_count[-len(evals) :], evals, label=\"return\")\n            plt.plot(traj_count[-len(mavgs) :], mavgs, label=\"mavg\")\n            plt.xlabel(\"trajectories collected\")\n            plt.legend()\n            plt.subplot(3, 2, 3)\n            plt.plot(frames[-len(losses) :], losses)\n            plt.xlabel(\"frames collected\")\n            plt.title(\"loss\")\n            plt.subplot(3, 2, 4)\n            plt.plot(frames[-len(values) :], values)\n            plt.xlabel(\"frames collected\")\n            plt.title(\"value\")\n            plt.subplot(3, 2, 5)\n            plt.plot(frames[-len(grad_vals) :], grad_vals)\n            plt.xlabel(\"frames collected\")\n            plt.title(\"grad norm\")\n            if len(traj_lengths):\n                plt.subplot(3, 2, 6)\n                plt.plot(traj_lengths)\n                plt.xlabel(\"batches\")\n                plt.title(\"traj length (training)\")\n        plt.savefig(\"dqn_tdlambda.png\")\n        if is_notebook():\n            plt.show()\n\n    # update policy weights\n    data_collector.update_policy_weights_()\n\nprint(\"shutting down\")\ndata_collector.shutdown()\ndel data_collector\n\nif is_notebook():\n    display.clear_output(wait=True)\n    display.display(plt.gcf())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note**: As already mentioned above, to get a more reasonable performance,\nuse a greater value for ``total_frames`` e.g. 500000.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 15))\nplt.imshow(plt.imread(\"dqn_tdlambda.png\"))\nplt.tight_layout()\nplt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# save results\ntorch.save(\n    {\n        \"frames\": frames,\n        \"evals\": evals,\n        \"mavgs\": mavgs,\n        \"losses\": losses,\n        \"values\": values,\n        \"grad_vals\": grad_vals,\n        \"traj_lengths_training\": traj_lengths,\n        \"traj_count\": traj_count,\n        \"weights\": (params,),\n    },\n    \"saved_results_tdlambda.pt\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's compare the results on a single plot. Because the TD(lambda) version\nworks better, we'll have fewer episodes collected for a given number of\nframes (as there are more frames per episode).\n\n**Note**: As already mentioned above, to get a more reasonable performance,\nuse a greater value for ``total_frames`` e.g. 500000.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "load_td0 = torch.load(\"saved_results_td0.pt\")\nload_tdlambda = torch.load(\"saved_results_tdlambda.pt\")\nframes_td0 = load_td0[\"frames\"]\nframes_tdlambda = load_tdlambda[\"frames\"]\nevals_td0 = load_td0[\"evals\"]\nevals_tdlambda = load_tdlambda[\"evals\"]\nmavgs_td0 = load_td0[\"mavgs\"]\nmavgs_tdlambda = load_tdlambda[\"mavgs\"]\nlosses_td0 = load_td0[\"losses\"]\nlosses_tdlambda = load_tdlambda[\"losses\"]\nvalues_td0 = load_td0[\"values\"]\nvalues_tdlambda = load_tdlambda[\"values\"]\ngrad_vals_td0 = load_td0[\"grad_vals\"]\ngrad_vals_tdlambda = load_tdlambda[\"grad_vals\"]\ntraj_lengths_td0 = load_td0[\"traj_lengths_training\"]\ntraj_lengths_tdlambda = load_tdlambda[\"traj_lengths_training\"]\ntraj_count_td0 = load_td0[\"traj_count\"]\ntraj_count_tdlambda = load_tdlambda[\"traj_count\"]\n\nplt.figure(figsize=(15, 15))\nplt.subplot(3, 2, 1)\nplt.plot(frames[-len(evals_td0) :], evals_td0, label=\"return (td0)\", alpha=0.5)\nplt.plot(\n    frames[-len(evals_tdlambda) :],\n    evals_tdlambda,\n    label=\"return (td(lambda))\",\n    alpha=0.5,\n)\nplt.plot(frames[-len(mavgs_td0) :], mavgs_td0, label=\"mavg (td0)\")\nplt.plot(frames[-len(mavgs_tdlambda) :], mavgs_tdlambda, label=\"mavg (td(lambda))\")\nplt.xlabel(\"frames collected\")\nplt.ylabel(\"trajectory length (= return)\")\nplt.subplot(3, 2, 2)\nplt.plot(traj_count_td0[-len(evals_td0) :], evals_td0, label=\"return (td0)\", alpha=0.5)\nplt.plot(\n    traj_count_tdlambda[-len(evals_tdlambda) :],\n    evals_tdlambda,\n    label=\"return (td(lambda))\",\n    alpha=0.5,\n)\nplt.plot(traj_count_td0[-len(mavgs_td0) :], mavgs_td0, label=\"mavg (td0)\")\nplt.plot(\n    traj_count_tdlambda[-len(mavgs_tdlambda) :],\n    mavgs_tdlambda,\n    label=\"mavg (td(lambda))\",\n)\nplt.xlabel(\"trajectories collected\")\nplt.legend()\nplt.subplot(3, 2, 3)\nplt.plot(frames[-len(losses_td0) :], losses_td0, label=\"loss (td0)\")\nplt.plot(frames[-len(losses_tdlambda) :], losses_tdlambda, label=\"loss (td(lambda))\")\nplt.xlabel(\"frames collected\")\nplt.title(\"loss\")\nplt.legend()\nplt.subplot(3, 2, 4)\nplt.plot(frames[-len(values_td0) :], values_td0, label=\"values (td0)\")\nplt.plot(frames[-len(values_tdlambda) :], values_tdlambda, label=\"values (td(lambda))\")\nplt.xlabel(\"frames collected\")\nplt.title(\"value\")\nplt.legend()\nplt.subplot(3, 2, 5)\nplt.plot(frames[-len(grad_vals_td0) :], grad_vals_td0, label=\"gradient norm (td0)\")\nplt.plot(\n    frames[-len(grad_vals_tdlambda) :],\n    grad_vals_tdlambda,\n    label=\"gradient norm (td(lambda))\",\n)\nplt.xlabel(\"frames collected\")\nplt.title(\"grad norm\")\nplt.legend()\nif len(traj_lengths):\n    plt.subplot(3, 2, 6)\n    plt.plot(traj_lengths_td0, label=\"episode length (td0)\")\n    plt.plot(traj_lengths_tdlambda, label=\"episode length (td(lambda))\")\n    plt.xlabel(\"batches\")\n    plt.legend()\n    plt.title(\"episode length (training)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we generate a new video to check what the algorithm has learnt.\nIf all goes well, the duration should be significantly longer than with the\ninitial, random rollout.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dummy_env.transform.insert(0, CatTensors([\"pixels\"], \"pixels_save\", del_keys=False))\neval_rollout = dummy_env.rollout(max_steps=10000, policy=actor, auto_reset=True).cpu()\nprint(eval_rollout)\ndel dummy_env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# imageio.mimwrite('cartpole.mp4', eval_rollout[\"pixels_save\"].numpy(), fps=30);\n# Video('cartpole.mp4', width=480, height=360) #the width and height option as additional thing new in Ipython 7.6.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion and possible improvements\nWe have seen that using TD(lambda) greatly improved the performance of our\nalgorithm. Other possible improvements could include:\n\n- Using the Multi-Step post-processing. Multi-step will project an action\n  to the nth following step, and create a discounted sum of the rewards in\n  between. This trick can make the algorithm noticebly less myopic. To use\n  this, simply create the collector with\n\n      from torchrl.data.postprocs.postprocs import MultiStep\n      collector = CollectorClass(..., postproc=MultiStep(gamma, n))\n\n  where ``n`` is the number of looking-forward steps. Pay attention to the\n  fact that the ``gamma`` factor has to be corrected by the number of\n  steps till the next observation when being passed to\n  ``vec_td_lambda_advantage_estimate``:\n\n      gamma = gamma ** tensordict[\"steps_to_next_obs\"]\n- A prioritized replay buffer could also be used. This will give a\n  higher priority to samples that have the worst value accuracy.\n- A distributional loss (see ``torchrl.objectives.DistributionalDQNLoss``\n  for more information).\n- More fancy exploration techniques, such as NoisyLinear layers and such\n  (check ``torchrl.modules.NoisyLinear``, which is fully compatible with the\n  ``MLP`` class used in our Dueling DQN).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}