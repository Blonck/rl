


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Coding DDPG using TorchRL &mdash; torchrl main documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Coding a pixel-based DQN using TorchRL" href="coding_dqn.html" />
    <link rel="prev" title="Task-specific policy in multi-task environments" href="multi_task.html" />
  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  main (None )
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="coding_ppo.html">Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="pendulum.html">Pendulum: Writing your environment and transforms with TorchRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchrl_demo.html">Introduction to TorchRL</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="torch_envs.html">TorchRL envs</a></li>
<li class="toctree-l1"><a class="reference internal" href="pretrained_models.html">Using pretrained models</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="multi_task.html">Task-specific policy in multi-task environments</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Coding DDPG using TorchRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="coding_dqn.html">Coding a pixel-based DQN using TorchRL</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../reference/index.html">API Reference</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../reference/knowledge_base.html">Knowledge Base</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Coding DDPG using TorchRL</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/tutorials/coding_ddpg.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <div class="pytorch-call-to-action-links">
            <div id="tutorial-type">tutorials/coding_ddpg</div>

            <div id="google-colab-link">
              <img class="call-to-action-img" src="../_static/images/pytorch-colab.svg"/>
              <div class="call-to-action-desktop-view">Run in Google Colab</div>
              <div class="call-to-action-mobile-view">Colab</div>
            </div>
            <div id="download-notebook-link">
              <img class="call-to-action-notebook-img" src="../_static/images/pytorch-download.svg"/>
              <div class="call-to-action-desktop-view">Download Notebook</div>
              <div class="call-to-action-mobile-view">Notebook</div>
            </div>
            <div id="github-view-link">
              <img class="call-to-action-img" src="../_static/images/pytorch-github.svg"/>
              <div class="call-to-action-desktop-view">View on GitHub</div>
              <div class="call-to-action-mobile-view">GitHub</div>
            </div>
          </div>

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-tutorials-coding-ddpg-py"><span class="std std-ref">here</span></a>
to download the full example code</p>
</div>
<section class="sphx-glr-example-title" id="coding-ddpg-using-torchrl">
<span id="sphx-glr-tutorials-coding-ddpg-py"></span><h1>Coding DDPG using TorchRL<a class="headerlink" href="#coding-ddpg-using-torchrl" title="Permalink to this heading">¶</a></h1>
<p><strong>Author</strong>: <a class="reference external" href="https://github.com/vmoens">Vincent Moens</a></p>
<p>This tutorial will guide you through the steps to code DDPG from scratch.</p>
<p>DDPG (<a href="#id4"><span class="problematic" id="id5">`Deep Deterministic Policy Gradient &lt;https://arxiv.org/abs/1509.02971&gt;_`_</span></a>)
is a simple continuous control algorithm. It consists in learning a
parametric value function for an action-observation pair, and
then learning a policy that outputs actions that maximise this value
function given a certain observation.</p>
<p>This tutorial is more  than the PPO tutorial: it covers
multiple topics that were left aside. We strongly advise the reader to go
through the PPO tutorial first before trying out this one. The goal is to
show how flexible torchrl is when it comes to writing scripts that can cover
multiple use cases.</p>
<p>Key learnings:</p>
<ul class="simple">
<li><p>how to build an environment in TorchRL, including transforms
(e.g. data normalization) and parallel execution;</p></li>
<li><p>how to design a policy and value network;</p></li>
<li><p>how to collect data from your environment efficiently and store them
in a replay buffer;</p></li>
<li><p>how to store trajectories (and not transitions) in your replay buffer);</p></li>
<li><p>and finally how to evaluate your model.</p></li>
</ul>
<p>This tutorial assumes the reader is familiar with some of TorchRL primitives,
such as <a class="reference external" href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="(in tensordict vmain (0.0.2b+696ac1a ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">tensordict.TensorDict</span></code></a> and
<code class="xref py py-class docutils literal notranslate"><span class="pre">tensordict.nn.TensorDictModules</span></code>, although it should be
sufficiently transparent to be understood without a deep understanding of
these classes.</p>
<p>We do not aim at giving a SOTA implementation of the algorithm, but rather
to provide a high-level illustration of TorchRL features in the context of
this algorithm.</p>
<section id="imports">
<h2>Imports<a class="headerlink" href="#imports" title="Permalink to this heading">¶</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.cuda</span>
<span class="kn">import</span> <span class="nn">tqdm</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">tensordict.nn</span> <span class="kn">import</span> <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.nn.TensorDictModule.html#tensordict.nn.TensorDictModule" title="tensordict.nn.TensorDictModule" class="sphx-glr-backref-module-tensordict-nn sphx-glr-backref-type-py-class"><span class="n">TensorDictModule</span></a>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">torchrl.collectors</span> <span class="kn">import</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.collectors.collectors.MultiaSyncDataCollector.html#torchrl.collectors.collectors.MultiaSyncDataCollector" title="torchrl.collectors.collectors.MultiaSyncDataCollector" class="sphx-glr-backref-module-torchrl-collectors-collectors sphx-glr-backref-type-py-class"><span class="n">MultiaSyncDataCollector</span></a>
<span class="kn">from</span> <span class="nn">torchrl.data</span> <span class="kn">import</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.data.CompositeSpec.html#torchrl.data.CompositeSpec" title="torchrl.data.CompositeSpec" class="sphx-glr-backref-module-torchrl-data sphx-glr-backref-type-py-class"><span class="n">CompositeSpec</span></a><span class="p">,</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.data.TensorDictReplayBuffer.html#torchrl.data.TensorDictReplayBuffer" title="torchrl.data.TensorDictReplayBuffer" class="sphx-glr-backref-module-torchrl-data sphx-glr-backref-type-py-class"><span class="n">TensorDictReplayBuffer</span></a>
<span class="kn">from</span> <span class="nn">torchrl.data.postprocs</span> <span class="kn">import</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.data.MultiStep.html#torchrl.data.MultiStep" title="torchrl.data.MultiStep" class="sphx-glr-backref-module-torchrl-data sphx-glr-backref-type-py-class"><span class="n">MultiStep</span></a>
<span class="kn">from</span> <span class="nn">torchrl.data.replay_buffers.samplers</span> <span class="kn">import</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.data.replay_buffers.samplers.PrioritizedSampler.html#torchrl.data.replay_buffers.samplers.PrioritizedSampler" title="torchrl.data.replay_buffers.samplers.PrioritizedSampler" class="sphx-glr-backref-module-torchrl-data-replay_buffers-samplers sphx-glr-backref-type-py-class"><span class="n">PrioritizedSampler</span></a><span class="p">,</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.data.replay_buffers.samplers.RandomSampler.html#torchrl.data.replay_buffers.samplers.RandomSampler" title="torchrl.data.replay_buffers.samplers.RandomSampler" class="sphx-glr-backref-module-torchrl-data-replay_buffers-samplers sphx-glr-backref-type-py-class"><span class="n">RandomSampler</span></a>
<span class="kn">from</span> <span class="nn">torchrl.data.replay_buffers.storages</span> <span class="kn">import</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.data.replay_buffers.storages.LazyMemmapStorage.html#torchrl.data.replay_buffers.storages.LazyMemmapStorage" title="torchrl.data.replay_buffers.storages.LazyMemmapStorage" class="sphx-glr-backref-module-torchrl-data-replay_buffers-storages sphx-glr-backref-type-py-class"><span class="n">LazyMemmapStorage</span></a>
<span class="kn">from</span> <span class="nn">torchrl.envs</span> <span class="kn">import</span> <span class="p">(</span>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.CatTensors.html#torchrl.envs.transforms.CatTensors" title="torchrl.envs.transforms.transforms.CatTensors" class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class"><span class="n">CatTensors</span></a><span class="p">,</span>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.DoubleToFloat.html#torchrl.envs.transforms.DoubleToFloat" title="torchrl.envs.transforms.transforms.DoubleToFloat" class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class"><span class="n">DoubleToFloat</span></a><span class="p">,</span>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.EnvCreator.html#torchrl.envs.EnvCreator" title="torchrl.envs.EnvCreator" class="sphx-glr-backref-module-torchrl-envs sphx-glr-backref-type-py-class"><span class="n">EnvCreator</span></a><span class="p">,</span>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.ObservationNorm.html#torchrl.envs.transforms.ObservationNorm" title="torchrl.envs.transforms.transforms.ObservationNorm" class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class"><span class="n">ObservationNorm</span></a><span class="p">,</span>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.ParallelEnv.html#torchrl.envs.ParallelEnv" title="torchrl.envs.ParallelEnv" class="sphx-glr-backref-module-torchrl-envs sphx-glr-backref-type-py-class"><span class="n">ParallelEnv</span></a><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torchrl.envs.libs.dm_control</span> <span class="kn">import</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.libs.dm_control.DMControlEnv.html#torchrl.envs.libs.dm_control.DMControlEnv" title="torchrl.envs.libs.dm_control.DMControlEnv" class="sphx-glr-backref-module-torchrl-envs-libs-dm_control sphx-glr-backref-type-py-function"><span class="n">DMControlEnv</span></a>
<span class="kn">from</span> <span class="nn">torchrl.envs.libs.gym</span> <span class="kn">import</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.libs.gym.GymEnv.html#torchrl.envs.libs.gym.GymEnv" title="torchrl.envs.libs.gym.GymEnv" class="sphx-glr-backref-module-torchrl-envs-libs-gym sphx-glr-backref-type-py-function"><span class="n">GymEnv</span></a>
<span class="kn">from</span> <span class="nn">torchrl.envs.transforms</span> <span class="kn">import</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.RewardScaling.html#torchrl.envs.transforms.RewardScaling" title="torchrl.envs.transforms.transforms.RewardScaling" class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class"><span class="n">RewardScaling</span></a><span class="p">,</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv" title="torchrl.envs.transforms.transforms.TransformedEnv" class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class"><span class="n">TransformedEnv</span></a>
<span class="kn">from</span> <span class="nn">torchrl.envs.utils</span> <span class="kn">import</span> <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.nn.set_interaction_mode.html#tensordict.nn.set_interaction_mode" title="tensordict.nn.set_interaction_mode" class="sphx-glr-backref-module-tensordict-nn sphx-glr-backref-type-py-class"><span class="n">set_exploration_mode</span></a><span class="p">,</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.utils.step_mdp.html#torchrl.envs.utils.step_mdp" title="torchrl.envs.utils.step_mdp" class="sphx-glr-backref-module-torchrl-envs-utils sphx-glr-backref-type-py-function"><span class="n">step_mdp</span></a>
<span class="kn">from</span> <span class="nn">torchrl.modules</span> <span class="kn">import</span> <span class="p">(</span>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.MLP.html#torchrl.modules.MLP" title="torchrl.modules.MLP" class="sphx-glr-backref-module-torchrl-modules sphx-glr-backref-type-py-class"><span class="n">MLP</span></a><span class="p">,</span>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.OrnsteinUhlenbeckProcessWrapper.html#torchrl.modules.tensordict_module.OrnsteinUhlenbeckProcessWrapper" title="torchrl.modules.tensordict_module.exploration.OrnsteinUhlenbeckProcessWrapper" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-exploration sphx-glr-backref-type-py-class"><span class="n">OrnsteinUhlenbeckProcessWrapper</span></a><span class="p">,</span>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor" title="torchrl.modules.tensordict_module.actors.ProbabilisticActor" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class"><span class="n">ProbabilisticActor</span></a><span class="p">,</span>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator" title="torchrl.modules.tensordict_module.actors.ValueOperator" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class"><span class="n">ValueOperator</span></a><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torchrl.modules.distributions.continuous</span> <span class="kn">import</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.TanhDelta.html#torchrl.modules.TanhDelta" title="torchrl.modules.TanhDelta" class="sphx-glr-backref-module-torchrl-modules sphx-glr-backref-type-py-class"><span class="n">TanhDelta</span></a>
<span class="kn">from</span> <span class="nn">torchrl.objectives.utils</span> <span class="kn">import</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.objectives.hold_out_net.html#torchrl.objectives.hold_out_net" title="torchrl.objectives.hold_out_net" class="sphx-glr-backref-module-torchrl-objectives sphx-glr-backref-type-py-class"><span class="n">hold_out_net</span></a>
<span class="kn">from</span> <span class="nn">torchrl.trainers</span> <span class="kn">import</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.trainers.Recorder.html#torchrl.trainers.Recorder" title="torchrl.trainers.Recorder" class="sphx-glr-backref-module-torchrl-trainers sphx-glr-backref-type-py-class"><span class="n">Recorder</span></a>
</pre></div>
</div>
</section>
<section id="environment">
<h2>Environment<a class="headerlink" href="#environment" title="Permalink to this heading">¶</a></h2>
<p>In most algorithms, the first thing that needs to be taken care of is the
construction of the environmet as it conditions the remainder of the
training script.</p>
<p>For this example, we will be using the <code class="docutils literal notranslate"><span class="pre">&quot;cheetah&quot;</span></code> task. The goal is to make
a half-cheetah run as fast as possible.</p>
<p>In TorchRL, one can create such a task by relying on dm_control or gym:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">env</span> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.libs.gym.GymEnv.html#torchrl.envs.libs.gym.GymEnv" title="torchrl.envs.libs.gym.GymEnv" class="sphx-glr-backref-module-torchrl-envs-libs-gym sphx-glr-backref-type-py-function"><span class="n">GymEnv</span></a><span class="p">(</span><span class="s2">&quot;HalfCheetah-v4&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>or</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">env</span> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.libs.dm_control.DMControlEnv.html#torchrl.envs.libs.dm_control.DMControlEnv" title="torchrl.envs.libs.dm_control.DMControlEnv" class="sphx-glr-backref-module-torchrl-envs-libs-dm_control sphx-glr-backref-type-py-function"><span class="n">DMControlEnv</span></a><span class="p">(</span><span class="s2">&quot;cheetah&quot;</span><span class="p">,</span> <span class="s2">&quot;run&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>By default, these environment disable rendering. Training from states is
usually easier than training from images. To keep things simple, we focus
on learning from states only. To pass the pixels to the tensordicts that
are collected by <code class="xref py py-func docutils literal notranslate"><span class="pre">env.step()</span></code>, simply pass the <code class="docutils literal notranslate"><span class="pre">from_pixels=True</span></code>
argument to the constructor:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">env</span> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.libs.gym.GymEnv.html#torchrl.envs.libs.gym.GymEnv" title="torchrl.envs.libs.gym.GymEnv" class="sphx-glr-backref-module-torchrl-envs-libs-gym sphx-glr-backref-type-py-function"><span class="n">GymEnv</span></a><span class="p">(</span><span class="s2">&quot;HalfCheetah-v4&quot;</span><span class="p">,</span> <span class="n">from_pixels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">pixels_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>We write a <code class="xref py py-func docutils literal notranslate"><span class="pre">make_env()</span></code> helper funciton that will create an environment
with either one of the two backends considered above (dm-control or gym).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><a href="https://pytorch.org/rl/reference/generated/torchrl.envs.libs.gym.GymEnv.html#torchrl.envs.libs.gym.GymEnv" title="torchrl.envs.libs.gym.GymEnv" class="sphx-glr-backref-module-torchrl-envs-libs-gym sphx-glr-backref-type-py-function"><span class="n">env_library</span></a> <span class="o">=</span> <span class="kc">None</span>
<span class="n">env_name</span> <span class="o">=</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">make_env</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create a base env.&quot;&quot;&quot;</span>
    <span class="k">global</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.libs.gym.GymEnv.html#torchrl.envs.libs.gym.GymEnv" title="torchrl.envs.libs.gym.GymEnv" class="sphx-glr-backref-module-torchrl-envs-libs-gym sphx-glr-backref-type-py-function"><span class="n">env_library</span></a>
    <span class="k">global</span> <span class="n">env_name</span>

    <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="s2">&quot;dm_control&quot;</span><span class="p">:</span>
        <span class="n">env_name</span> <span class="o">=</span> <span class="s2">&quot;cheetah&quot;</span>
        <span class="n">env_task</span> <span class="o">=</span> <span class="s2">&quot;run&quot;</span>
        <span class="n">env_args</span> <span class="o">=</span> <span class="p">(</span><span class="n">env_name</span><span class="p">,</span> <span class="n">env_task</span><span class="p">)</span>
        <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.libs.gym.GymEnv.html#torchrl.envs.libs.gym.GymEnv" title="torchrl.envs.libs.gym.GymEnv" class="sphx-glr-backref-module-torchrl-envs-libs-gym sphx-glr-backref-type-py-function"><span class="n">env_library</span></a> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.libs.dm_control.DMControlEnv.html#torchrl.envs.libs.dm_control.DMControlEnv" title="torchrl.envs.libs.dm_control.DMControlEnv" class="sphx-glr-backref-module-torchrl-envs-libs-dm_control sphx-glr-backref-type-py-function"><span class="n">DMControlEnv</span></a>
    <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s2">&quot;gym&quot;</span><span class="p">:</span>
        <span class="n">env_name</span> <span class="o">=</span> <span class="s2">&quot;HalfCheetah-v4&quot;</span>
        <span class="n">env_args</span> <span class="o">=</span> <span class="p">(</span><span class="n">env_name</span><span class="p">,)</span>
        <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.libs.gym.GymEnv.html#torchrl.envs.libs.gym.GymEnv" title="torchrl.envs.libs.gym.GymEnv" class="sphx-glr-backref-module-torchrl-envs-libs-gym sphx-glr-backref-type-py-function"><span class="n">env_library</span></a> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.libs.gym.GymEnv.html#torchrl.envs.libs.gym.GymEnv" title="torchrl.envs.libs.gym.GymEnv" class="sphx-glr-backref-module-torchrl-envs-libs-gym sphx-glr-backref-type-py-function"><span class="n">GymEnv</span></a>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="n">env_kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;device&quot;</span><span class="p">:</span> <a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">device</span></a><span class="p">,</span>
        <span class="s2">&quot;frame_skip&quot;</span><span class="p">:</span> <span class="n">frame_skip</span><span class="p">,</span>
        <span class="s2">&quot;from_pixels&quot;</span><span class="p">:</span> <span class="n">from_pixels</span><span class="p">,</span>
        <span class="s2">&quot;pixels_only&quot;</span><span class="p">:</span> <span class="n">from_pixels</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="n">env</span> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.libs.gym.GymEnv.html#torchrl.envs.libs.gym.GymEnv" title="torchrl.envs.libs.gym.GymEnv" class="sphx-glr-backref-module-torchrl-envs-libs-gym sphx-glr-backref-type-py-function"><span class="n">env_library</span></a><span class="p">(</span><span class="o">*</span><span class="n">env_args</span><span class="p">,</span> <span class="o">**</span><span class="n">env_kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">env</span>
</pre></div>
</div>
<section id="transforms">
<h3>Transforms<a class="headerlink" href="#transforms" title="Permalink to this heading">¶</a></h3>
<p>Now that we have a base environment, we may want to modify its representation
to make it more policy-friendly. In TorchRL, transforms are appended to the
base environment in a specialized <code class="xref py py-class docutils literal notranslate"><span class="pre">torchr.envs.TransformedEnv</span></code> class.</p>
<ul class="simple">
<li><p>It is common in DDPG to rescale the reward using some heuristic value. We
will multiply the reward by 5 in this example.</p></li>
<li><p>If we are using <code class="xref py py-mod docutils literal notranslate"><span class="pre">dm_control</span></code>, it is also important to build an interface
between the simulator which works with double precision numbers, and our
script which presumably uses single precision ones. This transformation goes
both ways: when calling <code class="xref py py-func docutils literal notranslate"><span class="pre">env.step()</span></code>, our actions will need to be
represented in double precision, and the output will need to be transformed
to single precision.
The <code class="xref py py-class docutils literal notranslate"><span class="pre">torchrl.envs.DoubleToFloat</span></code> transform does exactly this: the
<code class="docutils literal notranslate"><span class="pre">in_keys</span></code> list refers to the keys that will need to be transformed from
double to float, while the <code class="docutils literal notranslate"><span class="pre">in_keys_inv</span></code> refers to those that need to
be transformed to double before being passed to the environment.</p></li>
<li><p>We concatenate the state keys together using the <code class="xref py py-class docutils literal notranslate"><span class="pre">torchrl.envs.CatTensors</span></code>
transform.</p></li>
<li><p>Finally, we also leave the possibility of normalizing the states: we will
take care of computing the normalizing constants later on.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">make_transformed_env</span><span class="p">(</span>
    <span class="n">env</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Apply transforms to the env (such as reward scaling and state normalization).&quot;&quot;&quot;</span>

    <span class="n">env</span> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv" title="torchrl.envs.transforms.transforms.TransformedEnv" class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class"><span class="n">TransformedEnv</span></a><span class="p">(</span><span class="n">env</span><span class="p">)</span>

    <span class="c1"># we append transforms one by one, although we might as well create the</span>
    <span class="c1"># transformed environment using the `env = TransformedEnv(base_env, transforms)`</span>
    <span class="c1"># syntax.</span>
    <span class="n">env</span><span class="o">.</span><span class="n">append_transform</span><span class="p">(</span><a href="https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.RewardScaling.html#torchrl.envs.transforms.RewardScaling" title="torchrl.envs.transforms.transforms.RewardScaling" class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class"><span class="n">RewardScaling</span></a><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">reward_scaling</span><span class="p">))</span>

    <span class="n">double_to_float_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">double_to_float_inv_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.libs.gym.GymEnv.html#torchrl.envs.libs.gym.GymEnv" title="torchrl.envs.libs.gym.GymEnv" class="sphx-glr-backref-module-torchrl-envs-libs-gym sphx-glr-backref-type-py-function"><span class="n">env_library</span></a> <span class="ow">is</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.libs.dm_control.DMControlEnv.html#torchrl.envs.libs.dm_control.DMControlEnv" title="torchrl.envs.libs.dm_control.DMControlEnv" class="sphx-glr-backref-module-torchrl-envs-libs-dm_control sphx-glr-backref-type-py-function"><span class="n">DMControlEnv</span></a><span class="p">:</span>
        <span class="c1"># DMControl requires double-precision</span>
        <span class="n">double_to_float_list</span> <span class="o">+=</span> <span class="p">[</span>
            <span class="s2">&quot;reward&quot;</span><span class="p">,</span>
            <span class="s2">&quot;action&quot;</span><span class="p">,</span>
        <span class="p">]</span>
        <span class="n">double_to_float_inv_list</span> <span class="o">+=</span> <span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">]</span>

    <span class="c1"># We concatenate all states into a single &quot;observation_vector&quot;</span>
    <span class="c1"># even if there is a single tensor, it&#39;ll be renamed in &quot;observation_vector&quot;.</span>
    <span class="c1"># This facilitates the downstream operations as we know the name of the</span>
    <span class="c1"># output tensor.</span>
    <span class="c1"># In some environments (not half-cheetah), there may be more than one</span>
    <span class="c1"># observation vector: in this case this code snippet will concatenate them</span>
    <span class="c1"># all.</span>
    <span class="n">selected_keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">observation_spec</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="n">out_key</span> <span class="o">=</span> <span class="s2">&quot;observation_vector&quot;</span>
    <span class="n">env</span><span class="o">.</span><span class="n">append_transform</span><span class="p">(</span><a href="https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.CatTensors.html#torchrl.envs.transforms.CatTensors" title="torchrl.envs.transforms.transforms.CatTensors" class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class"><span class="n">CatTensors</span></a><span class="p">(</span><span class="n">in_keys</span><span class="o">=</span><span class="n">selected_keys</span><span class="p">,</span> <span class="n">out_key</span><span class="o">=</span><span class="n">out_key</span><span class="p">))</span>

    <span class="c1"># we normalize the states, but for now let&#39;s just instantiate a stateless</span>
    <span class="c1"># version of the transform</span>
    <span class="n">env</span><span class="o">.</span><span class="n">append_transform</span><span class="p">(</span><a href="https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.ObservationNorm.html#torchrl.envs.transforms.ObservationNorm" title="torchrl.envs.transforms.transforms.ObservationNorm" class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class"><span class="n">ObservationNorm</span></a><span class="p">(</span><span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="n">out_key</span><span class="p">],</span> <span class="n">standard_normal</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

    <span class="n">double_to_float_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out_key</span><span class="p">)</span>
    <span class="n">env</span><span class="o">.</span><span class="n">append_transform</span><span class="p">(</span>
        <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.DoubleToFloat.html#torchrl.envs.transforms.DoubleToFloat" title="torchrl.envs.transforms.transforms.DoubleToFloat" class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class"><span class="n">DoubleToFloat</span></a><span class="p">(</span>
            <span class="n">in_keys</span><span class="o">=</span><span class="n">double_to_float_list</span><span class="p">,</span> <span class="n">in_keys_inv</span><span class="o">=</span><span class="n">double_to_float_inv_list</span>
        <span class="p">)</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">env</span>
</pre></div>
</div>
</section>
<section id="normalization-of-the-observations">
<h3>Normalization of the observations<a class="headerlink" href="#normalization-of-the-observations" title="Permalink to this heading">¶</a></h3>
<p>To compute the normalizing statistics, we run an arbitrary number of random
steps in the environment and compute the mean and standard deviation of the
collected observations. The <code class="xref py py-func docutils literal notranslate"><span class="pre">ObservationNorm.init_stats()</span></code> method can
be used for this purpose. To get the summary statistics, we create a dummy
environment and run it for a given number of steps, collect data over a given
number of steps and compute its summary statistics.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_env_stats</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Gets the stats of an environment.&quot;&quot;&quot;</span>
    <span class="n">proof_env</span> <span class="o">=</span> <span class="n">make_transformed_env</span><span class="p">(</span><span class="n">make_env</span><span class="p">())</span>
    <span class="n">proof_env</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">proof_env</span><span class="o">.</span><span class="n">transform</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">t</span><span class="o">.</span><span class="n">init_stats</span><span class="p">(</span><span class="n">init_env_steps</span><span class="p">)</span>
    <span class="n">transform_state_dict</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
    <span class="n">proof_env</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">transform_state_dict</span>
</pre></div>
</div>
</section>
<section id="parallel-execution">
<h3>Parallel execution<a class="headerlink" href="#parallel-execution" title="Permalink to this heading">¶</a></h3>
<p>The following helper function allows us to run environments in parallel.
Running environments in parallel can significantly speed up the collection
throughput. When using transformed environment, we need to choose whether we
want to execute the transform individually for each environment, or
centralize the data and transform it in batch. Both approaches are easy to
code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">env</span> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.ParallelEnv.html#torchrl.envs.ParallelEnv" title="torchrl.envs.ParallelEnv" class="sphx-glr-backref-module-torchrl-envs sphx-glr-backref-type-py-class"><span class="n">ParallelEnv</span></a><span class="p">(</span>
    <span class="k">lambda</span><span class="p">:</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv" title="torchrl.envs.transforms.transforms.TransformedEnv" class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class"><span class="n">TransformedEnv</span></a><span class="p">(</span><a href="https://pytorch.org/rl/reference/generated/torchrl.envs.libs.gym.GymEnv.html#torchrl.envs.libs.gym.GymEnv" title="torchrl.envs.libs.gym.GymEnv" class="sphx-glr-backref-module-torchrl-envs-libs-gym sphx-glr-backref-type-py-function"><span class="n">GymEnv</span></a><span class="p">(</span><span class="s2">&quot;HalfCheetah-v4&quot;</span><span class="p">),</span> <span class="n">transforms</span><span class="p">),</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span>
<span class="p">)</span>
<span class="n">env</span> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv" title="torchrl.envs.transforms.transforms.TransformedEnv" class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class"><span class="n">TransformedEnv</span></a><span class="p">(</span>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.ParallelEnv.html#torchrl.envs.ParallelEnv" title="torchrl.envs.ParallelEnv" class="sphx-glr-backref-module-torchrl-envs sphx-glr-backref-type-py-class"><span class="n">ParallelEnv</span></a><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.libs.gym.GymEnv.html#torchrl.envs.libs.gym.GymEnv" title="torchrl.envs.libs.gym.GymEnv" class="sphx-glr-backref-module-torchrl-envs-libs-gym sphx-glr-backref-type-py-function"><span class="n">GymEnv</span></a><span class="p">(</span><span class="s2">&quot;HalfCheetah-v4&quot;</span><span class="p">),</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
    <span class="n">transforms</span>
<span class="p">)</span>
</pre></div>
</div>
<p>To leverage the vectorization capabilities of PyTorch, we adopt
the first method:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parallel_env_constructor</span><span class="p">(</span>
    <span class="n">transform_state_dict</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">if</span> <span class="n">env_per_collector</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>

        <span class="k">def</span> <span class="nf">make_t_env</span><span class="p">():</span>
            <span class="n">env</span> <span class="o">=</span> <span class="n">make_transformed_env</span><span class="p">(</span><span class="n">make_env</span><span class="p">())</span>
            <span class="n">env</span><span class="o">.</span><span class="n">transform</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">init_stats</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
            <span class="n">env</span><span class="o">.</span><span class="n">transform</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">loc</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">transform_state_dict</span><span class="p">[</span><span class="s2">&quot;loc&quot;</span><span class="p">])</span>
            <span class="n">env</span><span class="o">.</span><span class="n">transform</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">scale</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">transform_state_dict</span><span class="p">[</span><span class="s2">&quot;scale&quot;</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">env</span>

        <span class="n">env_creator</span> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.EnvCreator.html#torchrl.envs.EnvCreator" title="torchrl.envs.EnvCreator" class="sphx-glr-backref-module-torchrl-envs sphx-glr-backref-type-py-class"><span class="n">EnvCreator</span></a><span class="p">(</span><span class="n">make_t_env</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">env_creator</span>

    <span class="n">parallel_env</span> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.ParallelEnv.html#torchrl.envs.ParallelEnv" title="torchrl.envs.ParallelEnv" class="sphx-glr-backref-module-torchrl-envs sphx-glr-backref-type-py-class"><span class="n">ParallelEnv</span></a><span class="p">(</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="n">env_per_collector</span><span class="p">,</span>
        <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv" title="torchrl.envs.transforms.transforms.TransformedEnv" class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">create_env_fn</span></a><span class="o">=</span><a href="https://pytorch.org/rl/reference/generated/torchrl.envs.EnvCreator.html#torchrl.envs.EnvCreator" title="torchrl.envs.EnvCreator" class="sphx-glr-backref-module-torchrl-envs sphx-glr-backref-type-py-class"><span class="n">EnvCreator</span></a><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">make_env</span><span class="p">()),</span>
        <span class="n">create_env_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">pin_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">make_transformed_env</span><span class="p">(</span><span class="n">parallel_env</span><span class="p">)</span>
    <span class="c1"># we call `init_stats` for a limited number of steps, just to instantiate</span>
    <span class="c1"># the lazy buffers.</span>
    <span class="n">env</span><span class="o">.</span><span class="n">transform</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">init_stats</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">cat_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">reduce_dim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">env</span><span class="o">.</span><span class="n">transform</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">transform_state_dict</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">env</span>
</pre></div>
</div>
</section>
</section>
<section id="building-the-model">
<h2>Building the model<a class="headerlink" href="#building-the-model" title="Permalink to this heading">¶</a></h2>
<p>We now turn to the setup of the model and loss function. DDPG requires a
value network, trained to estimate the value of a state-action pair, and a
parametric actor that learns how to select actions that maximize this value.
In this tutorial, we will be using two independent networks for these
components.</p>
<p>Recall that building a torchrl module requires two steps:</p>
<ul class="simple">
<li><p>writing the <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v1.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a> that will be used as network</p></li>
<li><p>wrapping the network in a <a class="reference external" href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.nn.TensorDictModule.html#tensordict.nn.TensorDictModule" title="(in tensordict vmain (0.0.2b+696ac1a ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">tensordict.nn.TensorDictModule</span></code></a> where the
data flow is handled by specifying the input and output keys.</p></li>
</ul>
<p>In more complex scenarios, <a class="reference external" href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.nn.TensorDictSequential.html#tensordict.nn.TensorDictSequential" title="(in tensordict vmain (0.0.2b+696ac1a ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">tensordict.nn.TensorDictSequential</span></code></a> can
also be used.</p>
<p>In <code class="xref py py-func docutils literal notranslate"><span class="pre">make_ddpg_actor()</span></code>, we use a <code class="xref py py-class docutils literal notranslate"><span class="pre">torchrl.modules.ProbabilisticActor</span></code>
object to wrap our policy network. Since DDPG is a deterministic algorithm,
this is not strictly necessary. We rely on this class to map the output
action to the appropriate domain. Alternatively, one could perfectly use a
non-linearity such as <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.tanh</span></code> to map the output to the right
domain.</p>
<p>The Q-Value network is wrapped in a <code class="xref py py-class docutils literal notranslate"><span class="pre">torchrl.modules.ValueOperator</span></code>
that automatically sets the <code class="docutils literal notranslate"><span class="pre">out_keys</span></code> to <code class="docutils literal notranslate"><span class="pre">&quot;state_action_value</span></code> for q-value
networks and <code class="docutils literal notranslate"><span class="pre">state_value</span></code> for other value networks.</p>
<p>Since we use lazy modules, it is necessary to materialize the lazy modules
before being able to move the policy from device to device and achieve other
operations. Hence, it is good practice to run the modules with a small
sample of data. For this purpose, we generate fake data from the
environment specs.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">make_ddpg_actor</span><span class="p">(</span>
    <span class="n">transform_state_dict</span><span class="p">,</span>
    <a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">device</span></a><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">proof_environment</span> <span class="o">=</span> <span class="n">make_transformed_env</span><span class="p">(</span><span class="n">make_env</span><span class="p">())</span>
    <span class="n">proof_environment</span><span class="o">.</span><span class="n">transform</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">init_stats</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">proof_environment</span><span class="o">.</span><span class="n">transform</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">transform_state_dict</span><span class="p">)</span>

    <span class="n">env_specs</span> <span class="o">=</span> <span class="n">proof_environment</span><span class="o">.</span><span class="n">specs</span>
    <span class="n">out_features</span> <span class="o">=</span> <span class="n">env_specs</span><span class="p">[</span><span class="s2">&quot;action_spec&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">actor_net</span> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.MLP.html#torchrl.modules.MLP" title="torchrl.modules.MLP" class="sphx-glr-backref-module-torchrl-modules sphx-glr-backref-type-py-class"><span class="n">MLP</span></a><span class="p">(</span>
        <span class="n">num_cells</span><span class="o">=</span><span class="p">[</span><span class="n">num_cells</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_layers</span><span class="p">,</span>
        <span class="n">activation_class</span><span class="o">=</span><a href="https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh" title="torch.nn.Tanh" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span></a><span class="p">,</span>
        <span class="n">out_features</span><span class="o">=</span><span class="n">out_features</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">in_keys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;observation_vector&quot;</span><span class="p">]</span>
    <span class="n">out_keys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;param&quot;</span><span class="p">]</span>

    <span class="n">actor_module</span> <span class="o">=</span> <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.nn.TensorDictModule.html#tensordict.nn.TensorDictModule" title="tensordict.nn.TensorDictModule" class="sphx-glr-backref-module-tensordict-nn sphx-glr-backref-type-py-class"><span class="n">TensorDictModule</span></a><span class="p">(</span><span class="n">actor_net</span><span class="p">,</span> <span class="n">in_keys</span><span class="o">=</span><span class="n">in_keys</span><span class="p">,</span> <span class="n">out_keys</span><span class="o">=</span><span class="n">out_keys</span><span class="p">)</span>

    <span class="c1"># We use a ProbabilisticActor to make sure that we map the network output</span>
    <span class="c1"># to the right space using a TanhDelta distribution.</span>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor" title="torchrl.modules.tensordict_module.actors.ProbabilisticActor" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">actor</span></a> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor" title="torchrl.modules.tensordict_module.actors.ProbabilisticActor" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class"><span class="n">ProbabilisticActor</span></a><span class="p">(</span>
        <span class="n">module</span><span class="o">=</span><span class="n">actor_module</span><span class="p">,</span>
        <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;param&quot;</span><span class="p">],</span>
        <span class="n">spec</span><span class="o">=</span><a href="https://pytorch.org/rl/reference/generated/torchrl.data.CompositeSpec.html#torchrl.data.CompositeSpec" title="torchrl.data.CompositeSpec" class="sphx-glr-backref-module-torchrl-data sphx-glr-backref-type-py-class"><span class="n">CompositeSpec</span></a><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="n">env_specs</span><span class="p">[</span><span class="s2">&quot;action_spec&quot;</span><span class="p">]),</span>
        <span class="n">safe</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">distribution_class</span><span class="o">=</span><a href="https://pytorch.org/rl/reference/generated/torchrl.modules.TanhDelta.html#torchrl.modules.TanhDelta" title="torchrl.modules.TanhDelta" class="sphx-glr-backref-module-torchrl-modules sphx-glr-backref-type-py-class"><span class="n">TanhDelta</span></a><span class="p">,</span>
        <span class="n">distribution_kwargs</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;min&quot;</span><span class="p">:</span> <span class="n">env_specs</span><span class="p">[</span><span class="s2">&quot;action_spec&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">minimum</span><span class="p">,</span>
            <span class="s2">&quot;max&quot;</span><span class="p">:</span> <span class="n">env_specs</span><span class="p">[</span><span class="s2">&quot;action_spec&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">maximum</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">device</span></a><span class="p">)</span>

    <span class="n">q_net</span> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.MLP.html#torchrl.modules.MLP" title="torchrl.modules.MLP" class="sphx-glr-backref-module-torchrl-modules sphx-glr-backref-type-py-class"><span class="n">MLP</span></a><span class="p">(</span>
        <span class="n">num_cells</span><span class="o">=</span><span class="p">[</span><span class="n">num_cells</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_layers</span><span class="p">,</span>
        <span class="n">activation_class</span><span class="o">=</span><a href="https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh" title="torch.nn.Tanh" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span></a><span class="p">,</span>
        <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">in_keys</span> <span class="o">=</span> <span class="n">in_keys</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">]</span>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator" title="torchrl.modules.tensordict_module.actors.ValueOperator" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">qnet</span></a> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator" title="torchrl.modules.tensordict_module.actors.ValueOperator" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class"><span class="n">ValueOperator</span></a><span class="p">(</span>
        <span class="n">in_keys</span><span class="o">=</span><span class="n">in_keys</span><span class="p">,</span>
        <span class="n">module</span><span class="o">=</span><span class="n">q_net</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">device</span></a><span class="p">)</span>

    <span class="c1"># init: since we have lazy layers, we should run the network</span>
    <span class="c1"># once to initialize them</span>
    <span class="k">with</span> <a href="https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad" title="torch.no_grad" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span></a><span class="p">(),</span> <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.nn.set_interaction_mode.html#tensordict.nn.set_interaction_mode" title="tensordict.nn.set_interaction_mode" class="sphx-glr-backref-module-tensordict-nn sphx-glr-backref-type-py-class"><span class="n">set_exploration_mode</span></a><span class="p">(</span><span class="s2">&quot;random&quot;</span><span class="p">):</span>
        <span class="n">td</span> <span class="o">=</span> <span class="n">proof_environment</span><span class="o">.</span><span class="n">fake_tensordict</span><span class="p">()</span>
        <span class="n">td</span> <span class="o">=</span> <span class="n">td</span><span class="o">.</span><span class="n">expand</span><span class="p">((</span><span class="o">*</span><span class="n">td</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">td</span> <span class="o">=</span> <span class="n">td</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">device</span></a><span class="p">)</span>
        <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor" title="torchrl.modules.tensordict_module.actors.ProbabilisticActor" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">actor</span></a><span class="p">(</span><span class="n">td</span><span class="p">)</span>
        <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator" title="torchrl.modules.tensordict_module.actors.ValueOperator" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">qnet</span></a><span class="p">(</span><span class="n">td</span><span class="p">)</span>

    <span class="k">return</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor" title="torchrl.modules.tensordict_module.actors.ProbabilisticActor" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">actor</span></a><span class="p">,</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator" title="torchrl.modules.tensordict_module.actors.ValueOperator" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">qnet</span></a>
</pre></div>
</div>
</section>
<section id="evaluator-building-your-recorder-object">
<h2>Evaluator: building your recorder object<a class="headerlink" href="#evaluator-building-your-recorder-object" title="Permalink to this heading">¶</a></h2>
<p>As the training data is obtained using some exploration strategy, the true
performance of our algorithm needs to be assessed in deterministic mode. We
do this using a dedicated class, <code class="docutils literal notranslate"><span class="pre">Recorder</span></code>, which executes the policy in
the environment at a given frequency and returns some statistics obtained
from these simulations.</p>
<p>The following helper function builds this object:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">make_recorder</span><span class="p">(</span><a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.OrnsteinUhlenbeckProcessWrapper.html#torchrl.modules.tensordict_module.OrnsteinUhlenbeckProcessWrapper" title="torchrl.modules.tensordict_module.exploration.OrnsteinUhlenbeckProcessWrapper" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-exploration sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">actor_model_explore</span></a><span class="p">,</span> <span class="n">transform_state_dict</span><span class="p">):</span>
    <span class="n">base_env</span> <span class="o">=</span> <span class="n">make_env</span><span class="p">()</span>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.trainers.Recorder.html#torchrl.trainers.Recorder" title="torchrl.trainers.Recorder" class="sphx-glr-backref-module-torchrl-trainers sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">recorder</span></a> <span class="o">=</span> <span class="n">make_transformed_env</span><span class="p">(</span><span class="n">base_env</span><span class="p">)</span>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.trainers.Recorder.html#torchrl.trainers.Recorder" title="torchrl.trainers.Recorder" class="sphx-glr-backref-module-torchrl-trainers sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">recorder</span><span class="o">.</span><span class="n">transform</span></a><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">init_stats</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.trainers.Recorder.html#torchrl.trainers.Recorder" title="torchrl.trainers.Recorder" class="sphx-glr-backref-module-torchrl-trainers sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">recorder</span><span class="o">.</span><span class="n">transform</span></a><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">transform_state_dict</span><span class="p">)</span>

    <span class="n">recorder_obj</span> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.trainers.Recorder.html#torchrl.trainers.Recorder" title="torchrl.trainers.Recorder" class="sphx-glr-backref-module-torchrl-trainers sphx-glr-backref-type-py-class"><span class="n">Recorder</span></a><span class="p">(</span>
        <span class="n">record_frames</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
        <span class="n">frame_skip</span><span class="o">=</span><span class="n">frame_skip</span><span class="p">,</span>
        <span class="n">policy_exploration</span><span class="o">=</span><a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.OrnsteinUhlenbeckProcessWrapper.html#torchrl.modules.tensordict_module.OrnsteinUhlenbeckProcessWrapper" title="torchrl.modules.tensordict_module.exploration.OrnsteinUhlenbeckProcessWrapper" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-exploration sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">actor_model_explore</span></a><span class="p">,</span>
        <a href="https://pytorch.org/rl/reference/generated/torchrl.trainers.Recorder.html#torchrl.trainers.Recorder" title="torchrl.trainers.Recorder" class="sphx-glr-backref-module-torchrl-trainers sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">recorder</span></a><span class="o">=</span><a href="https://pytorch.org/rl/reference/generated/torchrl.trainers.Recorder.html#torchrl.trainers.Recorder" title="torchrl.trainers.Recorder" class="sphx-glr-backref-module-torchrl-trainers sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">recorder</span></a><span class="p">,</span>
        <span class="n">exploration_mode</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span>
        <span class="n">record_interval</span><span class="o">=</span><span class="n">record_interval</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">recorder_obj</span>
</pre></div>
</div>
</section>
<section id="replay-buffer">
<h2>Replay buffer<a class="headerlink" href="#replay-buffer" title="Permalink to this heading">¶</a></h2>
<p>Replay buffers come in two flavors: prioritized (where some error signal
is used to give a higher likelihood of sampling to some items than others)
and regular, circular experience replay.</p>
<p>TorchRL replay buffers are composable: one can pick up the storage, sampling
and writing strategies. It is also possible to
store tensors on physical memory using a memory-mapped array. The following
function takes care of creating the replay buffer with the desired
hyperparameters:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">make_replay_buffer</span><span class="p">(</span><span class="n">buffer_size</span><span class="p">,</span> <span class="n">prefetch</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">prb</span><span class="p">:</span>
        <span class="n">sampler</span> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.data.replay_buffers.samplers.PrioritizedSampler.html#torchrl.data.replay_buffers.samplers.PrioritizedSampler" title="torchrl.data.replay_buffers.samplers.PrioritizedSampler" class="sphx-glr-backref-module-torchrl-data-replay_buffers-samplers sphx-glr-backref-type-py-class"><span class="n">PrioritizedSampler</span></a><span class="p">(</span>
            <span class="n">max_capacity</span><span class="o">=</span><span class="n">buffer_size</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
            <span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">sampler</span> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.data.replay_buffers.samplers.RandomSampler.html#torchrl.data.replay_buffers.samplers.RandomSampler" title="torchrl.data.replay_buffers.samplers.RandomSampler" class="sphx-glr-backref-module-torchrl-data-replay_buffers-samplers sphx-glr-backref-type-py-class"><span class="n">RandomSampler</span></a><span class="p">()</span>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.data.TensorDictReplayBuffer.html#torchrl.data.TensorDictReplayBuffer" title="torchrl.data.TensorDictReplayBuffer" class="sphx-glr-backref-module-torchrl-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">replay_buffer</span></a> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.data.TensorDictReplayBuffer.html#torchrl.data.TensorDictReplayBuffer" title="torchrl.data.TensorDictReplayBuffer" class="sphx-glr-backref-module-torchrl-data sphx-glr-backref-type-py-class"><span class="n">TensorDictReplayBuffer</span></a><span class="p">(</span>
        <span class="n">storage</span><span class="o">=</span><a href="https://pytorch.org/rl/reference/generated/torchrl.data.replay_buffers.storages.LazyMemmapStorage.html#torchrl.data.replay_buffers.storages.LazyMemmapStorage" title="torchrl.data.replay_buffers.storages.LazyMemmapStorage" class="sphx-glr-backref-module-torchrl-data-replay_buffers-storages sphx-glr-backref-type-py-class"><span class="n">LazyMemmapStorage</span></a><span class="p">(</span>
            <span class="n">buffer_size</span><span class="p">,</span>
            <span class="n">scratch_dir</span><span class="o">=</span><span class="n">buffer_scratch_dir</span><span class="p">,</span>
            <a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">device</span></a><span class="o">=</span><a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">device</span></a><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">,</span>
        <span class="n">pin_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">prefetch</span><span class="o">=</span><span class="n">prefetch</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.data.TensorDictReplayBuffer.html#torchrl.data.TensorDictReplayBuffer" title="torchrl.data.TensorDictReplayBuffer" class="sphx-glr-backref-module-torchrl-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">replay_buffer</span></a>
</pre></div>
</div>
</section>
<section id="hyperparameters">
<h2>Hyperparameters<a class="headerlink" href="#hyperparameters" title="Permalink to this heading">¶</a></h2>
<p>After having written our helper functions, it is time to set the
experiment hyperparameters:</p>
<section id="id1">
<h3>Environment<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># The backend can be gym or dm_control</span>
<span class="n">backend</span> <span class="o">=</span> <span class="s2">&quot;gym&quot;</span>

<span class="n">exp_name</span> <span class="o">=</span> <span class="s2">&quot;cheetah&quot;</span>

<span class="c1"># frame_skip batches multiple step together with a single action</span>
<span class="c1"># If &gt; 1, the other frame counts (e.g. frames_per_batch, total_frames) need to</span>
<span class="c1"># be adjusted to have a consistent total number of frames collected across</span>
<span class="c1"># experiments.</span>
<span class="n">frame_skip</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">from_pixels</span> <span class="o">=</span> <span class="kc">False</span>
<span class="c1"># Scaling the reward helps us control the signal magnitude for a more</span>
<span class="c1"># efficient learning.</span>
<span class="n">reward_scaling</span> <span class="o">=</span> <span class="mf">5.0</span>

<span class="c1"># Number of random steps used as for stats computation using ObservationNorm</span>
<span class="n">init_env_steps</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Exploration: Number of frames before OU noise becomes null</span>
<span class="n">annealing_frames</span> <span class="o">=</span> <span class="mi">1000000</span> <span class="o">//</span> <span class="n">frame_skip</span>
</pre></div>
</div>
</section>
<section id="collection">
<h3>Collection<a class="headerlink" href="#collection" title="Permalink to this heading">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># We will execute the policy on cuda if available</span>
<a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">device</span></a> <span class="o">=</span> <span class="p">(</span>
    <a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">device</span></a><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span> <span class="k">if</span> <a href="https://pytorch.org/docs/stable/generated/torch.cuda.device_count.html#torch.cuda.device_count" title="torch.cuda.device_count" class="sphx-glr-backref-module-torch-cuda sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span></a><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">device</span></a><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Number of environments in each data collector</span>
<span class="n">env_per_collector</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># Total frames we will use during training. Scale up to 500K - 1M for a more</span>
<span class="c1"># meaningful training</span>
<span class="n">total_frames</span> <span class="o">=</span> <span class="mi">5000</span> <span class="o">//</span> <span class="n">frame_skip</span>
<span class="c1"># Number of frames returned by the collector at each iteration of the outer loop</span>
<span class="n">frames_per_batch</span> <span class="o">=</span> <span class="mi">1000</span> <span class="o">//</span> <span class="n">frame_skip</span>
<span class="n">init_random_frames</span> <span class="o">=</span> <span class="mi">0</span>
<span class="c1"># We&#39;ll be using the MultiStep class to have a less myopic representation of</span>
<span class="c1"># upcoming states</span>
<span class="n">n_steps_forward</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1"># record every 10 batch collected</span>
<span class="n">record_interval</span> <span class="o">=</span> <span class="mi">10</span>
</pre></div>
</div>
</section>
<section id="optimizer-and-optimization">
<h3>Optimizer and optimization<a class="headerlink" href="#optimizer-and-optimization" title="Permalink to this heading">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="mf">5e-4</span>
<span class="n">weight_decay</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="c1"># UTD: Number of iterations of the inner loop</span>
<span class="n">update_to_data</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
</pre></div>
</div>
</section>
<section id="model">
<h3>Model<a class="headerlink" href="#model" title="Permalink to this heading">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="n">tau</span> <span class="o">=</span> <span class="mf">0.005</span>  <span class="c1"># Decay factor for the target network</span>

<span class="c1"># Network specs</span>
<span class="n">num_cells</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">num_layers</span> <span class="o">=</span> <span class="mi">2</span>
</pre></div>
</div>
</section>
<section id="id2">
<h3>Replay buffer<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># If True, a Prioritized replay buffer will be used</span>
<span class="n">prb</span> <span class="o">=</span> <span class="kc">True</span>
<span class="c1"># Number of frames stored in the buffer</span>
<span class="n">buffer_size</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">total_frames</span><span class="p">,</span> <span class="mi">1000000</span> <span class="o">//</span> <span class="n">frame_skip</span><span class="p">)</span>
<span class="n">buffer_scratch_dir</span> <span class="o">=</span> <span class="s2">&quot;/tmp/&quot;</span>

<span class="n">seed</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
</section>
</section>
<section id="initialization">
<h2>Initialization<a class="headerlink" href="#initialization" title="Permalink to this heading">¶</a></h2>
<p>To initialize the experiment, we first acquire the observation statistics,
then build the networks, wrap them in an exploration wrapper (following the
seminal DDPG paper, we used an Ornstein-Uhlenbeck process to add noise to the
sampled actions).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Seeding</span>
<a href="https://pytorch.org/docs/stable/generated/torch.manual_seed.html#torch.manual_seed" title="torch.manual_seed" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span></a><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
</pre></div>
</div>
<section id="normalization-stats">
<h3>Normalization stats<a class="headerlink" href="#normalization-stats" title="Permalink to this heading">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">transform_state_dict</span> <span class="o">=</span> <span class="n">get_env_stats</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="models-policy-and-q-value-network">
<h3>Models: policy and q-value network<a class="headerlink" href="#models-policy-and-q-value-network" title="Permalink to this heading">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor" title="torchrl.modules.tensordict_module.actors.ProbabilisticActor" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">actor</span></a><span class="p">,</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator" title="torchrl.modules.tensordict_module.actors.ValueOperator" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">qnet</span></a> <span class="o">=</span> <span class="n">make_ddpg_actor</span><span class="p">(</span>
    <span class="n">transform_state_dict</span><span class="o">=</span><span class="n">transform_state_dict</span><span class="p">,</span>
    <a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">device</span></a><span class="o">=</span><a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">device</span></a><span class="p">,</span>
<span class="p">)</span>
<span class="k">if</span> <a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">device</span></a> <span class="o">==</span> <a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">device</span></a><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">):</span>
    <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.share_memory" title="torch.nn.Module.share_memory" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><span class="n">actor</span><span class="o">.</span><span class="n">share_memory</span></a><span class="p">()</span>
</pre></div>
</div>
<p>We create a copy of the q-value network to be used as target network</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator" title="torchrl.modules.tensordict_module.actors.ValueOperator" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">qnet_target</span></a> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator" title="torchrl.modules.tensordict_module.actors.ValueOperator" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">qnet</span></a><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>The policy is wrapped in a <code class="xref py py-class docutils literal notranslate"><span class="pre">torchrl.modules.OrnsteinUhlenbeckProcessWrapper</span></code>
exploration module:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.OrnsteinUhlenbeckProcessWrapper.html#torchrl.modules.tensordict_module.OrnsteinUhlenbeckProcessWrapper" title="torchrl.modules.tensordict_module.exploration.OrnsteinUhlenbeckProcessWrapper" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-exploration sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">actor_model_explore</span></a> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.OrnsteinUhlenbeckProcessWrapper.html#torchrl.modules.tensordict_module.OrnsteinUhlenbeckProcessWrapper" title="torchrl.modules.tensordict_module.exploration.OrnsteinUhlenbeckProcessWrapper" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-exploration sphx-glr-backref-type-py-class"><span class="n">OrnsteinUhlenbeckProcessWrapper</span></a><span class="p">(</span>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor" title="torchrl.modules.tensordict_module.actors.ProbabilisticActor" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">actor</span></a><span class="p">,</span>
    <span class="n">annealing_num_steps</span><span class="o">=</span><span class="n">annealing_frames</span><span class="p">,</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">device</span></a><span class="p">)</span>
<span class="k">if</span> <a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">device</span></a> <span class="o">==</span> <a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">device</span></a><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">):</span>
    <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.share_memory" title="torch.nn.Module.share_memory" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><span class="n">actor_model_explore</span><span class="o">.</span><span class="n">share_memory</span></a><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="parallel-environment-creation">
<h3>Parallel environment creation<a class="headerlink" href="#parallel-environment-creation" title="Permalink to this heading">¶</a></h3>
<p>We pass the stats computed earlier to normalize the output of our
environment:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><a href="https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv" title="torchrl.envs.transforms.transforms.TransformedEnv" class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">create_env_fn</span></a> <span class="o">=</span> <span class="n">parallel_env_constructor</span><span class="p">(</span>
    <span class="n">transform_state_dict</span><span class="o">=</span><span class="n">transform_state_dict</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="data-collector">
<h3>Data collector<a class="headerlink" href="#data-collector" title="Permalink to this heading">¶</a></h3>
<p>TorchRL provides specialized classes to help you collect data by executing
the policy in the environment. These “data collectors” iteratively compute
the action to be executed at a given time, then execute a step in the
environment and reset it when required.
Data collectors are designed to help developers have a tight control
on the number of frames per batch of data, on the (a)sync nature of this
collection and on the resources allocated to the data collection (e.g. GPU,
number of workers etc).</p>
<p>Here we will use
<code class="xref py py-class docutils literal notranslate"><span class="pre">torchrl.collectors.MultiaSyncDataCollector</span></code>, a data collector that
will be executed in an async manner (i.e. data will be collected while
the policy is being optimized). With the <code class="xref py py-class docutils literal notranslate"><span class="pre">MultiaSyncDataCollector</span></code>,
multiple workers are running rollouts separately. When a batch is asked, it
is gathered from the first worker that can provide it.</p>
<p>The parameters to specify are:</p>
<ul class="simple">
<li><p>the list of environment creation functions,</p></li>
<li><p>the policy,</p></li>
<li><p>the total number of frames before the collector is considered empty,</p></li>
<li><p>the maximum number of frames per trajectory (useful for non-terminating
environments, like dm_control ones).</p></li>
</ul>
<p>One should also pass:</p>
<ul class="simple">
<li><p>the number of frames in each batch collected,</p></li>
<li><p>the number of random steps executed independently from the policy,</p></li>
<li><p>the devices used for policy execution</p></li>
<li><p>the devices used to store data before the data is passed to the main
process.</p></li>
</ul>
<p>Collectors also accept post-processing hooks.
For instance, the <code class="xref py py-class docutils literal notranslate"><span class="pre">torchrl.data.postprocs.MultiStep</span></code> class passed as
<code class="docutils literal notranslate"><span class="pre">postproc</span></code> makes it so that the rewards of the <code class="docutils literal notranslate"><span class="pre">n</span></code> upcoming steps are
summed (with some discount factor) and the next observation is changed to
be the n-step forward observation. One could pass other transforms too:
using <a class="reference external" href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.nn.TensorDictModule.html#tensordict.nn.TensorDictModule" title="(in tensordict vmain (0.0.2b+696ac1a ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">tensordict.nn.TensorDictModule</span></code></a> and
<a class="reference external" href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.nn.TensorDictSequential.html#tensordict.nn.TensorDictSequential" title="(in tensordict vmain (0.0.2b+696ac1a ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">tensordict.nn.TensorDictSequential</span></code></a> we can seamlessly append a
wide range of transforms to our collector.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">n_steps_forward</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.data.MultiStep.html#torchrl.data.MultiStep" title="torchrl.data.MultiStep" class="sphx-glr-backref-module-torchrl-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">multistep</span></a> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.data.MultiStep.html#torchrl.data.MultiStep" title="torchrl.data.MultiStep" class="sphx-glr-backref-module-torchrl-data sphx-glr-backref-type-py-class"><span class="n">MultiStep</span></a><span class="p">(</span><span class="n">n_steps_max</span><span class="o">=</span><span class="n">n_steps_forward</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.data.MultiStep.html#torchrl.data.MultiStep" title="torchrl.data.MultiStep" class="sphx-glr-backref-module-torchrl-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">multistep</span></a> <span class="o">=</span> <span class="kc">None</span>

<a href="https://pytorch.org/rl/reference/generated/torchrl.collectors.collectors.MultiaSyncDataCollector.html#torchrl.collectors.collectors.MultiaSyncDataCollector" title="torchrl.collectors.collectors.MultiaSyncDataCollector" class="sphx-glr-backref-module-torchrl-collectors-collectors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">collector</span></a> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.collectors.collectors.MultiaSyncDataCollector.html#torchrl.collectors.collectors.MultiaSyncDataCollector" title="torchrl.collectors.collectors.MultiaSyncDataCollector" class="sphx-glr-backref-module-torchrl-collectors-collectors sphx-glr-backref-type-py-class"><span class="n">MultiaSyncDataCollector</span></a><span class="p">(</span>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv" title="torchrl.envs.transforms.transforms.TransformedEnv" class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">create_env_fn</span></a><span class="o">=</span><span class="p">[</span><a href="https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv" title="torchrl.envs.transforms.transforms.TransformedEnv" class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">create_env_fn</span></a><span class="p">,</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv" title="torchrl.envs.transforms.transforms.TransformedEnv" class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">create_env_fn</span></a><span class="p">],</span>
    <span class="n">policy</span><span class="o">=</span><a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.OrnsteinUhlenbeckProcessWrapper.html#torchrl.modules.tensordict_module.OrnsteinUhlenbeckProcessWrapper" title="torchrl.modules.tensordict_module.exploration.OrnsteinUhlenbeckProcessWrapper" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-exploration sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">actor_model_explore</span></a><span class="p">,</span>
    <span class="n">total_frames</span><span class="o">=</span><span class="n">total_frames</span><span class="p">,</span>
    <span class="n">max_frames_per_traj</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">frames_per_batch</span><span class="o">=</span><span class="n">frames_per_batch</span><span class="p">,</span>
    <span class="n">init_random_frames</span><span class="o">=</span><span class="n">init_random_frames</span><span class="p">,</span>
    <span class="n">reset_at_each_iter</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">postproc</span><span class="o">=</span><a href="https://pytorch.org/rl/reference/generated/torchrl.data.MultiStep.html#torchrl.data.MultiStep" title="torchrl.data.MultiStep" class="sphx-glr-backref-module-torchrl-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">multistep</span></a><span class="p">,</span>
    <span class="n">split_trajs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">devices</span><span class="o">=</span><span class="p">[</span><a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">device</span></a><span class="p">,</span> <a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">device</span></a><span class="p">],</span>  <span class="c1"># device for execution</span>
    <span class="n">storing_devices</span><span class="o">=</span><span class="p">[</span><a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">device</span></a><span class="p">,</span> <a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">device</span></a><span class="p">],</span>  <span class="c1"># device where data will be stored and passed</span>
    <span class="n">pin_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">update_at_each_batch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">exploration_mode</span><span class="o">=</span><span class="s2">&quot;random&quot;</span><span class="p">,</span>
<span class="p">)</span>

<a href="https://pytorch.org/rl/reference/generated/torchrl.collectors.collectors.MultiaSyncDataCollector.html#torchrl.collectors.collectors.MultiaSyncDataCollector.set_seed" title="torchrl.collectors.collectors.MultiaSyncDataCollector.set_seed" class="sphx-glr-backref-module-torchrl-collectors-collectors sphx-glr-backref-type-py-method"><span class="n">collector</span><span class="o">.</span><span class="n">set_seed</span></a><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>3018685293
</pre></div>
</div>
</section>
<section id="id3">
<h3>Replay buffer<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><a href="https://pytorch.org/rl/reference/generated/torchrl.data.TensorDictReplayBuffer.html#torchrl.data.TensorDictReplayBuffer" title="torchrl.data.TensorDictReplayBuffer" class="sphx-glr-backref-module-torchrl-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">replay_buffer</span></a> <span class="o">=</span> <span class="n">make_replay_buffer</span><span class="p">(</span><span class="n">buffer_size</span><span class="p">,</span> <span class="n">prefetch</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="recorder">
<h3>Recorder<a class="headerlink" href="#recorder" title="Permalink to this heading">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><a href="https://pytorch.org/rl/reference/generated/torchrl.trainers.Recorder.html#torchrl.trainers.Recorder" title="torchrl.trainers.Recorder" class="sphx-glr-backref-module-torchrl-trainers sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">recorder</span></a> <span class="o">=</span> <span class="n">make_recorder</span><span class="p">(</span><a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.OrnsteinUhlenbeckProcessWrapper.html#torchrl.modules.tensordict_module.OrnsteinUhlenbeckProcessWrapper" title="torchrl.modules.tensordict_module.exploration.OrnsteinUhlenbeckProcessWrapper" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-exploration sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">actor_model_explore</span></a><span class="p">,</span> <span class="n">transform_state_dict</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="optimizer">
<h3>Optimizer<a class="headerlink" href="#optimizer" title="Permalink to this heading">¶</a></h3>
<p>Finally, we will use the Adam optimizer for the policy and value network,
with the same learning rate for both.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">optimizer_actor</span></a> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class"><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span></a><span class="p">(</span><a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters" title="torch.nn.Module.parameters" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><span class="n">actor</span><span class="o">.</span><span class="n">parameters</span></a><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
<a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">optimizer_qnet</span></a> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class"><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span></a><span class="p">(</span><a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters" title="torch.nn.Module.parameters" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><span class="n">qnet</span><span class="o">.</span><span class="n">parameters</span></a><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
<span class="n">total_collection_steps</span> <span class="o">=</span> <span class="n">total_frames</span> <span class="o">//</span> <span class="n">frames_per_batch</span>

<a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR" title="torch.optim.lr_scheduler.CosineAnnealingLR" class="sphx-glr-backref-module-torch-optim-lr_scheduler sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">scheduler1</span></a> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR" title="torch.optim.lr_scheduler.CosineAnnealingLR" class="sphx-glr-backref-module-torch-optim-lr_scheduler sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span></a><span class="p">(</span>
    <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">optimizer_actor</span></a><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="n">total_collection_steps</span>
<span class="p">)</span>
<a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR" title="torch.optim.lr_scheduler.CosineAnnealingLR" class="sphx-glr-backref-module-torch-optim-lr_scheduler sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">scheduler2</span></a> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR" title="torch.optim.lr_scheduler.CosineAnnealingLR" class="sphx-glr-backref-module-torch-optim-lr_scheduler sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span></a><span class="p">(</span>
    <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">optimizer_qnet</span></a><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="n">total_collection_steps</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="time-to-train-the-policy">
<h2>Time to train the policy<a class="headerlink" href="#time-to-train-the-policy" title="Permalink to this heading">¶</a></h2>
<p>Some notes about the following training loop:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../reference/generated/torchrl.objectives.hold_out_net.html#torchrl.objectives.hold_out_net" title="torchrl.objectives.utils.hold_out_net"><code class="xref py py-func docutils literal notranslate"><span class="pre">torchrl.objectives.utils.hold_out_net()</span></code></a> is a TorchRL context manager
that temporarily sets <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.requires_grad_()</span></code> to False for
a designated set of network parameters. This is used to
prevent <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.backward()`()</span></code> from writing gradients on
parameters that need not to be differentiated given the loss at hand.</p></li>
<li><p>The value network is designed using the
<code class="xref py py-class docutils literal notranslate"><span class="pre">torchrl.modules.ValueOperator</span></code> subclass from
<a class="reference external" href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.nn.TensorDictModule.html#tensordict.nn.TensorDictModule" title="(in tensordict vmain (0.0.2b+696ac1a ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">tensordict.nn.TensorDictModule</span></code></a> class. As explained earlier,
this class will write a <code class="docutils literal notranslate"><span class="pre">&quot;state_action_value&quot;</span></code> entry if one of its
<code class="docutils literal notranslate"><span class="pre">in_keys</span></code> is named <code class="docutils literal notranslate"><span class="pre">&quot;action&quot;</span></code>, otherwise it will assume that only the
state-value is returned and the output key will simply be <code class="docutils literal notranslate"><span class="pre">&quot;state_value&quot;</span></code>.
In the case of DDPG, the value if of the state-action pair,
hence the <code class="docutils literal notranslate"><span class="pre">&quot;state_action_value&quot;</span></code> will be used.</p></li>
<li><p>The <code class="xref py py-func docutils literal notranslate"><span class="pre">torchrl.envs.utils.step_mdp(tensordict)()</span></code> helper function is the
equivalent of the <code class="docutils literal notranslate"><span class="pre">obs</span> <span class="pre">=</span> <span class="pre">next_obs</span></code> command found in multiple RL
algorithms. It will return a new <a class="reference external" href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="(in tensordict vmain (0.0.2b+696ac1a ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">tensordict.TensorDict</span></code></a> instance
that contains all the data that will need to be used in the next iteration.
This makes it possible to pass this new tensordict to the policy or
value network.</p></li>
<li><p>When using prioritized replay buffer, a priority key is added to the
sampled tensordict (named <code class="docutils literal notranslate"><span class="pre">&quot;td_error&quot;</span></code> by default). Then, this
TensorDict will be fed back to the replay buffer using the
<code class="xref py py-func docutils literal notranslate"><span class="pre">torchrl.data.replay_buffers.TensorDictReplayBuffer.update_tensordict_priority()</span></code>
method. Under the hood, this method will read the index present in the
TensorDict as well as the priority value, and update its list of priorities
at these indices.</p></li>
<li><p>TorchRL provides optimized versions of the loss functions (such as this one)
where one only needs to pass a sampled tensordict and obtains a dictionary
of losses and metadata in return (see <code class="xref py py-mod docutils literal notranslate"><span class="pre">torchrl.objectives</span></code> for more
context). Here we write the full loss function in the optimization loop
for transparency.
Similarly, the target network updates are written explicitly but
TorchRL provides a couple of dedicated classes for this
(see <a class="reference internal" href="../reference/generated/torchrl.objectives.SoftUpdate.html#torchrl.objectives.SoftUpdate" title="torchrl.objectives.SoftUpdate"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchrl.objectives.SoftUpdate</span></code></a> and
<a class="reference internal" href="../reference/generated/torchrl.objectives.HardUpdate.html#torchrl.objectives.HardUpdate" title="torchrl.objectives.HardUpdate"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchrl.objectives.HardUpdate</span></code></a>).</p></li>
<li><p>After each collection of data, we call <code class="xref py py-func docutils literal notranslate"><span class="pre">collector.update_policy_weights_()</span></code>,
which will update the policy network weights on the data collector. If the
code is executed on cpu or with a single cuda device, this part can be
omitted. If the collector is executed on another device, then its weights
must be synced with those on the main, training process and this method
should be incorporated in the training loop (ideally early in the loop in
async settings, and at the end of it in sync settings).</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">rewards_eval</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Main loop</span>
<span class="n">norm_factor_training</span> <span class="o">=</span> <span class="p">(</span>
    <span class="nb">sum</span><span class="p">(</span><span class="n">gamma</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps_forward</span><span class="p">))</span> <span class="k">if</span> <span class="n">n_steps_forward</span> <span class="k">else</span> <span class="mi">1</span>
<span class="p">)</span>

<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">collected_frames</span></a> <span class="o">=</span> <span class="mi">0</span>
<span class="n">pbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">total_frames</span><span class="p">)</span>
<span class="n">r0</span> <span class="o">=</span> <span class="kc">None</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">tensordict</span></a> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><a href="https://pytorch.org/rl/reference/generated/torchrl.collectors.collectors.MultiaSyncDataCollector.html#torchrl.collectors.collectors.MultiaSyncDataCollector" title="torchrl.collectors.collectors.MultiaSyncDataCollector" class="sphx-glr-backref-module-torchrl-collectors-collectors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">collector</span></a><span class="p">):</span>

    <span class="c1"># update weights of the inference policy</span>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.collectors.collectors.MultiaSyncDataCollector.html#torchrl.collectors.collectors.MultiaSyncDataCollector.update_policy_weights_" title="torchrl.collectors.collectors.MultiaSyncDataCollector.update_policy_weights_" class="sphx-glr-backref-module-torchrl-collectors-collectors sphx-glr-backref-type-py-method"><span class="n">collector</span><span class="o">.</span><span class="n">update_policy_weights_</span></a><span class="p">()</span>

    <span class="k">if</span> <span class="n">r0</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">r0</span> <span class="o">=</span> <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">tensordict</span></a><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.numel" title="tensordict.TensorDict.numel" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-method"><span class="n">tensordict</span><span class="o">.</span><span class="n">numel</span></a><span class="p">())</span>

    <span class="c1"># extend the replay buffer with the new data</span>
    <span class="k">if</span> <span class="p">(</span><span class="s2">&quot;collector&quot;</span><span class="p">,</span> <span class="s2">&quot;mask&quot;</span><span class="p">)</span> <span class="ow">in</span> <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.keys" title="tensordict.TensorDict.keys" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-method"><span class="n">tensordict</span><span class="o">.</span><span class="n">keys</span></a><span class="p">(</span><span class="kc">True</span><span class="p">):</span>
        <span class="c1"># if multi-step, a mask is present to help filter padded values</span>
        <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">current_frames</span></a> <span class="o">=</span> <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">tensordict</span></a><span class="p">[</span><span class="s2">&quot;collector&quot;</span><span class="p">,</span> <span class="s2">&quot;mask&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">tensordict</span></a> <span class="o">=</span> <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">tensordict</span></a><span class="p">[</span><a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.get" title="tensordict.TensorDict.get" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-method"><span class="n">tensordict</span><span class="o">.</span><span class="n">get</span></a><span class="p">((</span><span class="s2">&quot;collector&quot;</span><span class="p">,</span> <span class="s2">&quot;mask&quot;</span><span class="p">))]</span>
    <span class="k">else</span><span class="p">:</span>
        <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">tensordict</span></a> <span class="o">=</span> <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.view" title="tensordict.TensorDict.view" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-method"><span class="n">tensordict</span><span class="o">.</span><span class="n">view</span></a><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">current_frames</span></a> <span class="o">=</span> <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.numel" title="tensordict.TensorDict.numel" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-method"><span class="n">tensordict</span><span class="o">.</span><span class="n">numel</span></a><span class="p">()</span>
    <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">collected_frames</span></a> <span class="o">+=</span> <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">current_frames</span></a>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.data.TensorDictReplayBuffer.html#torchrl.data.TensorDictReplayBuffer.extend" title="torchrl.data.TensorDictReplayBuffer.extend" class="sphx-glr-backref-module-torchrl-data sphx-glr-backref-type-py-method"><span class="n">replay_buffer</span><span class="o">.</span><span class="n">extend</span></a><span class="p">(</span><a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.cpu" title="tensordict.TensorDict.cpu" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-method"><span class="n">tensordict</span><span class="o">.</span><span class="n">cpu</span></a><span class="p">())</span>

    <span class="c1"># optimization steps</span>
    <span class="k">if</span> <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">collected_frames</span></a> <span class="o">&gt;=</span> <span class="n">init_random_frames</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">update_to_data</span><span class="p">):</span>
            <span class="c1"># sample from replay buffer</span>
            <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sampled_tensordict</span></a> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.data.TensorDictReplayBuffer.html#torchrl.data.TensorDictReplayBuffer.sample" title="torchrl.data.TensorDictReplayBuffer.sample" class="sphx-glr-backref-module-torchrl-data sphx-glr-backref-type-py-method"><span class="n">replay_buffer</span><span class="o">.</span><span class="n">sample</span></a><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

            <span class="c1"># compute loss for qnet and backprop</span>
            <span class="k">with</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.objectives.hold_out_net.html#torchrl.objectives.hold_out_net" title="torchrl.objectives.hold_out_net" class="sphx-glr-backref-module-torchrl-objectives sphx-glr-backref-type-py-class"><span class="n">hold_out_net</span></a><span class="p">(</span><a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor" title="torchrl.modules.tensordict_module.actors.ProbabilisticActor" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">actor</span></a><span class="p">):</span>
                <span class="c1"># get next state value</span>
                <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">next_tensordict</span></a> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.utils.step_mdp.html#torchrl.envs.utils.step_mdp" title="torchrl.envs.utils.step_mdp" class="sphx-glr-backref-module-torchrl-envs-utils sphx-glr-backref-type-py-function"><span class="n">step_mdp</span></a><span class="p">(</span><a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sampled_tensordict</span></a><span class="p">)</span>
                <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator" title="torchrl.modules.tensordict_module.actors.ValueOperator" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">qnet_target</span></a><span class="p">(</span><a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor" title="torchrl.modules.tensordict_module.actors.ProbabilisticActor" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">actor</span></a><span class="p">(</span><a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">next_tensordict</span></a><span class="p">))</span>
                <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">next_value</span></a> <span class="o">=</span> <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">next_tensordict</span></a><span class="p">[</span><span class="s2">&quot;state_action_value&quot;</span><span class="p">]</span>
                <span class="k">assert</span> <span class="ow">not</span> <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">next_value</span></a><span class="o">.</span><span class="n">requires_grad</span>
            <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">value_est</span></a> <span class="o">=</span> <span class="p">(</span>
                <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sampled_tensordict</span></a><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span>
                <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sampled_tensordict</span></a><span class="p">[</span><span class="s2">&quot;done&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">())</span> <span class="o">*</span> <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">next_value</span></a>
            <span class="p">)</span>
            <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">value</span></a> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator" title="torchrl.modules.tensordict_module.actors.ValueOperator" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">qnet</span></a><span class="p">(</span><a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sampled_tensordict</span></a><span class="p">)[</span><span class="s2">&quot;state_action_value&quot;</span><span class="p">]</span>
            <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">value_loss</span></a> <span class="o">=</span> <span class="p">(</span><a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">value</span></a> <span class="o">-</span> <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">value_est</span></a><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
            <span class="c1"># we write the td_error in the sampled_tensordict for priority update</span>
            <span class="c1"># because the indices of the samples is tracked in sampled_tensordict</span>
            <span class="c1"># and the replay buffer will know which priorities to update.</span>
            <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sampled_tensordict</span></a><span class="p">[</span><span class="s2">&quot;td_error&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">value</span></a> <span class="o">-</span> <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">value_est</span></a><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward" title="torch.Tensor.backward" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-method"><span class="n">value_loss</span><span class="o">.</span><span class="n">backward</span></a><span class="p">()</span>

            <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">optimizer_qnet</span></a><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.zero_grad" title="torch.optim.Adam.zero_grad" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-method"><span class="n">optimizer_qnet</span><span class="o">.</span><span class="n">zero_grad</span></a><span class="p">()</span>

            <span class="c1"># compute loss for actor and backprop:</span>
            <span class="c1"># the actor must maximise the state-action value, hence the loss</span>
            <span class="c1"># is the neg value of this.</span>
            <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sampled_tensordict_actor</span></a> <span class="o">=</span> <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.select" title="tensordict.TensorDict.select" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-method"><span class="n">sampled_tensordict</span><span class="o">.</span><span class="n">select</span></a><span class="p">(</span><span class="o">*</span><a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor" title="torchrl.modules.tensordict_module.actors.ProbabilisticActor" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">actor</span></a><span class="o">.</span><span class="n">in_keys</span><span class="p">)</span>
            <span class="k">with</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.objectives.hold_out_net.html#torchrl.objectives.hold_out_net" title="torchrl.objectives.hold_out_net" class="sphx-glr-backref-module-torchrl-objectives sphx-glr-backref-type-py-class"><span class="n">hold_out_net</span></a><span class="p">(</span><a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator" title="torchrl.modules.tensordict_module.actors.ValueOperator" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">qnet</span></a><span class="p">):</span>
                <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator" title="torchrl.modules.tensordict_module.actors.ValueOperator" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">qnet</span></a><span class="p">(</span><a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor" title="torchrl.modules.tensordict_module.actors.ProbabilisticActor" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">actor</span></a><span class="p">(</span><a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sampled_tensordict_actor</span></a><span class="p">))</span>
            <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">actor_loss</span></a> <span class="o">=</span> <span class="o">-</span><a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sampled_tensordict_actor</span></a><span class="p">[</span><span class="s2">&quot;state_action_value&quot;</span><span class="p">]</span>
            <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">actor_loss</span></a><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

            <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">optimizer_actor</span></a><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.zero_grad" title="torch.optim.Adam.zero_grad" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-method"><span class="n">optimizer_actor</span><span class="o">.</span><span class="n">zero_grad</span></a><span class="p">()</span>

            <span class="c1"># update qnet_target params</span>
            <span class="k">for</span> <span class="p">(</span><a href="https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter" title="torch.nn.parameter.Parameter" class="sphx-glr-backref-module-torch-nn-parameter sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">p_in</span></a><span class="p">,</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter" title="torch.nn.parameter.Parameter" class="sphx-glr-backref-module-torch-nn-parameter sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">p_dest</span></a><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters" title="torch.nn.Module.parameters" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><span class="n">qnet</span><span class="o">.</span><span class="n">parameters</span></a><span class="p">(),</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters" title="torch.nn.Module.parameters" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><span class="n">qnet_target</span><span class="o">.</span><span class="n">parameters</span></a><span class="p">()):</span>
                <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">p_dest</span><span class="o">.</span><span class="n">data</span></a><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">tau</span> <span class="o">*</span> <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">p_in</span><span class="o">.</span><span class="n">data</span></a> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">tau</span><span class="p">)</span> <span class="o">*</span> <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">p_dest</span><span class="o">.</span><span class="n">data</span></a><span class="p">)</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">b_in</span><span class="p">,</span> <span class="n">b_dest</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.buffers" title="torch.nn.Module.buffers" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><span class="n">qnet</span><span class="o">.</span><span class="n">buffers</span></a><span class="p">(),</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.buffers" title="torch.nn.Module.buffers" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><span class="n">qnet_target</span><span class="o">.</span><span class="n">buffers</span></a><span class="p">()):</span>
                <span class="n">b_dest</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">tau</span> <span class="o">*</span> <span class="n">b_in</span><span class="o">.</span><span class="n">data</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">tau</span><span class="p">)</span> <span class="o">*</span> <span class="n">b_dest</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

            <span class="c1"># update priority</span>
            <span class="k">if</span> <span class="n">prb</span><span class="p">:</span>
                <a href="https://pytorch.org/rl/reference/generated/torchrl.data.TensorDictReplayBuffer.html#torchrl.data.TensorDictReplayBuffer" title="torchrl.data.TensorDictReplayBuffer" class="sphx-glr-backref-module-torchrl-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">replay_buffer</span></a><span class="o">.</span><span class="n">update_tensordict_priority</span><span class="p">(</span><a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sampled_tensordict</span></a><span class="p">)</span>

    <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="p">(</span><span class="n">i</span><span class="p">,</span> <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">tensordict</span></a><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="n">norm_factor_training</span> <span class="o">/</span> <span class="n">frame_skip</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">td_record</span> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.trainers.Recorder.html#torchrl.trainers.Recorder" title="torchrl.trainers.Recorder" class="sphx-glr-backref-module-torchrl-trainers sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">recorder</span></a><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">td_record</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">rewards_eval</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="n">td_record</span><span class="p">[</span><span class="s2">&quot;r_evaluation&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">rewards_eval</span><span class="p">):</span>
        <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;reward: </span><span class="si">{</span><span class="n">rewards</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2"> 4.4f</span><span class="si">}</span><span class="s2"> (r0 = </span><span class="si">{</span><span class="n">r0</span><span class="si">:</span><span class="s2"> 4.4f</span><span class="si">}</span><span class="s2">), reward eval: reward: </span><span class="si">{</span><span class="n">rewards_eval</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2"> 4.4f</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="c1"># update the exploration strategy</span>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.OrnsteinUhlenbeckProcessWrapper.html#torchrl.modules.tensordict_module.OrnsteinUhlenbeckProcessWrapper.step" title="torchrl.modules.tensordict_module.OrnsteinUhlenbeckProcessWrapper.step" class="sphx-glr-backref-module-torchrl-modules-tensordict_module sphx-glr-backref-type-py-method"><span class="n">actor_model_explore</span><span class="o">.</span><span class="n">step</span></a><span class="p">(</span><a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">current_frames</span></a><span class="p">)</span>
    <span class="k">if</span> <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">collected_frames</span></a> <span class="o">&gt;=</span> <span class="n">init_random_frames</span><span class="p">:</span>
        <a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR" title="torch.optim.lr_scheduler.CosineAnnealingLR" class="sphx-glr-backref-module-torch-optim-lr_scheduler sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">scheduler1</span></a><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR" title="torch.optim.lr_scheduler.CosineAnnealingLR" class="sphx-glr-backref-module-torch-optim-lr_scheduler sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">scheduler2</span></a><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<a href="https://pytorch.org/rl/reference/generated/torchrl.collectors.collectors.MultiaSyncDataCollector.html#torchrl.collectors.collectors.MultiaSyncDataCollector.shutdown" title="torchrl.collectors.collectors.MultiaSyncDataCollector.shutdown" class="sphx-glr-backref-module-torchrl-collectors-collectors sphx-glr-backref-type-py-method"><span class="n">collector</span><span class="o">.</span><span class="n">shutdown</span></a><span class="p">()</span>
<span class="k">del</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.collectors.collectors.MultiaSyncDataCollector.html#torchrl.collectors.collectors.MultiaSyncDataCollector" title="torchrl.collectors.collectors.MultiaSyncDataCollector" class="sphx-glr-backref-module-torchrl-collectors-collectors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">collector</span></a>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-pytb notranslate"><div class="highlight"><pre><span></span><span class="gt">Traceback (most recent call last):</span>
  File <span class="nb">&quot;/__w/rl/rl/docs/source/reference/generated/tutorials/coding_ddpg.py&quot;</span>, line <span class="m">786</span>, in <span class="n">&lt;module&gt;</span>
<span class="w">    </span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">tensordict</span></a> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><a href="https://pytorch.org/rl/reference/generated/torchrl.collectors.collectors.MultiaSyncDataCollector.html#torchrl.collectors.collectors.MultiaSyncDataCollector" title="torchrl.collectors.collectors.MultiaSyncDataCollector" class="sphx-glr-backref-module-torchrl-collectors-collectors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">collector</span></a><span class="p">):</span>
  File <span class="nb">&quot;/__w/rl/rl/torchrl/collectors/collectors.py&quot;</span>, line <span class="m">1424</span>, in <span class="n">iterator</span>
<span class="w">    </span><span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">postprocs</span><span class="p">[</span><span class="n">out</span><span class="o">.</span><a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">device</span></a><span class="p">](</span><span class="n">out</span><span class="p">)</span>
  File <span class="nb">&quot;/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py&quot;</span>, line <span class="m">1533</span>, in <span class="n">_call_impl</span>
<span class="w">    </span><span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  File <span class="nb">&quot;/__w/rl/rl/torchrl/data/postprocs/postprocs.py&quot;</span>, line <span class="m">180</span>, in <span class="n">forward</span>
<span class="w">    </span><span class="n">terminal</span><span class="p">,</span> <span class="n">post_terminal</span> <span class="o">=</span> <span class="n">_get_terminal</span><span class="p">(</span><span class="n">done</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_steps_max</span><span class="p">)</span>
  File <span class="nb">&quot;/__w/rl/rl/torchrl/data/postprocs/postprocs.py&quot;</span>, line <span class="m">38</span>, in <span class="n">_get_terminal</span>
<span class="w">    </span><span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Got more or less than one terminal state per episode.&quot;</span><span class="p">)</span>
<span class="gr">RuntimeError</span>: <span class="n">Got more or less than one terminal state per episode.</span>
</pre></div>
</div>
</section>
<section id="experiment-results">
<h2>Experiment results<a class="headerlink" href="#experiment-results" title="Permalink to this heading">¶</a></h2>
<p>We make a simple plot of the average rewards during training. We can observe
that our policy learned quite well to solve the task.</p>
<p><strong>Note</strong>: As already mentioned above, to get a more reasonable performance,
use a greater value for <code class="docutils literal notranslate"><span class="pre">total_frames</span></code> e.g. 1M.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">rewards</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;training&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">rewards_eval</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;eval&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;iter&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;reward&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="sampling-trajectories-and-using-td-lambda">
<h2>Sampling trajectories and using TD(lambda)<a class="headerlink" href="#sampling-trajectories-and-using-td-lambda" title="Permalink to this heading">¶</a></h2>
<p>TD(lambda) is known to be less biased than the regular TD-error we used in
the previous example. To use it, however, we need to sample trajectories and
not single transitions.</p>
<p>We modify the previous example to make this possible.</p>
<p>The first modification consists in building a replay buffer that stores
trajectories (and not transitions).</p>
<p>Specifically, we’ll collect trajectories of (at most)
250 steps (note that the total trajectory length is actually 1000 frames, but
we collect batches of 500 transitions obtained over 2 environments running in
parallel, hence only 250 steps per trajectory are collected at any given
time). Hence, we’ll divide our replay buffer size by 250:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">buffer_size</span> <span class="o">=</span> <span class="mi">100000</span> <span class="o">//</span> <span class="n">frame_skip</span> <span class="o">//</span> <span class="mi">250</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;the new buffer size is&quot;</span><span class="p">,</span> <span class="n">buffer_size</span><span class="p">)</span>
<span class="n">batch_size_traj</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">//</span> <span class="mi">250</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;the new batch size for trajectories is&quot;</span><span class="p">,</span> <span class="n">batch_size_traj</span><span class="p">)</span>

<span class="n">n_steps_forward</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># disable multi-step for simplicity</span>
</pre></div>
</div>
<p>The following code is identical to the initialization we made earlier:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><a href="https://pytorch.org/docs/stable/generated/torch.manual_seed.html#torch.manual_seed" title="torch.manual_seed" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span></a><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="c1"># get stats for normalization</span>
<span class="n">transform_state_dict</span> <span class="o">=</span> <span class="n">get_env_stats</span><span class="p">()</span>

<span class="c1"># Actor and qnet instantiation</span>
<a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor" title="torchrl.modules.tensordict_module.actors.ProbabilisticActor" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">actor</span></a><span class="p">,</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator" title="torchrl.modules.tensordict_module.actors.ValueOperator" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">qnet</span></a> <span class="o">=</span> <span class="n">make_ddpg_actor</span><span class="p">(</span>
    <span class="n">transform_state_dict</span><span class="o">=</span><span class="n">transform_state_dict</span><span class="p">,</span>
    <a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">device</span></a><span class="o">=</span><a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">device</span></a><span class="p">,</span>
<span class="p">)</span>
<span class="k">if</span> <a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">device</span></a> <span class="o">==</span> <a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">device</span></a><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">):</span>
    <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.share_memory" title="torch.nn.Module.share_memory" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><span class="n">actor</span><span class="o">.</span><span class="n">share_memory</span></a><span class="p">()</span>

<span class="c1"># Target network</span>
<a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator" title="torchrl.modules.tensordict_module.actors.ValueOperator" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">qnet_target</span></a> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator" title="torchrl.modules.tensordict_module.actors.ValueOperator" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">qnet</span></a><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Exploration wrappers:</span>
<a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.OrnsteinUhlenbeckProcessWrapper.html#torchrl.modules.tensordict_module.OrnsteinUhlenbeckProcessWrapper" title="torchrl.modules.tensordict_module.exploration.OrnsteinUhlenbeckProcessWrapper" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-exploration sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">actor_model_explore</span></a> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.OrnsteinUhlenbeckProcessWrapper.html#torchrl.modules.tensordict_module.OrnsteinUhlenbeckProcessWrapper" title="torchrl.modules.tensordict_module.exploration.OrnsteinUhlenbeckProcessWrapper" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-exploration sphx-glr-backref-type-py-class"><span class="n">OrnsteinUhlenbeckProcessWrapper</span></a><span class="p">(</span>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor" title="torchrl.modules.tensordict_module.actors.ProbabilisticActor" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">actor</span></a><span class="p">,</span>
    <span class="n">annealing_num_steps</span><span class="o">=</span><span class="n">annealing_frames</span><span class="p">,</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">device</span></a><span class="p">)</span>
<span class="k">if</span> <a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">device</span></a> <span class="o">==</span> <a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">device</span></a><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">):</span>
    <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.share_memory" title="torch.nn.Module.share_memory" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><span class="n">actor_model_explore</span><span class="o">.</span><span class="n">share_memory</span></a><span class="p">()</span>

<span class="c1"># Environment setting:</span>
<a href="https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv" title="torchrl.envs.transforms.transforms.TransformedEnv" class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">create_env_fn</span></a> <span class="o">=</span> <span class="n">parallel_env_constructor</span><span class="p">(</span>
    <span class="n">transform_state_dict</span><span class="o">=</span><span class="n">transform_state_dict</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Batch collector:</span>
<span class="k">if</span> <span class="n">n_steps_forward</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.data.MultiStep.html#torchrl.data.MultiStep" title="torchrl.data.MultiStep" class="sphx-glr-backref-module-torchrl-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">multistep</span></a> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.data.MultiStep.html#torchrl.data.MultiStep" title="torchrl.data.MultiStep" class="sphx-glr-backref-module-torchrl-data sphx-glr-backref-type-py-class"><span class="n">MultiStep</span></a><span class="p">(</span><span class="n">n_steps_max</span><span class="o">=</span><span class="n">n_steps_forward</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.data.MultiStep.html#torchrl.data.MultiStep" title="torchrl.data.MultiStep" class="sphx-glr-backref-module-torchrl-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">multistep</span></a> <span class="o">=</span> <span class="kc">None</span>
<a href="https://pytorch.org/rl/reference/generated/torchrl.collectors.collectors.MultiaSyncDataCollector.html#torchrl.collectors.collectors.MultiaSyncDataCollector" title="torchrl.collectors.collectors.MultiaSyncDataCollector" class="sphx-glr-backref-module-torchrl-collectors-collectors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">collector</span></a> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.collectors.collectors.MultiaSyncDataCollector.html#torchrl.collectors.collectors.MultiaSyncDataCollector" title="torchrl.collectors.collectors.MultiaSyncDataCollector" class="sphx-glr-backref-module-torchrl-collectors-collectors sphx-glr-backref-type-py-class"><span class="n">MultiaSyncDataCollector</span></a><span class="p">(</span>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv" title="torchrl.envs.transforms.transforms.TransformedEnv" class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">create_env_fn</span></a><span class="o">=</span><span class="p">[</span><a href="https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv" title="torchrl.envs.transforms.transforms.TransformedEnv" class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">create_env_fn</span></a><span class="p">,</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv" title="torchrl.envs.transforms.transforms.TransformedEnv" class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">create_env_fn</span></a><span class="p">],</span>
    <span class="n">policy</span><span class="o">=</span><a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.OrnsteinUhlenbeckProcessWrapper.html#torchrl.modules.tensordict_module.OrnsteinUhlenbeckProcessWrapper" title="torchrl.modules.tensordict_module.exploration.OrnsteinUhlenbeckProcessWrapper" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-exploration sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">actor_model_explore</span></a><span class="p">,</span>
    <span class="n">total_frames</span><span class="o">=</span><span class="n">total_frames</span><span class="p">,</span>
    <span class="n">max_frames_per_traj</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">frames_per_batch</span><span class="o">=</span><span class="n">frames_per_batch</span><span class="p">,</span>
    <span class="n">init_random_frames</span><span class="o">=</span><span class="n">init_random_frames</span><span class="p">,</span>
    <span class="n">reset_at_each_iter</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">postproc</span><span class="o">=</span><a href="https://pytorch.org/rl/reference/generated/torchrl.data.MultiStep.html#torchrl.data.MultiStep" title="torchrl.data.MultiStep" class="sphx-glr-backref-module-torchrl-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">multistep</span></a><span class="p">,</span>
    <span class="n">split_trajs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">devices</span><span class="o">=</span><span class="p">[</span><a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">device</span></a><span class="p">,</span> <a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">device</span></a><span class="p">],</span>  <span class="c1"># device for execution</span>
    <span class="n">storing_devices</span><span class="o">=</span><span class="p">[</span><a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">device</span></a><span class="p">,</span> <a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="torch.device" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">device</span></a><span class="p">],</span>  <span class="c1"># device where data will be stored and passed</span>
    <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">pin_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">update_at_each_batch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">exploration_mode</span><span class="o">=</span><span class="s2">&quot;random&quot;</span><span class="p">,</span>
<span class="p">)</span>
<a href="https://pytorch.org/rl/reference/generated/torchrl.collectors.collectors.MultiaSyncDataCollector.html#torchrl.collectors.collectors.MultiaSyncDataCollector.set_seed" title="torchrl.collectors.collectors.MultiaSyncDataCollector.set_seed" class="sphx-glr-backref-module-torchrl-collectors-collectors sphx-glr-backref-type-py-method"><span class="n">collector</span><span class="o">.</span><span class="n">set_seed</span></a><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="c1"># Replay buffer:</span>
<a href="https://pytorch.org/rl/reference/generated/torchrl.data.TensorDictReplayBuffer.html#torchrl.data.TensorDictReplayBuffer" title="torchrl.data.TensorDictReplayBuffer" class="sphx-glr-backref-module-torchrl-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">replay_buffer</span></a> <span class="o">=</span> <span class="n">make_replay_buffer</span><span class="p">(</span><span class="n">buffer_size</span><span class="p">,</span> <span class="n">prefetch</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># trajectory recorder</span>
<a href="https://pytorch.org/rl/reference/generated/torchrl.trainers.Recorder.html#torchrl.trainers.Recorder" title="torchrl.trainers.Recorder" class="sphx-glr-backref-module-torchrl-trainers sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">recorder</span></a> <span class="o">=</span> <span class="n">make_recorder</span><span class="p">(</span><a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.OrnsteinUhlenbeckProcessWrapper.html#torchrl.modules.tensordict_module.OrnsteinUhlenbeckProcessWrapper" title="torchrl.modules.tensordict_module.exploration.OrnsteinUhlenbeckProcessWrapper" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-exploration sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">actor_model_explore</span></a><span class="p">,</span> <span class="n">transform_state_dict</span><span class="p">)</span>

<span class="c1"># Optimizers</span>
<a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">optimizer_actor</span></a> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class"><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span></a><span class="p">(</span><a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters" title="torch.nn.Module.parameters" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><span class="n">actor</span><span class="o">.</span><span class="n">parameters</span></a><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
<a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">optimizer_qnet</span></a> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class"><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span></a><span class="p">(</span><a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters" title="torch.nn.Module.parameters" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><span class="n">qnet</span><span class="o">.</span><span class="n">parameters</span></a><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
<span class="n">total_collection_steps</span> <span class="o">=</span> <span class="n">total_frames</span> <span class="o">//</span> <span class="n">frames_per_batch</span>

<a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR" title="torch.optim.lr_scheduler.CosineAnnealingLR" class="sphx-glr-backref-module-torch-optim-lr_scheduler sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">scheduler1</span></a> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR" title="torch.optim.lr_scheduler.CosineAnnealingLR" class="sphx-glr-backref-module-torch-optim-lr_scheduler sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span></a><span class="p">(</span>
    <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">optimizer_actor</span></a><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="n">total_collection_steps</span>
<span class="p">)</span>
<a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR" title="torch.optim.lr_scheduler.CosineAnnealingLR" class="sphx-glr-backref-module-torch-optim-lr_scheduler sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">scheduler2</span></a> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR" title="torch.optim.lr_scheduler.CosineAnnealingLR" class="sphx-glr-backref-module-torch-optim-lr_scheduler sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span></a><span class="p">(</span>
    <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">optimizer_qnet</span></a><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="n">total_collection_steps</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The training loop needs to be slightly adapted.
First, whereas before extending the replay buffer we used to flatten the
collected data, this won’t be the case anymore. To understand why, let’s
check the output shape of the data collector:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.collectors.collectors.MultiaSyncDataCollector.html#torchrl.collectors.collectors.MultiaSyncDataCollector" title="torchrl.collectors.collectors.MultiaSyncDataCollector" class="sphx-glr-backref-module-torchrl-collectors-collectors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">collector</span></a><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">break</span>
</pre></div>
</div>
<p>We see that our data has shape <code class="docutils literal notranslate"><span class="pre">[2,</span> <span class="pre">250]</span></code> as expected: 2 envs, each
returning 250 frames.</p>
<p>Let’s import the td_lambda function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchrl.objectives.value.functional</span> <span class="kn">import</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.objectives.value.functional.vec_td_lambda_advantage_estimate.html#torchrl.objectives.value.functional.vec_td_lambda_advantage_estimate" title="torchrl.objectives.value.functional.vec_td_lambda_advantage_estimate" class="sphx-glr-backref-module-torchrl-objectives-value-functional sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">vec_td_lambda_advantage_estimate</span></a>

<span class="n">lmbda</span> <span class="o">=</span> <span class="mf">0.95</span>
</pre></div>
</div>
<p>The training loop is roughly the same as before, with the exception that we
don’t flatten the collected data. Also, the sampling from the replay buffer
is slightly different: We will collect at minimum four trajectories, compute
the returns (TD(lambda)), then sample from these the values we’ll be using
to compute gradients. This ensures that do not have batches that are
‘too big’ but still compute an accurate return.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">rewards_eval</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Main loop</span>
<span class="n">norm_factor_training</span> <span class="o">=</span> <span class="p">(</span>
    <span class="nb">sum</span><span class="p">(</span><span class="n">gamma</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps_forward</span><span class="p">))</span> <span class="k">if</span> <span class="n">n_steps_forward</span> <span class="k">else</span> <span class="mi">1</span>
<span class="p">)</span>

<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">collected_frames</span></a> <span class="o">=</span> <span class="mi">0</span>
<span class="c1"># # if tqdm is to be used</span>
<span class="c1"># pbar = tqdm.tqdm(total=total_frames)</span>
<span class="n">r0</span> <span class="o">=</span> <span class="kc">None</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">tensordict</span></a> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><a href="https://pytorch.org/rl/reference/generated/torchrl.collectors.collectors.MultiaSyncDataCollector.html#torchrl.collectors.collectors.MultiaSyncDataCollector" title="torchrl.collectors.collectors.MultiaSyncDataCollector" class="sphx-glr-backref-module-torchrl-collectors-collectors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">collector</span></a><span class="p">):</span>

    <span class="c1"># update weights of the inference policy</span>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.collectors.collectors.MultiaSyncDataCollector.html#torchrl.collectors.collectors.MultiaSyncDataCollector.update_policy_weights_" title="torchrl.collectors.collectors.MultiaSyncDataCollector.update_policy_weights_" class="sphx-glr-backref-module-torchrl-collectors-collectors sphx-glr-backref-type-py-method"><span class="n">collector</span><span class="o">.</span><span class="n">update_policy_weights_</span></a><span class="p">()</span>

    <span class="k">if</span> <span class="n">r0</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">r0</span> <span class="o">=</span> <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">tensordict</span></a><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="c1"># extend the replay buffer with the new data</span>
    <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">current_frames</span></a> <span class="o">=</span> <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.numel" title="tensordict.TensorDict.numel" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-method"><span class="n">tensordict</span><span class="o">.</span><span class="n">numel</span></a><span class="p">()</span>
    <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">collected_frames</span></a> <span class="o">+=</span> <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">current_frames</span></a>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.data.TensorDictReplayBuffer.html#torchrl.data.TensorDictReplayBuffer.extend" title="torchrl.data.TensorDictReplayBuffer.extend" class="sphx-glr-backref-module-torchrl-data sphx-glr-backref-type-py-method"><span class="n">replay_buffer</span><span class="o">.</span><span class="n">extend</span></a><span class="p">(</span><a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.cpu" title="tensordict.TensorDict.cpu" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-method"><span class="n">tensordict</span><span class="o">.</span><span class="n">cpu</span></a><span class="p">())</span>

    <span class="c1"># optimization steps</span>
    <span class="k">if</span> <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">collected_frames</span></a> <span class="o">&gt;=</span> <span class="n">init_random_frames</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">update_to_data</span><span class="p">):</span>
            <span class="c1"># sample from replay buffer</span>
            <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sampled_tensordict</span></a> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.data.TensorDictReplayBuffer.html#torchrl.data.TensorDictReplayBuffer.sample" title="torchrl.data.TensorDictReplayBuffer.sample" class="sphx-glr-backref-module-torchrl-data sphx-glr-backref-type-py-method"><span class="n">replay_buffer</span><span class="o">.</span><span class="n">sample</span></a><span class="p">(</span><span class="n">batch_size_traj</span><span class="p">)</span>
            <span class="c1"># reset the batch size temporarily, and exclude index</span>
            <span class="c1"># whose shape is incompatible with the new size</span>
            <span class="n">index</span> <span class="o">=</span> <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.get" title="tensordict.TensorDict.get" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-method"><span class="n">sampled_tensordict</span><span class="o">.</span><span class="n">get</span></a><span class="p">(</span><span class="s2">&quot;index&quot;</span><span class="p">)</span>
            <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sampled_tensordict</span></a><span class="o">.</span><span class="n">exclude</span><span class="p">(</span><span class="s2">&quot;index&quot;</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="c1"># compute loss for qnet and backprop</span>
            <span class="k">with</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.objectives.hold_out_net.html#torchrl.objectives.hold_out_net" title="torchrl.objectives.hold_out_net" class="sphx-glr-backref-module-torchrl-objectives sphx-glr-backref-type-py-class"><span class="n">hold_out_net</span></a><span class="p">(</span><a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor" title="torchrl.modules.tensordict_module.actors.ProbabilisticActor" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">actor</span></a><span class="p">):</span>
                <span class="c1"># get next state value</span>
                <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">next_tensordict</span></a> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.utils.step_mdp.html#torchrl.envs.utils.step_mdp" title="torchrl.envs.utils.step_mdp" class="sphx-glr-backref-module-torchrl-envs-utils sphx-glr-backref-type-py-function"><span class="n">step_mdp</span></a><span class="p">(</span><a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sampled_tensordict</span></a><span class="p">)</span>
                <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator" title="torchrl.modules.tensordict_module.actors.ValueOperator" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">qnet_target</span></a><span class="p">(</span><a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor" title="torchrl.modules.tensordict_module.actors.ProbabilisticActor" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">actor</span></a><span class="p">(</span><a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.view" title="tensordict.TensorDict.view" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-method"><span class="n">next_tensordict</span><span class="o">.</span><span class="n">view</span></a><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                    <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.shape" title="tensordict.TensorDict.shape" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-property"><span class="n">sampled_tensordict</span><span class="o">.</span><span class="n">shape</span></a>
                <span class="p">)</span>
                <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">next_value</span></a> <span class="o">=</span> <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">next_tensordict</span></a><span class="p">[</span><span class="s2">&quot;state_action_value&quot;</span><span class="p">]</span>
                <span class="k">assert</span> <span class="ow">not</span> <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">next_value</span></a><span class="o">.</span><span class="n">requires_grad</span>

            <span class="c1"># This is the crucial part: we&#39;ll compute the TD(lambda)</span>
            <span class="c1"># instead of a simple single step estimate</span>
            <span class="n">done</span> <span class="o">=</span> <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sampled_tensordict</span></a><span class="p">[</span><span class="s2">&quot;done&quot;</span><span class="p">]</span>
            <span class="n">reward</span> <span class="o">=</span> <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sampled_tensordict</span></a><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span>
            <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">value</span></a> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator" title="torchrl.modules.tensordict_module.actors.ValueOperator" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">qnet</span></a><span class="p">(</span><a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.view" title="tensordict.TensorDict.view" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-method"><span class="n">sampled_tensordict</span><span class="o">.</span><span class="n">view</span></a><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.shape" title="tensordict.TensorDict.shape" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-property"><span class="n">sampled_tensordict</span><span class="o">.</span><span class="n">shape</span></a><span class="p">)[</span>
                <span class="s2">&quot;state_action_value&quot;</span>
            <span class="p">]</span>
            <span class="n">advantage</span> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.objectives.value.functional.vec_td_lambda_advantage_estimate.html#torchrl.objectives.value.functional.vec_td_lambda_advantage_estimate" title="torchrl.objectives.value.functional.vec_td_lambda_advantage_estimate" class="sphx-glr-backref-module-torchrl-objectives-value-functional sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">vec_td_lambda_advantage_estimate</span></a><span class="p">(</span>
                <span class="n">gamma</span><span class="p">,</span> <span class="n">lmbda</span><span class="p">,</span> <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">value</span></a><span class="p">,</span> <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">next_value</span></a><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span>
            <span class="p">)</span>
            <span class="c1"># we sample from the values we have computed</span>
            <span class="n">rand_idx</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint" title="torch.randint" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">randint</span></a><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">advantage</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,))</span>
            <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">value_loss</span></a> <span class="o">=</span> <span class="n">advantage</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)[</span><span class="n">rand_idx</span><span class="p">]</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

            <span class="c1"># we write the td_error in the sampled_tensordict for priority update</span>
            <span class="c1"># because the indices of the samples is tracked in sampled_tensordict</span>
            <span class="c1"># and the replay buffer will know which priorities to update.</span>
            <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward" title="torch.Tensor.backward" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-method"><span class="n">value_loss</span><span class="o">.</span><span class="n">backward</span></a><span class="p">()</span>

            <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">optimizer_qnet</span></a><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.zero_grad" title="torch.optim.Adam.zero_grad" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-method"><span class="n">optimizer_qnet</span><span class="o">.</span><span class="n">zero_grad</span></a><span class="p">()</span>

            <span class="c1"># compute loss for actor and backprop: the actor must maximise the state-action value, hence the loss is the neg value of this.</span>
            <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sampled_tensordict_actor</span></a> <span class="o">=</span> <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.select" title="tensordict.TensorDict.select" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-method"><span class="n">sampled_tensordict</span><span class="o">.</span><span class="n">select</span></a><span class="p">(</span><span class="o">*</span><a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor" title="torchrl.modules.tensordict_module.actors.ProbabilisticActor" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">actor</span></a><span class="o">.</span><span class="n">in_keys</span><span class="p">)</span>
            <span class="k">with</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.objectives.hold_out_net.html#torchrl.objectives.hold_out_net" title="torchrl.objectives.hold_out_net" class="sphx-glr-backref-module-torchrl-objectives sphx-glr-backref-type-py-class"><span class="n">hold_out_net</span></a><span class="p">(</span><a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator" title="torchrl.modules.tensordict_module.actors.ValueOperator" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">qnet</span></a><span class="p">):</span>
                <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ValueOperator.html#torchrl.modules.tensordict_module.ValueOperator" title="torchrl.modules.tensordict_module.actors.ValueOperator" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">qnet</span></a><span class="p">(</span><a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.ProbabilisticActor.html#torchrl.modules.tensordict_module.ProbabilisticActor" title="torchrl.modules.tensordict_module.actors.ProbabilisticActor" class="sphx-glr-backref-module-torchrl-modules-tensordict_module-actors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">actor</span></a><span class="p">(</span><a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.view" title="tensordict.TensorDict.view" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-method"><span class="n">sampled_tensordict_actor</span><span class="o">.</span><span class="n">view</span></a><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                    <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.shape" title="tensordict.TensorDict.shape" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-property"><span class="n">sampled_tensordict</span><span class="o">.</span><span class="n">shape</span></a>
                <span class="p">)</span>
            <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">actor_loss</span></a> <span class="o">=</span> <span class="o">-</span><a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sampled_tensordict_actor</span></a><span class="p">[</span><span class="s2">&quot;state_action_value&quot;</span><span class="p">]</span>
            <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">actor_loss</span></a><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)[</span><span class="n">rand_idx</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

            <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">optimizer_actor</span></a><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.zero_grad" title="torch.optim.Adam.zero_grad" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-method"><span class="n">optimizer_actor</span><span class="o">.</span><span class="n">zero_grad</span></a><span class="p">()</span>

            <span class="c1"># update qnet_target params</span>
            <span class="k">for</span> <span class="p">(</span><a href="https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter" title="torch.nn.parameter.Parameter" class="sphx-glr-backref-module-torch-nn-parameter sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">p_in</span></a><span class="p">,</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter" title="torch.nn.parameter.Parameter" class="sphx-glr-backref-module-torch-nn-parameter sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">p_dest</span></a><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters" title="torch.nn.Module.parameters" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><span class="n">qnet</span><span class="o">.</span><span class="n">parameters</span></a><span class="p">(),</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters" title="torch.nn.Module.parameters" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><span class="n">qnet_target</span><span class="o">.</span><span class="n">parameters</span></a><span class="p">()):</span>
                <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">p_dest</span><span class="o">.</span><span class="n">data</span></a><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">tau</span> <span class="o">*</span> <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">p_in</span><span class="o">.</span><span class="n">data</span></a> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">tau</span><span class="p">)</span> <span class="o">*</span> <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">p_dest</span><span class="o">.</span><span class="n">data</span></a><span class="p">)</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">b_in</span><span class="p">,</span> <span class="n">b_dest</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.buffers" title="torch.nn.Module.buffers" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><span class="n">qnet</span><span class="o">.</span><span class="n">buffers</span></a><span class="p">(),</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.buffers" title="torch.nn.Module.buffers" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><span class="n">qnet_target</span><span class="o">.</span><span class="n">buffers</span></a><span class="p">()):</span>
                <span class="n">b_dest</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">tau</span> <span class="o">*</span> <span class="n">b_in</span><span class="o">.</span><span class="n">data</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">tau</span><span class="p">)</span> <span class="o">*</span> <span class="n">b_dest</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

            <span class="c1"># update priority</span>
            <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.batch_size" title="tensordict.TensorDict.batch_size" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-property"><span class="n">sampled_tensordict</span><span class="o">.</span><span class="n">batch_size</span></a> <span class="o">=</span> <span class="p">[</span><span class="n">batch_size_traj</span><span class="p">]</span>
            <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sampled_tensordict</span></a><span class="p">[</span><span class="s2">&quot;td_error&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">advantage</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sampled_tensordict</span></a><span class="p">[</span><span class="s2">&quot;index&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">index</span>
            <span class="k">if</span> <span class="n">prb</span><span class="p">:</span>
                <a href="https://pytorch.org/rl/reference/generated/torchrl.data.TensorDictReplayBuffer.html#torchrl.data.TensorDictReplayBuffer" title="torchrl.data.TensorDictReplayBuffer" class="sphx-glr-backref-module-torchrl-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">replay_buffer</span></a><span class="o">.</span><span class="n">update_tensordict_priority</span><span class="p">(</span><a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sampled_tensordict</span></a><span class="p">)</span>

    <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="p">(</span><span class="n">i</span><span class="p">,</span> <a href="https://pytorch-labs.github.io/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">tensordict</span></a><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="n">norm_factor_training</span> <span class="o">/</span> <span class="n">frame_skip</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">td_record</span> <span class="o">=</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.trainers.Recorder.html#torchrl.trainers.Recorder" title="torchrl.trainers.Recorder" class="sphx-glr-backref-module-torchrl-trainers sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">recorder</span></a><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">td_record</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">rewards_eval</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="n">td_record</span><span class="p">[</span><span class="s2">&quot;r_evaluation&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
    <span class="c1">#     if len(rewards_eval):</span>
    <span class="c1">#         pbar.set_description(f&quot;reward: {rewards[-1][1]: 4.4f} (r0 = {r0: 4.4f}), reward eval: reward: {rewards_eval[-1][1]: 4.4f}&quot;)</span>

    <span class="c1"># update the exploration strategy</span>
    <a href="https://pytorch.org/rl/reference/generated/torchrl.modules.tensordict_module.OrnsteinUhlenbeckProcessWrapper.html#torchrl.modules.tensordict_module.OrnsteinUhlenbeckProcessWrapper.step" title="torchrl.modules.tensordict_module.OrnsteinUhlenbeckProcessWrapper.step" class="sphx-glr-backref-module-torchrl-modules-tensordict_module sphx-glr-backref-type-py-method"><span class="n">actor_model_explore</span><span class="o">.</span><span class="n">step</span></a><span class="p">(</span><a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">current_frames</span></a><span class="p">)</span>
    <span class="k">if</span> <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">collected_frames</span></a> <span class="o">&gt;=</span> <span class="n">init_random_frames</span><span class="p">:</span>
        <a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR" title="torch.optim.lr_scheduler.CosineAnnealingLR" class="sphx-glr-backref-module-torch-optim-lr_scheduler sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">scheduler1</span></a><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR" title="torch.optim.lr_scheduler.CosineAnnealingLR" class="sphx-glr-backref-module-torch-optim-lr_scheduler sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">scheduler2</span></a><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<a href="https://pytorch.org/rl/reference/generated/torchrl.collectors.collectors.MultiaSyncDataCollector.html#torchrl.collectors.collectors.MultiaSyncDataCollector.shutdown" title="torchrl.collectors.collectors.MultiaSyncDataCollector.shutdown" class="sphx-glr-backref-module-torchrl-collectors-collectors sphx-glr-backref-type-py-method"><span class="n">collector</span><span class="o">.</span><span class="n">shutdown</span></a><span class="p">()</span>
<span class="k">del</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.TransformedEnv.html#torchrl.envs.transforms.TransformedEnv" title="torchrl.envs.transforms.transforms.TransformedEnv" class="sphx-glr-backref-module-torchrl-envs-transforms-transforms sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">create_env_fn</span></a>
<span class="k">del</span> <a href="https://pytorch.org/rl/reference/generated/torchrl.collectors.collectors.MultiaSyncDataCollector.html#torchrl.collectors.collectors.MultiaSyncDataCollector" title="torchrl.collectors.collectors.MultiaSyncDataCollector" class="sphx-glr-backref-module-torchrl-collectors-collectors sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">collector</span></a>
</pre></div>
</div>
<p>We can observe that using TD(lambda) made our results considerably more
stable for a similar training speed:</p>
<p><strong>Note</strong>: As already mentioned above, to get a more reasonable performance,
use a greater value for <code class="docutils literal notranslate"><span class="pre">total_frames</span></code> e.g. 1000000.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">rewards</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;training&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">rewards_eval</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;eval&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;iter&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;reward&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;TD-labmda DDPG results&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 1 minutes  28.138 seconds)</p>
<p><strong>Estimated memory usage:</strong>  5312 MB</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-tutorials-coding-ddpg-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/0dfdb6ef4cc9782fdb39c4a9d0900d37/coding_ddpg.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">coding_ddpg.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/7ef773e36507adbc389133424f663224/coding_ddpg.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">coding_ddpg.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="coding_dqn.html" class="btn btn-neutral float-right" title="Coding a pixel-based DQN using TorchRL" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="multi_task.html" class="btn btn-neutral" title="Task-specific policy in multi-task environments" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, Meta.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Coding DDPG using TorchRL</a><ul>
<li><a class="reference internal" href="#imports">Imports</a></li>
<li><a class="reference internal" href="#environment">Environment</a><ul>
<li><a class="reference internal" href="#transforms">Transforms</a></li>
<li><a class="reference internal" href="#normalization-of-the-observations">Normalization of the observations</a></li>
<li><a class="reference internal" href="#parallel-execution">Parallel execution</a></li>
</ul>
</li>
<li><a class="reference internal" href="#building-the-model">Building the model</a></li>
<li><a class="reference internal" href="#evaluator-building-your-recorder-object">Evaluator: building your recorder object</a></li>
<li><a class="reference internal" href="#replay-buffer">Replay buffer</a></li>
<li><a class="reference internal" href="#hyperparameters">Hyperparameters</a><ul>
<li><a class="reference internal" href="#id1">Environment</a></li>
<li><a class="reference internal" href="#collection">Collection</a></li>
<li><a class="reference internal" href="#optimizer-and-optimization">Optimizer and optimization</a></li>
<li><a class="reference internal" href="#model">Model</a></li>
<li><a class="reference internal" href="#id2">Replay buffer</a></li>
</ul>
</li>
<li><a class="reference internal" href="#initialization">Initialization</a><ul>
<li><a class="reference internal" href="#normalization-stats">Normalization stats</a></li>
<li><a class="reference internal" href="#models-policy-and-q-value-network">Models: policy and q-value network</a></li>
<li><a class="reference internal" href="#parallel-environment-creation">Parallel environment creation</a></li>
<li><a class="reference internal" href="#data-collector">Data collector</a></li>
<li><a class="reference internal" href="#id3">Replay buffer</a></li>
<li><a class="reference internal" href="#recorder">Recorder</a></li>
<li><a class="reference internal" href="#optimizer">Optimizer</a></li>
</ul>
</li>
<li><a class="reference internal" href="#time-to-train-the-policy">Time to train the policy</a></li>
<li><a class="reference internal" href="#experiment-results">Experiment results</a></li>
<li><a class="reference internal" href="#sampling-trajectories-and-using-td-lambda">Sampling trajectories and using TD(lambda)</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/sphinx_highlight.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>

        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>

           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>