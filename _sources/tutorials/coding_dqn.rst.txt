
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorials/coding_dqn.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_tutorials_coding_dqn.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorials_coding_dqn.py:


Coding a pixel-based DQN using TorchRL
======================================
**Author**: `Vincent Moens <https://github.com/vmoens>`_

.. GENERATED FROM PYTHON SOURCE LINES 10-67

This tutorial will guide you through the steps to code DQN to solve the
CartPole task from scratch. DQN
(`Deep Q-Learning <https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf>`_) was
the founding work in deep reinforcement learning.
On a high level, the algorithm is quite simple: Q-learning consists in learning a table of
state-action values in such a way that, when encountering any particular state,
we know which action to pick just by searching for the action with the
highest value. This simple setting requires the actions and states to be
discrete, otherwise a lookup table cannot be built.

DQN uses a neural network that encodes a map from the state-action space to
a value (scalar) space, which amortizes the cost of storing and exploring all
the possible state-action combinations: if a state has not been seen in the
past, we can still pass it in conjunction with the various actions available
through our neural network and get an interpolated value for each of the
actions available.

We will solve the classic control problem of the cart pole. From the
Gymnasium doc from where this environment is retrieved:

| A pole is attached by an un-actuated joint to a cart, which moves along a
| frictionless track. The pendulum is placed upright on the cart and the goal
| is to balance the pole by applying forces in the left and right direction
| on the cart.

.. figure:: /_static/img/cartpole_demo.gif
   :alt: Cart Pole

**Prerequisites**: We encourage you to get familiar with torchrl through the
`PPO tutorial <https://pytorch.org/rl/tutorials/coding_ppo.html>`_ first.
This tutorial is more complex and full-fleshed, but it may be .

In this tutorial, you will learn:

- how to build an environment in TorchRL, including transforms (e.g. data
  normalization, frame concatenation, resizing and turning to grayscale)
  and parallel execution. Unlike what we did in the
  `DDPG tutorial <https://pytorch.org/rl/tutorials/coding_ddpg.html>`_, we
  will normalize the pixels and not the state vector.
- how to design a QValue actor, i.e. an actor that estimates the action
  values and picks up the action with the highest estimated return;
- how to collect data from your environment efficiently and store them
  in a replay buffer;
- how to store trajectories (and not transitions) in your replay buffer),
  and how to estimate returns using TD(lambda);
- how to make a module functional and use ;
- and finally how to evaluate your model.

This tutorial assumes the reader is familiar with some of TorchRL
primitives, such as :class:`tensordict.TensorDict` and
:class:`tensordict.TensorDictModules`, although it
should be sufficiently transparent to be understood without a deep
understanding of these classes.

We do not aim at giving a SOTA implementation of the algorithm, but rather
to provide a high-level illustration of TorchRL features in the context
of this algorithm.

.. GENERATED FROM PYTHON SOURCE LINES 67-107

.. code-block:: default



    import torch
    import tqdm
    from functorch import vmap
    from matplotlib import pyplot as plt
    from tensordict import TensorDict
    from tensordict.nn import get_functional
    from torch import nn
    from torchrl.collectors import MultiaSyncDataCollector
    from torchrl.data import LazyMemmapStorage, TensorDictReplayBuffer
    from torchrl.envs import EnvCreator, ParallelEnv, RewardScaling, StepCounter
    from torchrl.envs.libs.gym import GymEnv
    from torchrl.envs.transforms import (
        CatFrames,
        CatTensors,
        Compose,
        GrayScale,
        ObservationNorm,
        Resize,
        ToTensorImage,
        TransformedEnv,
    )
    from torchrl.envs.utils import set_exploration_mode, step_mdp
    from torchrl.modules import DuelingCnnDQNet, EGreedyWrapper, QValueActor


    def is_notebook() -> bool:
        try:
            shell = get_ipython().__class__.__name__
            if shell == "ZMQInteractiveShell":
                return True  # Jupyter notebook or qtconsole
            elif shell == "TerminalInteractiveShell":
                return False  # Terminal running IPython
            else:
                return False  # Other type (?)
        except NameError:
            return False  # Probably standard Python interpreter









.. GENERATED FROM PYTHON SOURCE LINES 114-120

Hyperparameters
---------------

Let's start with our hyperparameters. The following setting should work well
in practice, and the performance of the algorithm should hopefully not be
too sensitive to slight variations of these.

.. GENERATED FROM PYTHON SOURCE LINES 120-123

.. code-block:: default


    device = "cuda:0" if torch.cuda.device_count() > 0 else "cpu"








.. GENERATED FROM PYTHON SOURCE LINES 124-126

Optimizer
^^^^^^^^^

.. GENERATED FROM PYTHON SOURCE LINES 126-134

.. code-block:: default


    # the learning rate of the optimizer
    lr = 2e-3
    # the beta parameters of Adam
    betas = (0.9, 0.999)
    # Optimization steps per batch collected (aka UPD or updates per data)
    n_optim = 8








.. GENERATED FROM PYTHON SOURCE LINES 135-137

DQN parameters
^^^^^^^^^^^^^^

.. GENERATED FROM PYTHON SOURCE LINES 139-140

gamma decay factor

.. GENERATED FROM PYTHON SOURCE LINES 140-142

.. code-block:: default

    gamma = 0.99








.. GENERATED FROM PYTHON SOURCE LINES 143-144

lambda decay factor (see second the part with TD(:math:`\lambda`)

.. GENERATED FROM PYTHON SOURCE LINES 144-146

.. code-block:: default

    lmbda = 0.95








.. GENERATED FROM PYTHON SOURCE LINES 147-150

Smooth target network update decay parameter.
This loosely corresponds to a 1/(1-tau) interval with hard target network
update

.. GENERATED FROM PYTHON SOURCE LINES 150-152

.. code-block:: default

    tau = 0.005








.. GENERATED FROM PYTHON SOURCE LINES 153-164

Data collection and replay buffer
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Values to be used for proper training have been commented.

Total frames collected in the environment. In other implementations, the
user defines a maximum number of episodes.
This is harder to do with our data collectors since they return batches
of N collected frames, where N is a constant.
However, one can easily get the same restriction on number of episodes by
breaking the training loop when a certain number
episodes has been collected.

.. GENERATED FROM PYTHON SOURCE LINES 164-166

.. code-block:: default

    total_frames = 5000  # 500000








.. GENERATED FROM PYTHON SOURCE LINES 167-168

Random frames used to initialize the replay buffer.

.. GENERATED FROM PYTHON SOURCE LINES 168-170

.. code-block:: default

    init_random_frames = 100  # 1000








.. GENERATED FROM PYTHON SOURCE LINES 171-172

Frames in each batch collected.

.. GENERATED FROM PYTHON SOURCE LINES 172-174

.. code-block:: default

    frames_per_batch = 32  # 128








.. GENERATED FROM PYTHON SOURCE LINES 175-176

Frames sampled from the replay buffer at each optimization step

.. GENERATED FROM PYTHON SOURCE LINES 176-178

.. code-block:: default

    batch_size = 32  # 256








.. GENERATED FROM PYTHON SOURCE LINES 179-180

Size of the replay buffer in terms of frames

.. GENERATED FROM PYTHON SOURCE LINES 180-182

.. code-block:: default

    buffer_size = min(total_frames, 100000)








.. GENERATED FROM PYTHON SOURCE LINES 183-184

Number of environments run in parallel in each data collector

.. GENERATED FROM PYTHON SOURCE LINES 184-188

.. code-block:: default

    num_workers = 2  # 8
    num_collectors = 2  # 4









.. GENERATED FROM PYTHON SOURCE LINES 189-196

Environment and exploration
^^^^^^^^^^^^^^^^^^^^^^^^^^^

We set the initial and final value of the epsilon factor in Epsilon-greedy
exploration.
Since our policy is deterministic, exploration is crucial: without it, the
only source of randomness would be the environment reset.

.. GENERATED FROM PYTHON SOURCE LINES 196-200

.. code-block:: default


    eps_greedy_val = 0.1
    eps_greedy_val_env = 0.005








.. GENERATED FROM PYTHON SOURCE LINES 201-203

To speed up learning, we set the bias of the last layer of our value network
to a predefined value (this is not mandatory)

.. GENERATED FROM PYTHON SOURCE LINES 203-205

.. code-block:: default

    init_bias = 2.0








.. GENERATED FROM PYTHON SOURCE LINES 206-241

**Note**: for fast rendering of the tutorial ``total_frames`` hyperparameter
was set to a very low number. To get a reasonable performance, use a greater
value e.g. 500000

Building the environment
------------------------

Our environment builder has two arguments:

- ``parallel``: determines whether multiple environments have to be run in
  parallel. We stack the transforms after the
  :class:`torchrl.envs.ParallelEnv` to take advantage
  of vectorization of the operations on device, although this would
  technically work with every single environment attached to its own set of
  transforms.
- ``observation_norm_state_dict`` will contain the normalizing constants for
  the :class:`torchrl.envs.ObservationNorm` tranform.

We will be using five transforms:

- :class:`torchrl.envs.ToTensorImage` will convert a ``[W, H, C]`` uint8
  tensor in a floating point tensor in the ``[0, 1]`` space with shape
  ``[C, W, H]``;
- :class:`torchrl.envs.RewardScaling` to reduce the scale of the return;
- :class:`torchrl.envs.GrayScale` will turn our image into grayscale;
- :class:`torchrl.envs.Resize` will resize the image in a 64x64 format;
- :class:`torchrl.envs.CatFrames` will concatenate an arbitrary number of
  successive frames (``N=4``) in a single tensor along the channel dimension.
  This is useful as a single image does not carry information about the
  motion of the cartpole. Some memory about past observations and actions
  is needed, either via a recurrent neural network or using a stack of
  frames.
- :class:`torchrl.envs.ObservationNorm` which will normalize our observations
  given some custom summary statistics.


.. GENERATED FROM PYTHON SOURCE LINES 241-275

.. code-block:: default



    def make_env(parallel=False, observation_norm_state_dict=None):
        if observation_norm_state_dict is None:
            observation_norm_state_dict = {"standard_normal": True}
        if parallel:
            base_env = ParallelEnv(
                num_workers,
                EnvCreator(
                    lambda: GymEnv(
                        "CartPole-v1", from_pixels=True, pixels_only=True, device=device
                    )
                ),
            )
        else:
            base_env = GymEnv(
                "CartPole-v1", from_pixels=True, pixels_only=True, device=device
            )

        env = TransformedEnv(
            base_env,
            Compose(
                StepCounter(),  # to count the steps of each trajectory
                ToTensorImage(),
                RewardScaling(loc=0.0, scale=0.1),
                GrayScale(),
                Resize(64, 64),
                CatFrames(4, in_keys=["pixels"], dim=-3),
                ObservationNorm(in_keys=["pixels"], **observation_norm_state_dict),
            ),
        )
        return env









.. GENERATED FROM PYTHON SOURCE LINES 276-285

Compute normalizing constants
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To normalize images, we don't want to normalize each pixel independently
with a full ``[C, W, H]`` normalizing mask, but with simpler ``[C, 1, 1]``
shaped loc and scale parameters. We will be using the ``reduce_dim`` argument
of :func:`torchrl.envs.ObservationNorm.init_stats` to instruct which
dimensions must be reduced, and the ``keep_dims`` parameter to ensure that
not all dimensions disappear in the process:

.. GENERATED FROM PYTHON SOURCE LINES 285-292

.. code-block:: default


    test_env = make_env()
    test_env.transform[-1].init_stats(
        num_iter=1000, cat_dim=0, reduce_dim=[-1, -2, -4], keep_dims=(-1, -2)
    )
    observation_norm_state_dict = test_env.transform[-1].state_dict()








.. GENERATED FROM PYTHON SOURCE LINES 293-295

let's check that normalizing constants have a size of ``[C, 1, 1]`` where
``C=4`` (because of :class:`torchrl.envs.CatFrames`).

.. GENERATED FROM PYTHON SOURCE LINES 295-297

.. code-block:: default

    print(observation_norm_state_dict)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    OrderedDict([('standard_normal', tensor(True)), ('loc', tensor([[[0.9920]],

            [[0.9920]],

            [[0.9920]],

            [[0.9920]]])), ('scale', tensor([[[0.0790]],

            [[0.0790]],

            [[0.0790]],

            [[0.0790]]]))])




.. GENERATED FROM PYTHON SOURCE LINES 298-339

Building the model (Deep Q-network)
-----------------------------------

The following function builds a :class:`torchrl.modules.DuelingCnnDQNet`
object which is a simple CNN followed by a two-layer MLP. The only trick used
here is that the action values (i.e. left and right action value) are
computed using

.. math::

   val = b(obs) + v(obs) - \mathbb{E}[v(obs)]

where :math:`b` is a :math:`\# obs \rightarrow 1` function and :math:`v` is a
:math:`\# obs \rightarrow num_actions` function.

Our network is wrapped in a :class:`torchrl.modules.QValueActor`, which will read the state-action
values, pick up the one with the maximum value and write all those results
in the input :class:`tensordict.TensorDict`.

Target parameters
^^^^^^^^^^^^^^^^^

Many off-policy RL algorithms use the concept of "target parameters" when it
comes to estimate the value of the ``t+1`` state or state-action pair.
The target parameters are lagged copies of the model parameters. Because
their predictions mismatch those of the current model configuration, they
help learning by putting a pessimistic bound on the value being estimated.
This is a powerful trick (known as "Double Q-Learning") that is ubiquitous
in similar algorithms.

Functionalizing modules
^^^^^^^^^^^^^^^^^^^^^^^

One of the features of torchrl is its usage of functional modules: as the
same architecture is often used with multiple sets of parameters (e.g.
trainable and target parameters), we functionalize the modules and isolate
the various sets of parameters in separate tensordicts.

To this aim, we use :func:`tensordict.nn.get_functional`, which augments
our modules with some extra feature that make them compatible with parameters
passed in the ``TensorDict`` format.

.. GENERATED FROM PYTHON SOURCE LINES 339-403

.. code-block:: default



    def make_model(dummy_env):
        cnn_kwargs = {
            "num_cells": [32, 64, 64],
            "kernel_sizes": [6, 4, 3],
            "strides": [2, 2, 1],
            "activation_class": nn.ELU,
            # This can be used to reduce the size of the last layer of the CNN
            # "squeeze_output": True,
            # "aggregator_class": nn.AdaptiveAvgPool2d,
            # "aggregator_kwargs": {"output_size": (1, 1)},
        }
        mlp_kwargs = {
            "depth": 2,
            "num_cells": [
                64,
                64,
            ],
            "activation_class": nn.ELU,
        }
        net = DuelingCnnDQNet(
            dummy_env.action_spec.shape[-1], 1, cnn_kwargs, mlp_kwargs
        ).to(device)
        net.value[-1].bias.data.fill_(init_bias)

        actor = QValueActor(net, in_keys=["pixels"], spec=dummy_env.action_spec).to(device)
        # init actor: because the model is composed of lazy conv/linear layers,
        # we must pass a fake batch of data through it to instantiate them.
        tensordict = dummy_env.fake_tensordict()
        actor(tensordict)

        # Make functional:
        # here's an explicit way of creating the parameters and buffer tensordict.
        # Alternatively, we could have used `params = make_functional(actor)` from
        # tensordict.nn
        params = TensorDict({k: v for k, v in actor.named_parameters()}, [])
        buffers = TensorDict({k: v for k, v in actor.named_buffers()}, [])
        params = params.update(buffers)
        params = params.unflatten_keys(".")  # creates a nested TensorDict
        factor = get_functional(actor)

        # creating the target parameters is fairly easy with tensordict:
        params_target = params.clone().detach()

        # we wrap our actor in an EGreedyWrapper for data collection
        actor_explore = EGreedyWrapper(
            actor,
            annealing_num_steps=total_frames,
            eps_init=eps_greedy_val,
            eps_end=eps_greedy_val_env,
        )

        return factor, actor, actor_explore, params, params_target


    (
        factor,
        actor,
        actor_explore,
        params,
        params_target,
    ) = make_model(test_env)








.. GENERATED FROM PYTHON SOURCE LINES 404-406

We represent the parameters and targets as flat structures, but unflattening
them is quite easy:

.. GENERATED FROM PYTHON SOURCE LINES 406-409

.. code-block:: default


    params_flat = params.flatten_keys(".")








.. GENERATED FROM PYTHON SOURCE LINES 410-411

We will be using the adam optimizer:

.. GENERATED FROM PYTHON SOURCE LINES 411-414

.. code-block:: default


    optim = torch.optim.Adam(list(params_flat.values()), lr, betas=betas)








.. GENERATED FROM PYTHON SOURCE LINES 415-416

We create a test environment for evaluation of the policy:

.. GENERATED FROM PYTHON SOURCE LINES 416-423

.. code-block:: default


    test_env = make_env(
        parallel=False, observation_norm_state_dict=observation_norm_state_dict
    )
    # sanity check:
    print(actor_explore(test_env.reset()))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    TensorDict(
        fields={
            action: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, is_shared=False),
            action_value: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.float32, is_shared=False),
            chosen_action_value: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),
            done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),
            pixels: Tensor(shape=torch.Size([4, 64, 64]), device=cpu, dtype=torch.float32, is_shared=False),
            reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),
            step_count: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},
        batch_size=torch.Size([]),
        device=cpu,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 424-443

Collecting and storing data
---------------------------

Replay buffers
^^^^^^^^^^^^^^

Replay buffers play a central role in off-policy RL algorithms such as DQN.
They constitute the dataset we will be sampling from during training.

Here, we will use a regular sampling strategy, although a prioritized RB
could improve the performance significantly.

We place the storage on disk using
:class:`torchrl.data.replay_buffers.storages.LazyMemmapStorage` class. This
storage is created in a lazy manner: it will only be instantiated once the
first batch of data is passed to it.

The only requirement of this storage is that the data passed to it at write
time must always have the same shape.

.. GENERATED FROM PYTHON SOURCE LINES 443-449

.. code-block:: default


    replay_buffer = TensorDictReplayBuffer(
        storage=LazyMemmapStorage(buffer_size),
        prefetch=n_optim,
    )








.. GENERATED FROM PYTHON SOURCE LINES 450-478

Data collector
^^^^^^^^^^^^^^

As in `PPO <https://pytorch.org/rl/tutorials/coding_ppo.html>` and
`DDPG <https://pytorch.org/rl/tutorials/coding_ddpg.html>`, we will be using
a data collector as a dataloader in the outer loop.

We choose the following configuration: we will be running a series of
parallel environments synchronously in parallel in different collectors,
themselves running in parallel but asynchronously.
The advantage of this configuration is that we can balance the amount of
compute that is executed in batch with what we want to be executed
asynchronously. We encourage the reader to experiment how the collection
speed is impacted by modifying the number of collectors (ie the number of
environment constructors passed to the collector) and the number of
environment executed in parallel in each collector (controlled by the
``num_workers`` hyperparameter).

When building the collector, we can choose on which device we want the
environment and policy to execute the operations through the ``device``
keyword argument. The ``storing_devices`` argument will modify the
location of the data being collected: if the batches that we are gathering
have a considerable size, we may want to store them on a different location
than the device where the computation is happening. For asynchronous data
collectors such as ours, different storing devices mean that the data that
we collect won't sit on the same device each time, which is something that
out training loop must account for. For simplicity, we set the devices to
the same value for all sub-collectors.

.. GENERATED FROM PYTHON SOURCE LINES 478-501

.. code-block:: default


    data_collector = MultiaSyncDataCollector(
        # ``num_collectors`` collectors, each with an set of `num_workers` environments being run in parallel
        [
            make_env(
                parallel=True, observation_norm_state_dict=observation_norm_state_dict
            ),
        ]
        * num_collectors,
        policy=actor_explore,
        frames_per_batch=frames_per_batch,
        total_frames=total_frames,
        # this is the default behaviour: the collector runs in ``"random"`` (or explorative) mode
        exploration_mode="random",
        # We set the all the devices to be identical. Below is an example of
        # heterogeneous devices
        devices=[device] * num_collectors,
        storing_devices=[device] * num_collectors,
        # devices=[f"cuda:{i}" for i in range(1, 1 + num_collectors)],
        # storing_devices=[f"cuda:{i}" for i in range(1, 1 + num_collectors)],
        split_trajs=False,
    )








.. GENERATED FROM PYTHON SOURCE LINES 502-517

Training loop of a regular DQN
------------------------------

We'll start with a simple implementation of DQN where the returns are
computed without bootstrapping, i.e.

.. math::

      Q_{t}(s, a) = R(s, a) + \gamma * V_{t+1}(s)

where :math:`Q(s, a)` is the Q-value of the current state-action pair,
:math:`R(s, a)` is the result of the reward function, and :math:`V(s)` is a
value function that returns 0 for terminating states.

We store the logs in a defaultdict:

.. GENERATED FROM PYTHON SOURCE LINES 517-621

.. code-block:: default


    logs_exp1 = defaultdict(list)
    prev_traj_count = 0

    pbar = tqdm.tqdm(total=total_frames)
    for j, data in enumerate(data_collector):
        current_frames = data.numel()
        pbar.update(current_frames)
        data = data.view(-1)

        # We store the values on the replay buffer, after placing them on CPU.
        # When called for the first time, this will instantiate our storage
        # object which will print its content.
        replay_buffer.extend(data.cpu())

        # some logging
        if len(logs_exp1["frames"]):
            logs_exp1["frames"].append(current_frames + logs_exp1["frames"][-1])
        else:
            logs_exp1["frames"].append(current_frames)

        if data["next", "done"].any():
            done = data["next", "done"].squeeze(-1)
            logs_exp1["traj_lengths"].append(
                data["next", "step_count"][done].float().mean().item()
            )

        # check that we have enough data to start training
        if sum(logs_exp1["frames"]) > init_random_frames:
            for _ in range(n_optim):
                # sample from the RB and send to device
                sampled_data = replay_buffer.sample(batch_size)
                sampled_data = sampled_data.to(device, non_blocking=True)

                # collect data from RB
                reward = sampled_data["next", "reward"].squeeze(-1)
                done = sampled_data["next", "done"].squeeze(-1).to(reward.dtype)
                action = sampled_data["action"].clone()

                # Compute action value (of the action actually taken) at time t
                # By default, TorchRL uses one-hot encodings for discrete actions
                sampled_data_out = sampled_data.select(*actor.in_keys)
                sampled_data_out = factor(sampled_data_out, params=params)
                action_value = sampled_data_out["action_value"]
                action_value = (action_value * action.to(action_value.dtype)).sum(-1)
                with torch.no_grad():
                    # compute best action value for the next step, using target parameters
                    tdstep = step_mdp(sampled_data)
                    next_value = factor(
                        tdstep.select(*actor.in_keys),
                        params=params_target,
                    )["chosen_action_value"].squeeze(-1)
                    exp_value = reward + gamma * next_value * (1 - done)
                assert exp_value.shape == action_value.shape
                # we use MSE loss but L1 or smooth L1 should also work
                error = nn.functional.mse_loss(exp_value, action_value).mean()
                error.backward()

                gv = nn.utils.clip_grad_norm_(list(params_flat.values()), 1)

                optim.step()
                optim.zero_grad()

                # update of the target parameters
                params_target.apply(
                    lambda p_target, p_orig: p_orig * tau + p_target * (1 - tau),
                    params.detach(),
                    inplace=True,
                )

            actor_explore.step(current_frames)

            # Logging
            logs_exp1["grad_vals"].append(float(gv))
            logs_exp1["losses"].append(error.item())
            logs_exp1["values"].append(action_value.mean().item())
            logs_exp1["traj_count"].append(
                prev_traj_count + data["next", "done"].sum().item()
            )
            prev_traj_count = logs_exp1["traj_count"][-1]

            if j % 10 == 0:
                with set_exploration_mode("mode"), torch.no_grad():
                    # execute a rollout. The `set_exploration_mode("mode")` has no effect here since the policy is deterministic, but we add it for completeness
                    eval_rollout = test_env.rollout(
                        max_steps=10000,
                        policy=actor,
                    ).cpu()
                logs_exp1["traj_lengths_eval"].append(eval_rollout.shape[-1])
                logs_exp1["evals"].append(eval_rollout["next", "reward"].sum().item())
                if len(logs_exp1["mavgs"]):
                    logs_exp1["mavgs"].append(
                        logs_exp1["evals"][-1] * 0.05 + logs_exp1["mavgs"][-1] * 0.95
                    )
                else:
                    logs_exp1["mavgs"].append(logs_exp1["evals"][-1])
                logs_exp1["traj_count_eval"].append(logs_exp1["traj_count"][-1])
                pbar.set_description(
                    f"error: {error: 4.4f}, value: {action_value.mean(): 4.4f}, test return: {logs_exp1['evals'][-1]: 4.4f}"
                )

        # update policy weights
        data_collector.update_policy_weights_()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/5000 [00:00<?, ?it/s]      1%|          | 32/5000 [00:00<02:19, 35.53it/s]Creating a MemmapStorage...
    The storage is being created: 
            action: /tmp/tmpapvv5xzs, 0.0762939453125 Mb of storage (size: torch.Size([5000, 2])).
            action_value: /tmp/tmpolpoc6xa, 0.03814697265625 Mb of storage (size: torch.Size([5000, 2])).
            chosen_action_value: /tmp/tmpynmkc2ix, 0.019073486328125 Mb of storage (size: torch.Size([5000, 1])).
            done: /tmp/tmp9ovw4nl6, 0.00476837158203125 Mb of storage (size: torch.Size([5000, 1])).
            index: /tmp/tmp8vt1xhci, 0.019073486328125 Mb of storage (size: torch.Size([5000])).
            pixels: /tmp/tmpr1b6vfoy, 312.5 Mb of storage (size: torch.Size([5000, 4, 64, 64])).
            reward: /tmp/tmpa_qdgos4, 0.019073486328125 Mb of storage (size: torch.Size([5000, 1])).
            step_count: /tmp/tmpl7dmha2m, 0.03814697265625 Mb of storage (size: torch.Size([5000])).
            ('collector', 'traj_ids'): /tmp/tmp3y0hoeff, 0.03814697265625 Mb of storage (size: torch.Size([5000])).
            ('next', 'done'): /tmp/tmpv1ksa6qh, 0.00476837158203125 Mb of storage (size: torch.Size([5000, 1])).
            ('next', 'pixels'): /tmp/tmpu7ev4mnc, 312.5 Mb of storage (size: torch.Size([5000, 4, 64, 64])).
            ('next', 'reward'): /tmp/tmp8zk5zffa, 0.019073486328125 Mb of storage (size: torch.Size([5000, 1])).
            ('next', 'step_count'): /tmp/tmpy11rq9si, 0.03814697265625 Mb of storage (size: torch.Size([5000])).
      1%|1         | 64/5000 [00:01<01:09, 71.22it/s]      2%|1         | 96/5000 [00:01<01:32, 52.86it/s]      3%|2         | 128/5000 [00:02<01:45, 46.01it/s]      3%|3         | 160/5000 [00:03<01:45, 45.90it/s]      4%|3         | 192/5000 [00:04<01:47, 44.89it/s]      4%|4         | 224/5000 [00:04<01:46, 44.65it/s]      5%|5         | 256/5000 [00:05<01:45, 44.81it/s]      6%|5         | 288/5000 [00:06<01:43, 45.72it/s]      6%|6         | 320/5000 [00:06<01:39, 46.87it/s]      7%|7         | 352/5000 [00:07<01:36, 48.00it/s]    error:  0.0308, value:  1.7754, test return:  0.9000:   7%|7         | 352/5000 [00:08<01:36, 48.00it/s]    error:  0.0308, value:  1.7754, test return:  0.9000:   8%|7         | 384/5000 [00:08<01:38, 46.93it/s]    error:  0.0308, value:  1.7754, test return:  0.9000:   8%|8         | 416/5000 [00:08<01:37, 47.23it/s]    error:  0.0308, value:  1.7754, test return:  0.9000:   9%|8         | 448/5000 [00:09<01:34, 48.09it/s]    error:  0.0308, value:  1.7754, test return:  0.9000:  10%|9         | 480/5000 [00:10<01:32, 48.69it/s]    error:  0.0308, value:  1.7754, test return:  0.9000:  10%|#         | 512/5000 [00:10<01:30, 49.39it/s]    error:  0.0308, value:  1.7754, test return:  0.9000:  11%|#         | 544/5000 [00:11<01:28, 50.22it/s]    error:  0.0308, value:  1.7754, test return:  0.9000:  12%|#1        | 576/5000 [00:12<01:27, 50.32it/s]    error:  0.0308, value:  1.7754, test return:  0.9000:  12%|#2        | 608/5000 [00:12<01:28, 49.60it/s]    error:  0.0308, value:  1.7754, test return:  0.9000:  13%|#2        | 640/5000 [00:13<01:27, 49.60it/s]    error:  0.0308, value:  1.7754, test return:  0.9000:  13%|#3        | 672/5000 [00:14<01:29, 48.35it/s]    error:  0.1582, value:  1.7516, test return:  1.0000:  13%|#3        | 672/5000 [00:14<01:29, 48.35it/s]    error:  0.1582, value:  1.7516, test return:  1.0000:  14%|#4        | 704/5000 [00:14<01:34, 45.43it/s]    error:  0.1582, value:  1.7516, test return:  1.0000:  15%|#4        | 736/5000 [00:15<01:32, 46.26it/s]    error:  0.1582, value:  1.7516, test return:  1.0000:  15%|#5        | 768/5000 [00:16<01:29, 47.10it/s]    error:  0.1582, value:  1.7516, test return:  1.0000:  16%|#6        | 800/5000 [00:16<01:27, 47.95it/s]    error:  0.1582, value:  1.7516, test return:  1.0000:  17%|#6        | 832/5000 [00:17<01:26, 47.96it/s]    error:  0.1582, value:  1.7516, test return:  1.0000:  17%|#7        | 864/5000 [00:18<01:24, 48.81it/s]    error:  0.1582, value:  1.7516, test return:  1.0000:  18%|#7        | 896/5000 [00:18<01:22, 49.82it/s]    error:  0.1582, value:  1.7516, test return:  1.0000:  19%|#8        | 928/5000 [00:19<01:21, 50.26it/s]    error:  0.1582, value:  1.7516, test return:  1.0000:  19%|#9        | 960/5000 [00:19<01:20, 50.10it/s]    error:  0.1582, value:  1.7516, test return:  1.0000:  20%|#9        | 992/5000 [00:20<01:19, 50.30it/s]    error:  0.0570, value:  1.6106, test return:  3.6000:  20%|#9        | 992/5000 [00:21<01:19, 50.30it/s]    error:  0.0570, value:  1.6106, test return:  3.6000:  20%|##        | 1024/5000 [00:21<01:30, 43.79it/s]    error:  0.0570, value:  1.6106, test return:  3.6000:  21%|##1       | 1056/5000 [00:22<01:26, 45.37it/s]    error:  0.0570, value:  1.6106, test return:  3.6000:  22%|##1       | 1088/5000 [00:22<01:29, 43.69it/s]    error:  0.0570, value:  1.6106, test return:  3.6000:  22%|##2       | 1120/5000 [00:23<01:29, 43.26it/s]    error:  0.0570, value:  1.6106, test return:  3.6000:  23%|##3       | 1152/5000 [00:24<01:29, 43.13it/s]    error:  0.0570, value:  1.6106, test return:  3.6000:  24%|##3       | 1184/5000 [00:25<01:26, 44.28it/s]    error:  0.0570, value:  1.6106, test return:  3.6000:  24%|##4       | 1216/5000 [00:25<01:22, 45.75it/s]    error:  0.0570, value:  1.6106, test return:  3.6000:  25%|##4       | 1248/5000 [00:26<01:19, 47.11it/s]    error:  0.0570, value:  1.6106, test return:  3.6000:  26%|##5       | 1280/5000 [00:27<01:18, 47.55it/s]    error:  0.0570, value:  1.6106, test return:  3.6000:  26%|##6       | 1312/5000 [00:27<01:16, 48.25it/s]    error:  0.0626, value:  1.8990, test return:  0.9000:  26%|##6       | 1312/5000 [00:28<01:16, 48.25it/s]    error:  0.0626, value:  1.8990, test return:  0.9000:  27%|##6       | 1344/5000 [00:28<01:17, 47.47it/s]    error:  0.0626, value:  1.8990, test return:  0.9000:  28%|##7       | 1376/5000 [00:29<01:14, 48.38it/s]    error:  0.0626, value:  1.8990, test return:  0.9000:  28%|##8       | 1408/5000 [00:29<01:14, 48.11it/s]    error:  0.0626, value:  1.8990, test return:  0.9000:  29%|##8       | 1440/5000 [00:30<01:11, 49.58it/s]    error:  0.0626, value:  1.8990, test return:  0.9000:  29%|##9       | 1472/5000 [00:30<01:10, 50.35it/s]    error:  0.0626, value:  1.8990, test return:  0.9000:  30%|###       | 1504/5000 [00:31<01:09, 50.18it/s]    error:  0.0626, value:  1.8990, test return:  0.9000:  31%|###       | 1536/5000 [00:32<01:08, 50.39it/s]    error:  0.0626, value:  1.8990, test return:  0.9000:  31%|###1      | 1568/5000 [00:32<01:10, 48.93it/s]    error:  0.0626, value:  1.8990, test return:  0.9000:  32%|###2      | 1600/5000 [00:33<01:09, 48.62it/s]    error:  0.0626, value:  1.8990, test return:  0.9000:  33%|###2      | 1632/5000 [00:34<01:10, 47.83it/s]    error:  0.0408, value:  1.6403, test return:  0.9000:  33%|###2      | 1632/5000 [00:35<01:10, 47.83it/s]    error:  0.0408, value:  1.6403, test return:  0.9000:  33%|###3      | 1664/5000 [00:35<01:14, 45.03it/s]    error:  0.0408, value:  1.6403, test return:  0.9000:  34%|###3      | 1696/5000 [00:35<01:10, 46.85it/s]    error:  0.0408, value:  1.6403, test return:  0.9000:  35%|###4      | 1728/5000 [00:36<01:08, 47.75it/s]    error:  0.0408, value:  1.6403, test return:  0.9000:  35%|###5      | 1760/5000 [00:37<01:08, 47.48it/s]    error:  0.0408, value:  1.6403, test return:  0.9000:  36%|###5      | 1792/5000 [00:37<01:05, 48.64it/s]    error:  0.0408, value:  1.6403, test return:  0.9000:  36%|###6      | 1824/5000 [00:38<01:03, 49.74it/s]    error:  0.0408, value:  1.6403, test return:  0.9000:  37%|###7      | 1856/5000 [00:38<01:03, 49.83it/s]    error:  0.0408, value:  1.6403, test return:  0.9000:  38%|###7      | 1888/5000 [00:39<01:02, 50.07it/s]    error:  0.0408, value:  1.6403, test return:  0.9000:  38%|###8      | 1920/5000 [00:40<01:02, 49.65it/s]    error:  0.0408, value:  1.6403, test return:  0.9000:  39%|###9      | 1952/5000 [00:40<01:00, 50.22it/s]    error:  0.1718, value:  1.5935, test return:  3.2000:  39%|###9      | 1952/5000 [00:41<01:00, 50.22it/s]    error:  0.1718, value:  1.5935, test return:  3.2000:  40%|###9      | 1984/5000 [00:41<01:07, 44.39it/s]    error:  0.1718, value:  1.5935, test return:  3.2000:  40%|####      | 2016/5000 [00:42<01:04, 46.41it/s]    error:  0.1718, value:  1.5935, test return:  3.2000:  41%|####      | 2048/5000 [00:43<01:05, 45.18it/s]    error:  0.1718, value:  1.5935, test return:  3.2000:  42%|####1     | 2080/5000 [00:43<01:03, 45.67it/s]    error:  0.1718, value:  1.5935, test return:  3.2000:  42%|####2     | 2112/5000 [00:44<01:04, 44.72it/s]    error:  0.1718, value:  1.5935, test return:  3.2000:  43%|####2     | 2144/5000 [00:45<01:03, 45.11it/s]    error:  0.1718, value:  1.5935, test return:  3.2000:  44%|####3     | 2176/5000 [00:45<01:00, 46.77it/s]    error:  0.1718, value:  1.5935, test return:  3.2000:  44%|####4     | 2208/5000 [00:46<00:58, 47.97it/s]    error:  0.1718, value:  1.5935, test return:  3.2000:  45%|####4     | 2240/5000 [00:47<00:56, 48.85it/s]    error:  0.1718, value:  1.5935, test return:  3.2000:  45%|####5     | 2272/5000 [00:47<00:55, 49.33it/s]    error:  0.0325, value:  1.8029, test return:  1.2000:  45%|####5     | 2272/5000 [00:48<00:55, 49.33it/s]    error:  0.0325, value:  1.8029, test return:  1.2000:  46%|####6     | 2304/5000 [00:48<00:57, 47.01it/s]    error:  0.0325, value:  1.8029, test return:  1.2000:  47%|####6     | 2336/5000 [00:49<00:55, 47.90it/s]    error:  0.0325, value:  1.8029, test return:  1.2000:  47%|####7     | 2368/5000 [00:49<00:54, 48.36it/s]    error:  0.0325, value:  1.8029, test return:  1.2000:  48%|####8     | 2400/5000 [00:50<00:53, 48.93it/s]    error:  0.0325, value:  1.8029, test return:  1.2000:  49%|####8     | 2432/5000 [00:51<00:51, 50.20it/s]    error:  0.0325, value:  1.8029, test return:  1.2000:  49%|####9     | 2464/5000 [00:51<00:50, 50.29it/s]    error:  0.0325, value:  1.8029, test return:  1.2000:  50%|####9     | 2496/5000 [00:52<00:49, 50.32it/s]    error:  0.0325, value:  1.8029, test return:  1.2000:  51%|#####     | 2528/5000 [00:52<00:49, 49.49it/s]    error:  0.0325, value:  1.8029, test return:  1.2000:  51%|#####1    | 2560/5000 [00:53<00:51, 47.61it/s]    error:  0.0325, value:  1.8029, test return:  1.2000:  52%|#####1    | 2592/5000 [00:54<00:53, 44.95it/s]    error:  0.0390, value:  1.8723, test return:  4.4000:  52%|#####1    | 2592/5000 [00:55<00:53, 44.95it/s]    error:  0.0390, value:  1.8723, test return:  4.4000:  52%|#####2    | 2624/5000 [00:55<01:00, 39.26it/s]    error:  0.0390, value:  1.8723, test return:  4.4000:  53%|#####3    | 2656/5000 [00:56<00:55, 42.46it/s]    error:  0.0390, value:  1.8723, test return:  4.4000:  54%|#####3    | 2688/5000 [00:56<00:52, 44.28it/s]    error:  0.0390, value:  1.8723, test return:  4.4000:  54%|#####4    | 2720/5000 [00:57<00:49, 46.28it/s]    error:  0.0390, value:  1.8723, test return:  4.4000:  55%|#####5    | 2752/5000 [00:58<00:46, 48.04it/s]    error:  0.0390, value:  1.8723, test return:  4.4000:  56%|#####5    | 2784/5000 [00:58<00:46, 47.99it/s]    error:  0.0390, value:  1.8723, test return:  4.4000:  56%|#####6    | 2816/5000 [00:59<00:44, 48.94it/s]    error:  0.0390, value:  1.8723, test return:  4.4000:  57%|#####6    | 2848/5000 [00:59<00:43, 49.95it/s]    error:  0.0390, value:  1.8723, test return:  4.4000:  58%|#####7    | 2880/5000 [01:00<00:41, 50.62it/s]    error:  0.0390, value:  1.8723, test return:  4.4000:  58%|#####8    | 2912/5000 [01:01<00:41, 50.40it/s]    error:  0.0458, value:  1.6854, test return:  1.3000:  58%|#####8    | 2912/5000 [01:01<00:41, 50.40it/s]    error:  0.0458, value:  1.6854, test return:  1.3000:  59%|#####8    | 2944/5000 [01:01<00:42, 48.04it/s]    error:  0.0458, value:  1.6854, test return:  1.3000:  60%|#####9    | 2976/5000 [01:02<00:41, 48.36it/s]    error:  0.0458, value:  1.6854, test return:  1.3000:  60%|######    | 3008/5000 [01:03<00:41, 48.39it/s]    error:  0.0458, value:  1.6854, test return:  1.3000:  61%|######    | 3040/5000 [01:03<00:40, 48.24it/s]    error:  0.0458, value:  1.6854, test return:  1.3000:  61%|######1   | 3072/5000 [01:04<00:41, 46.01it/s]    error:  0.0458, value:  1.6854, test return:  1.3000:  62%|######2   | 3104/5000 [01:05<00:41, 46.22it/s]    error:  0.0458, value:  1.6854, test return:  1.3000:  63%|######2   | 3136/5000 [01:05<00:39, 47.55it/s]    error:  0.0458, value:  1.6854, test return:  1.3000:  63%|######3   | 3168/5000 [01:06<00:37, 48.41it/s]    error:  0.0458, value:  1.6854, test return:  1.3000:  64%|######4   | 3200/5000 [01:07<00:36, 49.10it/s]    error:  0.0458, value:  1.6854, test return:  1.3000:  65%|######4   | 3232/5000 [01:07<00:35, 49.30it/s]    error:  0.0882, value:  1.6021, test return:  1.7000:  65%|######4   | 3232/5000 [01:08<00:35, 49.30it/s]    error:  0.0882, value:  1.6021, test return:  1.7000:  65%|######5   | 3264/5000 [01:08<00:38, 44.70it/s]    error:  0.0882, value:  1.6021, test return:  1.7000:  66%|######5   | 3296/5000 [01:09<00:36, 46.55it/s]    error:  0.0882, value:  1.6021, test return:  1.7000:  67%|######6   | 3328/5000 [01:10<00:35, 47.55it/s]    error:  0.0882, value:  1.6021, test return:  1.7000:  67%|######7   | 3360/5000 [01:10<00:33, 48.49it/s]    error:  0.0882, value:  1.6021, test return:  1.7000:  68%|######7   | 3392/5000 [01:11<00:32, 49.17it/s]    error:  0.0882, value:  1.6021, test return:  1.7000:  68%|######8   | 3424/5000 [01:11<00:31, 49.92it/s]    error:  0.0882, value:  1.6021, test return:  1.7000:  69%|######9   | 3456/5000 [01:12<00:31, 49.80it/s]    error:  0.0882, value:  1.6021, test return:  1.7000:  70%|######9   | 3488/5000 [01:13<00:30, 48.82it/s]    error:  0.0882, value:  1.6021, test return:  1.7000:  70%|#######   | 3520/5000 [01:13<00:30, 48.47it/s]    error:  0.0882, value:  1.6021, test return:  1.7000:  71%|#######1  | 3552/5000 [01:14<00:30, 47.51it/s]    error:  0.1415, value:  1.7113, test return:  1.7000:  71%|#######1  | 3552/5000 [01:15<00:30, 47.51it/s]    error:  0.1415, value:  1.7113, test return:  1.7000:  72%|#######1  | 3584/5000 [01:15<00:31, 44.47it/s]    error:  0.1415, value:  1.7113, test return:  1.7000:  72%|#######2  | 3616/5000 [01:16<00:30, 46.07it/s]    error:  0.1415, value:  1.7113, test return:  1.7000:  73%|#######2  | 3648/5000 [01:16<00:28, 46.90it/s]    error:  0.1415, value:  1.7113, test return:  1.7000:  74%|#######3  | 3680/5000 [01:17<00:27, 48.32it/s]    error:  0.1415, value:  1.7113, test return:  1.7000:  74%|#######4  | 3712/5000 [01:17<00:26, 49.10it/s]    error:  0.1415, value:  1.7113, test return:  1.7000:  75%|#######4  | 3744/5000 [01:18<00:25, 49.67it/s]    error:  0.1415, value:  1.7113, test return:  1.7000:  76%|#######5  | 3776/5000 [01:19<00:24, 49.82it/s]    error:  0.1415, value:  1.7113, test return:  1.7000:  76%|#######6  | 3808/5000 [01:19<00:23, 50.04it/s]    error:  0.1415, value:  1.7113, test return:  1.7000:  77%|#######6  | 3840/5000 [01:20<00:22, 50.55it/s]    error:  0.1415, value:  1.7113, test return:  1.7000:  77%|#######7  | 3872/5000 [01:21<00:22, 50.93it/s]    error:  0.0460, value:  1.8127, test return:  0.9000:  77%|#######7  | 3872/5000 [01:21<00:22, 50.93it/s]    error:  0.0460, value:  1.8127, test return:  0.9000:  78%|#######8  | 3904/5000 [01:21<00:22, 48.32it/s]    error:  0.0460, value:  1.8127, test return:  0.9000:  79%|#######8  | 3936/5000 [01:22<00:22, 48.17it/s]    error:  0.0460, value:  1.8127, test return:  0.9000:  79%|#######9  | 3968/5000 [01:23<00:22, 46.66it/s]    error:  0.0460, value:  1.8127, test return:  0.9000:  80%|########  | 4000/5000 [01:23<00:21, 46.71it/s]    error:  0.0460, value:  1.8127, test return:  0.9000:  81%|########  | 4032/5000 [01:24<00:21, 46.04it/s]    error:  0.0460, value:  1.8127, test return:  0.9000:  81%|########1 | 4064/5000 [01:25<00:20, 46.50it/s]    error:  0.0460, value:  1.8127, test return:  0.9000:  82%|########1 | 4096/5000 [01:25<00:18, 47.89it/s]    error:  0.0460, value:  1.8127, test return:  0.9000:  83%|########2 | 4128/5000 [01:26<00:17, 48.60it/s]    error:  0.0460, value:  1.8127, test return:  0.9000:  83%|########3 | 4160/5000 [01:27<00:16, 49.97it/s]    error:  0.0460, value:  1.8127, test return:  0.9000:  84%|########3 | 4192/5000 [01:27<00:15, 50.58it/s]    error:  0.0707, value:  1.7829, test return:  2.7000:  84%|########3 | 4192/5000 [01:28<00:15, 50.58it/s]    error:  0.0707, value:  1.7829, test return:  2.7000:  84%|########4 | 4224/5000 [01:28<00:16, 45.96it/s]    error:  0.0707, value:  1.7829, test return:  2.7000:  85%|########5 | 4256/5000 [01:29<00:15, 46.90it/s]    error:  0.0707, value:  1.7829, test return:  2.7000:  86%|########5 | 4288/5000 [01:29<00:14, 48.77it/s]    error:  0.0707, value:  1.7829, test return:  2.7000:  86%|########6 | 4320/5000 [01:30<00:13, 49.67it/s]    error:  0.0707, value:  1.7829, test return:  2.7000:  87%|########7 | 4352/5000 [01:31<00:12, 50.10it/s]    error:  0.0707, value:  1.7829, test return:  2.7000:  88%|########7 | 4384/5000 [01:31<00:12, 50.30it/s]    error:  0.0707, value:  1.7829, test return:  2.7000:  88%|########8 | 4416/5000 [01:32<00:11, 50.05it/s]    error:  0.0707, value:  1.7829, test return:  2.7000:  89%|########8 | 4448/5000 [01:33<00:11, 49.53it/s]    error:  0.0707, value:  1.7829, test return:  2.7000:  90%|########9 | 4480/5000 [01:33<00:10, 47.99it/s]    error:  0.0707, value:  1.7829, test return:  2.7000:  90%|######### | 4512/5000 [01:34<00:10, 47.02it/s]    error:  0.1540, value:  1.7193, test return:  2.5000:  90%|######### | 4512/5000 [01:35<00:10, 47.02it/s]    error:  0.1540, value:  1.7193, test return:  2.5000:  91%|######### | 4544/5000 [01:35<00:10, 42.99it/s]    error:  0.1540, value:  1.7193, test return:  2.5000:  92%|#########1| 4576/5000 [01:36<00:09, 44.92it/s]    error:  0.1540, value:  1.7193, test return:  2.5000:  92%|#########2| 4608/5000 [01:36<00:08, 46.20it/s]    error:  0.1540, value:  1.7193, test return:  2.5000:  93%|#########2| 4640/5000 [01:37<00:07, 47.52it/s]    error:  0.1540, value:  1.7193, test return:  2.5000:  93%|#########3| 4672/5000 [01:37<00:06, 47.93it/s]    error:  0.1540, value:  1.7193, test return:  2.5000:  94%|#########4| 4704/5000 [01:38<00:06, 48.55it/s]    error:  0.1540, value:  1.7193, test return:  2.5000:  95%|#########4| 4736/5000 [01:39<00:05, 49.25it/s]    error:  0.1540, value:  1.7193, test return:  2.5000:  95%|#########5| 4768/5000 [01:39<00:04, 48.23it/s]    error:  0.1540, value:  1.7193, test return:  2.5000:  96%|#########6| 4800/5000 [01:40<00:04, 48.56it/s]    error:  0.1540, value:  1.7193, test return:  2.5000:  97%|#########6| 4832/5000 [01:41<00:03, 49.70it/s]    error:  0.1047, value:  1.8499, test return:  1.0000:  97%|#########6| 4832/5000 [01:41<00:03, 49.70it/s]    error:  0.1047, value:  1.8499, test return:  1.0000:  97%|#########7| 4864/5000 [01:41<00:02, 48.45it/s]    error:  0.1047, value:  1.8499, test return:  1.0000:  98%|#########7| 4896/5000 [01:42<00:02, 48.42it/s]    error:  0.1047, value:  1.8499, test return:  1.0000:  99%|#########8| 4928/5000 [01:43<00:01, 48.76it/s]    error:  0.1047, value:  1.8499, test return:  1.0000:  99%|#########9| 4960/5000 [01:43<00:00, 47.99it/s]    error:  0.1047, value:  1.8499, test return:  1.0000: 100%|#########9| 4992/5000 [01:44<00:00, 47.08it/s]    error:  0.1047, value:  1.8499, test return:  1.0000: : 5024it [01:45, 46.37it/s]                        



.. GENERATED FROM PYTHON SOURCE LINES 622-624

We write a custom plot function to display the performance of our algorithm


.. GENERATED FROM PYTHON SOURCE LINES 624-679

.. code-block:: default



    def plot(logs, name):
        plt.figure(figsize=(15, 10))
        plt.subplot(2, 3, 1)
        plt.plot(
            logs["frames"][-len(logs["evals"]) :],
            logs["evals"],
            label="return (eval)",
        )
        plt.plot(
            logs["frames"][-len(logs["mavgs"]) :],
            logs["mavgs"],
            label="mavg of returns (eval)",
        )
        plt.xlabel("frames collected")
        plt.ylabel("trajectory length (= return)")
        plt.subplot(2, 3, 2)
        plt.plot(
            logs["traj_count"][-len(logs["evals"]) :],
            logs["evals"],
            label="return",
        )
        plt.plot(
            logs["traj_count"][-len(logs["mavgs"]) :],
            logs["mavgs"],
            label="mavg",
        )
        plt.xlabel("trajectories collected")
        plt.legend()
        plt.subplot(2, 3, 3)
        plt.plot(logs["frames"][-len(logs["losses"]) :], logs["losses"])
        plt.xlabel("frames collected")
        plt.title("loss")
        plt.subplot(2, 3, 4)
        plt.plot(logs["frames"][-len(logs["values"]) :], logs["values"])
        plt.xlabel("frames collected")
        plt.title("value")
        plt.subplot(2, 3, 5)
        plt.plot(
            logs["frames"][-len(logs["grad_vals"]) :],
            logs["grad_vals"],
        )
        plt.xlabel("frames collected")
        plt.title("grad norm")
        if len(logs["traj_lengths"]):
            plt.subplot(2, 3, 6)
            plt.plot(logs["traj_lengths"])
            plt.xlabel("batches")
            plt.title("traj length (training)")
        plt.savefig(name)
        if is_notebook():
            plt.show()









.. GENERATED FROM PYTHON SOURCE LINES 680-691

The performance of the policy can be measured as the length of trajectories.
As we can see on the results of the :func:`plot` function, the performance
of the policy increases, albeit slowly.

.. code-block:: python

   plot(logs_exp1, "dqn_td0.png")

.. figure:: /_static/img/dqn_td0.png
   :alt: Cart Pole results with TD(0)


.. GENERATED FROM PYTHON SOURCE LINES 691-696

.. code-block:: default


    print("shutting down")
    data_collector.shutdown()
    del data_collector





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    shutting down




.. GENERATED FROM PYTHON SOURCE LINES 697-715

DQN with TD(:math:`\lambda`)
----------------------------

We can improve the above algorithm by getting a better estimate of the
return, using not only the next state value but the whole sequence of rewards
and values that follow a particular step.

TorchRL provides a vectorized version of TD(lambda) named
:func:`torchrl.objectives.value.functional.vec_td_lambda_advantage_estimate`.
We'll use this to obtain a target value that the value network will be
trained to match.

The big difference in this implementation is that we'll store entire
trajectories and not single steps in the replay buffer. This will be done
automatically as long as we're not "flattening" the tensordict collected:
by keeping a shape ``[Batch x timesteps]`` and giving this
to the RB, we'll be creating a replay buffer of size
``[Capacity x timesteps]``.

.. GENERATED FROM PYTHON SOURCE LINES 715-719

.. code-block:: default



    from torchrl.objectives.value.functional import vec_td_lambda_advantage_estimate








.. GENERATED FROM PYTHON SOURCE LINES 720-722

We reset the actor parameters:


.. GENERATED FROM PYTHON SOURCE LINES 722-738

.. code-block:: default


    (
        factor,
        actor,
        actor_explore,
        params,
        params_target,
    ) = make_model(test_env)
    params_flat = params.flatten_keys(".")

    optim = torch.optim.Adam(list(params_flat.values()), lr, betas=betas)
    test_env = make_env(
        parallel=False, observation_norm_state_dict=observation_norm_state_dict
    )
    print(actor_explore(test_env.reset()))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    error:  0.1047, value:  1.8499, test return:  1.0000: : 5024it [01:55, 46.37it/s]TensorDict(
        fields={
            action: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, is_shared=False),
            action_value: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.float32, is_shared=False),
            chosen_action_value: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),
            done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),
            pixels: Tensor(shape=torch.Size([4, 64, 64]), device=cpu, dtype=torch.float32, is_shared=False),
            reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),
            step_count: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},
        batch_size=torch.Size([]),
        device=cpu,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 739-744

Data: Replay buffer and collector
---------------------------------

We need to build a new replay buffer of the appropriate size:


.. GENERATED FROM PYTHON SOURCE LINES 744-774

.. code-block:: default


    max_size = frames_per_batch // num_workers

    replay_buffer = TensorDictReplayBuffer(
        storage=LazyMemmapStorage(-(-buffer_size // max_size)),
        prefetch=n_optim,
    )

    data_collector = MultiaSyncDataCollector(
        [
            make_env(
                parallel=True, observation_norm_state_dict=observation_norm_state_dict
            ),
        ]
        * num_collectors,
        policy=actor_explore,
        frames_per_batch=frames_per_batch,
        total_frames=total_frames,
        exploration_mode="random",
        devices=[device] * num_collectors,
        storing_devices=[device] * num_collectors,
        # devices=[f"cuda:{i}" for i in range(1, 1 + num_collectors)],
        # storing_devices=[f"cuda:{i}" for i in range(1, 1 + num_collectors)],
        split_trajs=False,
    )


    logs_exp2 = defaultdict(list)
    prev_traj_count = 0








.. GENERATED FROM PYTHON SOURCE LINES 775-785

Training loop
-------------

There are very few differences with the training loop above:

- The tensordict received by the collector is used as-is, without being
  flattened (recall the ``data.view(-1)`` above), to keep the temporal
  relation between consecutive steps.
- We use :func:`vec_td_lambda_advantage_estimate` to compute the target
  value.

.. GENERATED FROM PYTHON SOURCE LINES 785-882

.. code-block:: default


    pbar = tqdm.tqdm(total=total_frames)
    for j, data in enumerate(data_collector):
        current_frames = data.numel()
        pbar.update(current_frames)

        replay_buffer.extend(data.cpu())
        if len(logs_exp2["frames"]):
            logs_exp2["frames"].append(current_frames + logs_exp2["frames"][-1])
        else:
            logs_exp2["frames"].append(current_frames)

        if data["next", "done"].any():
            done = data["next", "done"].squeeze(-1)
            logs_exp2["traj_lengths"].append(
                data["next", "step_count"][done].float().mean().item()
            )

        if sum(logs_exp2["frames"]) > init_random_frames:
            for _ in range(n_optim):
                sampled_data = replay_buffer.sample(batch_size // max_size)
                sampled_data = sampled_data.clone().to(device, non_blocking=True)

                reward = sampled_data["next", "reward"]
                done = sampled_data["next", "done"].to(reward.dtype)
                action = sampled_data["action"].clone()

                sampled_data_out = sampled_data.select(*actor.in_keys)
                sampled_data_out = vmap(factor, (0, None))(sampled_data_out, params)
                action_value = sampled_data_out["action_value"]
                action_value = (action_value * action.to(action_value.dtype)).sum(-1, True)
                with torch.no_grad():
                    tdstep = step_mdp(sampled_data)
                    next_value = vmap(factor, (0, None))(
                        tdstep.select(*actor.in_keys), params
                    )
                    next_value = next_value["chosen_action_value"]
                error = vec_td_lambda_advantage_estimate(
                    gamma,
                    lmbda,
                    action_value,
                    next_value,
                    reward,
                    done,
                ).pow(2)
                error = error.mean()
                error.backward()

                gv = nn.utils.clip_grad_norm_(list(params_flat.values()), 1)

                optim.step()
                optim.zero_grad()

                # update of the target parameters
                params_target.apply(
                    lambda p_target, p_orig: p_orig * tau + p_target * (1 - tau),
                    params.detach(),
                    inplace=True,
                )

            actor_explore.step(current_frames)

            # Logging
            logs_exp2["grad_vals"].append(float(gv))

            logs_exp2["losses"].append(error.item())
            logs_exp2["values"].append(action_value.mean().item())
            logs_exp2["traj_count"].append(
                prev_traj_count + data["next", "done"].sum().item()
            )
            prev_traj_count = logs_exp2["traj_count"][-1]
            if j % 10 == 0:
                with set_exploration_mode("mode"), torch.no_grad():
                    # execute a rollout. The `set_exploration_mode("mode")` has
                    # no effect here since the policy is deterministic, but we add
                    # it for completeness
                    eval_rollout = test_env.rollout(
                        max_steps=10000,
                        policy=actor,
                    ).cpu()
                logs_exp2["traj_lengths_eval"].append(eval_rollout.shape[-1])
                logs_exp2["evals"].append(eval_rollout["next", "reward"].sum().item())
                if len(logs_exp2["mavgs"]):
                    logs_exp2["mavgs"].append(
                        logs_exp2["evals"][-1] * 0.05 + logs_exp2["mavgs"][-1] * 0.95
                    )
                else:
                    logs_exp2["mavgs"].append(logs_exp2["evals"][-1])
                logs_exp2["traj_count_eval"].append(logs_exp2["traj_count"][-1])
                pbar.set_description(
                    f"error: {error: 4.4f}, value: {action_value.mean(): 4.4f}, test return: {logs_exp2['evals'][-1]: 4.4f}"
                )

        # update policy weights
        data_collector.update_policy_weights_()






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


      0%|          | 0/5000 [00:00<?, ?it/s]    error:  0.1047, value:  1.8499, test return:  1.0000: : 5024it [02:08, 39.20it/s]

      1%|          | 32/5000 [00:00<02:17, 36.08it/s]Creating a MemmapStorage...
    The storage is being created: 
            _batch_size: /tmp/tmpcqtokagw, 0.00238800048828125 Mb of storage (size: torch.Size([313, 1])).
            action: /tmp/tmpdd26b4js, 0.076416015625 Mb of storage (size: torch.Size([313, 16, 2])).
            action_value: /tmp/tmpu79mhdwt, 0.0382080078125 Mb of storage (size: torch.Size([313, 16, 2])).
            chosen_action_value: /tmp/tmp4dkwezdw, 0.01910400390625 Mb of storage (size: torch.Size([313, 16, 1])).
            done: /tmp/tmp773twq9q, 0.0047760009765625 Mb of storage (size: torch.Size([313, 16, 1])).
            index: /tmp/tmpvidv_95c, 0.001194000244140625 Mb of storage (size: torch.Size([313])).
            pixels: /tmp/tmpao5gsqaw, 313.0 Mb of storage (size: torch.Size([313, 16, 4, 64, 64])).
            reward: /tmp/tmpk9idle_w, 0.01910400390625 Mb of storage (size: torch.Size([313, 16, 1])).
            step_count: /tmp/tmp7rl96f6w, 0.0382080078125 Mb of storage (size: torch.Size([313, 16])).
            ('collector', 'traj_ids'): /tmp/tmp1pp5eqqc, 0.0382080078125 Mb of storage (size: torch.Size([313, 16])).
            ('next', 'done'): /tmp/tmpgczdapbi, 0.0047760009765625 Mb of storage (size: torch.Size([313, 16, 1])).
            ('next', 'pixels'): /tmp/tmp7km0vr3i, 313.0 Mb of storage (size: torch.Size([313, 16, 4, 64, 64])).
            ('next', 'reward'): /tmp/tmpw6pgtz6e, 0.01910400390625 Mb of storage (size: torch.Size([313, 16, 1])).
            ('next', 'step_count'): /tmp/tmpl61jfupu, 0.0382080078125 Mb of storage (size: torch.Size([313, 16])).

      2%|1         | 96/5000 [00:01<01:30, 54.46it/s]
      3%|2         | 128/5000 [00:02<01:37, 50.04it/s]
      3%|3         | 160/5000 [00:03<01:39, 48.44it/s]
      4%|3         | 192/5000 [00:03<01:41, 47.43it/s]
      4%|4         | 224/5000 [00:04<01:45, 45.23it/s]
      5%|5         | 256/5000 [00:05<01:47, 43.94it/s]
      6%|5         | 288/5000 [00:06<01:47, 43.78it/s]
      6%|6         | 320/5000 [00:07<01:46, 43.77it/s]
      7%|7         | 352/5000 [00:07<01:44, 44.50it/s]
    error:  0.9617, value:  2.2182, test return:  0.9000:   7%|7         | 352/5000 [00:08<01:44, 44.50it/s]
    error:  0.9617, value:  2.2182, test return:  0.9000:   8%|7         | 384/5000 [00:08<01:44, 43.98it/s]
    error:  0.9617, value:  2.2182, test return:  0.9000:   8%|8         | 416/5000 [00:09<01:42, 44.90it/s]
    error:  0.9617, value:  2.2182, test return:  0.9000:   9%|8         | 448/5000 [00:09<01:39, 45.64it/s]
    error:  0.9617, value:  2.2182, test return:  0.9000:  10%|9         | 480/5000 [00:10<01:38, 46.05it/s]
    error:  0.9617, value:  2.2182, test return:  0.9000:  10%|#         | 512/5000 [00:11<01:36, 46.55it/s]
    error:  0.9617, value:  2.2182, test return:  0.9000:  11%|#         | 544/5000 [00:11<01:33, 47.47it/s]
    error:  0.9617, value:  2.2182, test return:  0.9000:  12%|#1        | 576/5000 [00:12<01:32, 48.06it/s]
    error:  0.9617, value:  2.2182, test return:  0.9000:  12%|#2        | 608/5000 [00:13<01:30, 48.65it/s]
    error:  0.9617, value:  2.2182, test return:  0.9000:  13%|#2        | 640/5000 [00:13<01:30, 48.08it/s]
    error:  0.9617, value:  2.2182, test return:  0.9000:  13%|#3        | 672/5000 [00:14<01:31, 47.47it/s]
    error:  0.7502, value:  0.9740, test return:  1.3000:  13%|#3        | 672/5000 [00:15<01:31, 47.47it/s]
    error:  0.7502, value:  0.9740, test return:  1.3000:  14%|#4        | 704/5000 [00:15<01:37, 44.25it/s]
    error:  0.7502, value:  0.9740, test return:  1.3000:  15%|#4        | 736/5000 [00:16<01:38, 43.42it/s]
    error:  0.7502, value:  0.9740, test return:  1.3000:  15%|#5        | 768/5000 [00:16<01:38, 42.83it/s]
    error:  0.7502, value:  0.9740, test return:  1.3000:  16%|#6        | 800/5000 [00:17<01:34, 44.27it/s]
    error:  0.7502, value:  0.9740, test return:  1.3000:  17%|#6        | 832/5000 [00:18<01:30, 46.05it/s]
    error:  0.7502, value:  0.9740, test return:  1.3000:  17%|#7        | 864/5000 [00:18<01:28, 46.66it/s]
    error:  0.7502, value:  0.9740, test return:  1.3000:  18%|#7        | 896/5000 [00:19<01:27, 47.17it/s]
    error:  0.7502, value:  0.9740, test return:  1.3000:  19%|#8        | 928/5000 [00:20<01:25, 47.55it/s]
    error:  0.7502, value:  0.9740, test return:  1.3000:  19%|#9        | 960/5000 [00:20<01:24, 47.78it/s]
    error:  0.7502, value:  0.9740, test return:  1.3000:  20%|#9        | 992/5000 [00:21<01:23, 47.99it/s]
    error:  0.7031, value:  1.6319, test return:  2.8000:  20%|#9        | 992/5000 [00:22<01:23, 47.99it/s]
    error:  0.7031, value:  1.6319, test return:  2.8000:  20%|##        | 1024/5000 [00:22<01:30, 43.88it/s]
    error:  0.7031, value:  1.6319, test return:  2.8000:  21%|##1       | 1056/5000 [00:22<01:26, 45.61it/s]
    error:  0.7031, value:  1.6319, test return:  2.8000:  22%|##1       | 1088/5000 [00:23<01:24, 46.25it/s]
    error:  0.7031, value:  1.6319, test return:  2.8000:  22%|##2       | 1120/5000 [00:24<01:24, 46.05it/s]
    error:  0.7031, value:  1.6319, test return:  2.8000:  23%|##3       | 1152/5000 [00:25<01:23, 46.19it/s]
    error:  0.7031, value:  1.6319, test return:  2.8000:  24%|##3       | 1184/5000 [00:25<01:23, 45.56it/s]
    error:  0.7031, value:  1.6319, test return:  2.8000:  24%|##4       | 1216/5000 [00:26<01:25, 44.14it/s]
    error:  0.7031, value:  1.6319, test return:  2.8000:  25%|##4       | 1248/5000 [00:27<01:24, 44.32it/s]
    error:  0.7031, value:  1.6319, test return:  2.8000:  26%|##5       | 1280/5000 [00:27<01:21, 45.56it/s]
    error:  0.7031, value:  1.6319, test return:  2.8000:  26%|##6       | 1312/5000 [00:28<01:19, 46.63it/s]
    error:  1.4977, value:  2.0827, test return:  0.8000:  26%|##6       | 1312/5000 [00:29<01:19, 46.63it/s]
    error:  1.4977, value:  2.0827, test return:  0.8000:  27%|##6       | 1344/5000 [00:29<01:20, 45.57it/s]
    error:  1.4977, value:  2.0827, test return:  0.8000:  28%|##7       | 1376/5000 [00:29<01:18, 46.41it/s]
    error:  1.4977, value:  2.0827, test return:  0.8000:  28%|##8       | 1408/5000 [00:30<01:16, 47.14it/s]
    error:  1.4977, value:  2.0827, test return:  0.8000:  29%|##8       | 1440/5000 [00:31<01:14, 48.08it/s]
    error:  1.4977, value:  2.0827, test return:  0.8000:  29%|##9       | 1472/5000 [00:31<01:13, 48.00it/s]
    error:  1.4977, value:  2.0827, test return:  0.8000:  30%|###       | 1504/5000 [00:32<01:12, 47.94it/s]
    error:  1.4977, value:  2.0827, test return:  0.8000:  31%|###       | 1536/5000 [00:33<01:12, 47.75it/s]
    error:  1.4977, value:  2.0827, test return:  0.8000:  31%|###1      | 1568/5000 [00:33<01:12, 47.33it/s]
    error:  1.4977, value:  2.0827, test return:  0.8000:  32%|###2      | 1600/5000 [00:34<01:15, 45.29it/s]
    error:  1.4977, value:  2.0827, test return:  0.8000:  33%|###2      | 1632/5000 [00:35<01:13, 45.76it/s]
    error:  1.2213, value:  2.3996, test return:  0.9000:  33%|###2      | 1632/5000 [00:36<01:13, 45.76it/s]
    error:  1.2213, value:  2.3996, test return:  0.9000:  33%|###3      | 1664/5000 [00:36<01:16, 43.42it/s]
    error:  1.2213, value:  2.3996, test return:  0.9000:  34%|###3      | 1696/5000 [00:36<01:15, 43.59it/s]
    error:  1.2213, value:  2.3996, test return:  0.9000:  35%|###4      | 1728/5000 [00:37<01:13, 44.77it/s]
    error:  1.2213, value:  2.3996, test return:  0.9000:  35%|###5      | 1760/5000 [00:38<01:10, 46.02it/s]
    error:  1.2213, value:  2.3996, test return:  0.9000:  36%|###5      | 1792/5000 [00:38<01:09, 46.06it/s]
    error:  1.2213, value:  2.3996, test return:  0.9000:  36%|###6      | 1824/5000 [00:39<01:07, 46.75it/s]
    error:  1.2213, value:  2.3996, test return:  0.9000:  37%|###7      | 1856/5000 [00:40<01:06, 47.47it/s]
    error:  1.2213, value:  2.3996, test return:  0.9000:  38%|###7      | 1888/5000 [00:40<01:04, 48.21it/s]
    error:  1.2213, value:  2.3996, test return:  0.9000:  38%|###8      | 1920/5000 [00:41<01:04, 48.04it/s]
    error:  1.2213, value:  2.3996, test return:  0.9000:  39%|###9      | 1952/5000 [00:42<01:03, 48.37it/s]
    error:  0.7361, value:  0.6275, test return:  3.0000:  39%|###9      | 1952/5000 [00:43<01:03, 48.37it/s]
    error:  0.7361, value:  0.6275, test return:  3.0000:  40%|###9      | 1984/5000 [00:43<01:10, 42.93it/s]
    error:  0.7361, value:  0.6275, test return:  3.0000:  40%|####      | 2016/5000 [00:43<01:06, 44.69it/s]
    error:  0.7361, value:  0.6275, test return:  3.0000:  41%|####      | 2048/5000 [00:44<01:05, 45.00it/s]
    error:  0.7361, value:  0.6275, test return:  3.0000:  42%|####1     | 2080/5000 [00:45<01:04, 45.61it/s]
    error:  0.7361, value:  0.6275, test return:  3.0000:  42%|####2     | 2112/5000 [00:45<01:05, 44.33it/s]
    error:  0.7361, value:  0.6275, test return:  3.0000:  43%|####2     | 2144/5000 [00:46<01:05, 43.43it/s]
    error:  0.7361, value:  0.6275, test return:  3.0000:  44%|####3     | 2176/5000 [00:47<01:03, 44.48it/s]
    error:  0.7361, value:  0.6275, test return:  3.0000:  44%|####4     | 2208/5000 [00:48<01:01, 45.68it/s]
    error:  0.7361, value:  0.6275, test return:  3.0000:  45%|####4     | 2240/5000 [00:48<00:59, 46.09it/s]
    error:  0.7361, value:  0.6275, test return:  3.0000:  45%|####5     | 2272/5000 [00:49<00:58, 46.58it/s]
    error:  0.3508, value:  1.3348, test return:  4.5000:  45%|####5     | 2272/5000 [00:50<00:58, 46.58it/s]
    error:  0.3508, value:  1.3348, test return:  4.5000:  46%|####6     | 2304/5000 [00:50<01:06, 40.46it/s]
    error:  0.3508, value:  1.3348, test return:  4.5000:  47%|####6     | 2336/5000 [00:51<01:01, 42.98it/s]
    error:  0.3508, value:  1.3348, test return:  4.5000:  47%|####7     | 2368/5000 [00:51<00:58, 45.16it/s]
    error:  0.3508, value:  1.3348, test return:  4.5000:  48%|####8     | 2400/5000 [00:52<00:55, 46.56it/s]
    error:  0.3508, value:  1.3348, test return:  4.5000:  49%|####8     | 2432/5000 [00:53<00:54, 47.17it/s]
    error:  0.3508, value:  1.3348, test return:  4.5000:  49%|####9     | 2464/5000 [00:53<00:53, 47.06it/s]
    error:  0.3508, value:  1.3348, test return:  4.5000:  50%|####9     | 2496/5000 [00:54<00:52, 47.52it/s]
    error:  0.3508, value:  1.3348, test return:  4.5000:  51%|#####     | 2528/5000 [00:55<00:52, 46.65it/s]
    error:  0.3508, value:  1.3348, test return:  4.5000:  51%|#####1    | 2560/5000 [00:55<00:53, 45.42it/s]
    error:  0.3508, value:  1.3348, test return:  4.5000:  52%|#####1    | 2592/5000 [00:56<00:54, 43.92it/s]
    error:  0.5576, value:  1.4360, test return:  2.5000:  52%|#####1    | 2592/5000 [00:57<00:54, 43.92it/s]
    error:  0.5576, value:  1.4360, test return:  2.5000:  52%|#####2    | 2624/5000 [00:57<00:58, 40.49it/s]
    error:  0.5576, value:  1.4360, test return:  2.5000:  53%|#####3    | 2656/5000 [00:58<00:54, 42.93it/s]
    error:  0.5576, value:  1.4360, test return:  2.5000:  54%|#####3    | 2688/5000 [00:58<00:52, 43.88it/s]
    error:  0.5576, value:  1.4360, test return:  2.5000:  54%|#####4    | 2720/5000 [00:59<00:49, 45.74it/s]
    error:  0.5576, value:  1.4360, test return:  2.5000:  55%|#####5    | 2752/5000 [01:00<00:47, 47.06it/s]
    error:  0.5576, value:  1.4360, test return:  2.5000:  56%|#####5    | 2784/5000 [01:00<00:45, 48.27it/s]
    error:  0.5576, value:  1.4360, test return:  2.5000:  56%|#####6    | 2816/5000 [01:01<00:45, 47.92it/s]
    error:  0.5576, value:  1.4360, test return:  2.5000:  57%|#####6    | 2848/5000 [01:02<00:44, 48.24it/s]
    error:  0.5576, value:  1.4360, test return:  2.5000:  58%|#####7    | 2880/5000 [01:02<00:43, 48.40it/s]
    error:  0.5576, value:  1.4360, test return:  2.5000:  58%|#####8    | 2912/5000 [01:03<00:42, 48.62it/s]
    error:  0.6888, value:  1.0340, test return:  3.0000:  58%|#####8    | 2912/5000 [01:04<00:42, 48.62it/s]
    error:  0.6888, value:  1.0340, test return:  3.0000:  59%|#####8    | 2944/5000 [01:04<00:47, 43.42it/s]
    error:  0.6888, value:  1.0340, test return:  3.0000:  60%|#####9    | 2976/5000 [01:05<00:46, 43.86it/s]
    error:  0.6888, value:  1.0340, test return:  3.0000:  60%|######    | 3008/5000 [01:05<00:45, 43.78it/s]
    error:  0.6888, value:  1.0340, test return:  3.0000:  61%|######    | 3040/5000 [01:06<00:44, 43.64it/s]
    error:  0.6888, value:  1.0340, test return:  3.0000:  61%|######1   | 3072/5000 [01:07<00:44, 43.08it/s]
    error:  0.6888, value:  1.0340, test return:  3.0000:  62%|######2   | 3104/5000 [01:07<00:42, 44.51it/s]
    error:  0.6888, value:  1.0340, test return:  3.0000:  63%|######2   | 3136/5000 [01:08<00:40, 46.01it/s]
    error:  0.6888, value:  1.0340, test return:  3.0000:  63%|######3   | 3168/5000 [01:09<00:39, 46.67it/s]
    error:  0.6888, value:  1.0340, test return:  3.0000:  64%|######4   | 3200/5000 [01:09<00:38, 46.84it/s]
    error:  0.6888, value:  1.0340, test return:  3.0000:  65%|######4   | 3232/5000 [01:10<00:37, 47.56it/s]
    error:  0.8237, value:  2.0964, test return:  3.8000:  65%|######4   | 3232/5000 [01:11<00:37, 47.56it/s]
    error:  0.8237, value:  2.0964, test return:  3.8000:  65%|######5   | 3264/5000 [01:11<00:41, 42.19it/s]
    error:  0.8237, value:  2.0964, test return:  3.8000:  66%|######5   | 3296/5000 [01:12<00:38, 44.14it/s]
    error:  0.8237, value:  2.0964, test return:  3.8000:  67%|######6   | 3328/5000 [01:12<00:36, 45.40it/s]
    error:  0.8237, value:  2.0964, test return:  3.8000:  67%|######7   | 3360/5000 [01:13<00:35, 46.38it/s]
    error:  0.8237, value:  2.0964, test return:  3.8000:  68%|######7   | 3392/5000 [01:14<00:34, 47.27it/s]
    error:  0.8237, value:  2.0964, test return:  3.8000:  68%|######8   | 3424/5000 [01:14<00:33, 47.36it/s]
    error:  0.8237, value:  2.0964, test return:  3.8000:  69%|######9   | 3456/5000 [01:15<00:33, 46.59it/s]
    error:  0.8237, value:  2.0964, test return:  3.8000:  70%|######9   | 3488/5000 [01:16<00:33, 44.76it/s]
    error:  0.8237, value:  2.0964, test return:  3.8000:  70%|#######   | 3520/5000 [01:16<00:32, 45.13it/s]
    error:  0.8237, value:  2.0964, test return:  3.8000:  71%|#######1  | 3552/5000 [01:17<00:31, 45.35it/s]
    error:  0.5162, value:  1.6644, test return:  4.9000:  71%|#######1  | 3552/5000 [01:18<00:31, 45.35it/s]
    error:  0.5162, value:  1.6644, test return:  4.9000:  72%|#######1  | 3584/5000 [01:18<00:36, 38.69it/s]
    error:  0.5162, value:  1.6644, test return:  4.9000:  72%|#######2  | 3616/5000 [01:19<00:33, 41.45it/s]
    error:  0.5162, value:  1.6644, test return:  4.9000:  73%|#######2  | 3648/5000 [01:20<00:31, 43.32it/s]
    error:  0.5162, value:  1.6644, test return:  4.9000:  74%|#######3  | 3680/5000 [01:20<00:29, 44.90it/s]
    error:  0.5162, value:  1.6644, test return:  4.9000:  74%|#######4  | 3712/5000 [01:21<00:27, 46.02it/s]
    error:  0.5162, value:  1.6644, test return:  4.9000:  75%|#######4  | 3744/5000 [01:22<00:26, 46.77it/s]
    error:  0.5162, value:  1.6644, test return:  4.9000:  76%|#######5  | 3776/5000 [01:22<00:25, 47.29it/s]
    error:  0.5162, value:  1.6644, test return:  4.9000:  76%|#######6  | 3808/5000 [01:23<00:24, 47.78it/s]
    error:  0.5162, value:  1.6644, test return:  4.9000:  77%|#######6  | 3840/5000 [01:24<00:24, 47.75it/s]
    error:  0.5162, value:  1.6644, test return:  4.9000:  77%|#######7  | 3872/5000 [01:24<00:24, 46.81it/s]
    error:  1.0170, value:  2.2165, test return:  2.2000:  77%|#######7  | 3872/5000 [01:25<00:24, 46.81it/s]
    error:  1.0170, value:  2.2165, test return:  2.2000:  78%|#######8  | 3904/5000 [01:25<00:25, 42.90it/s]
    error:  1.0170, value:  2.2165, test return:  2.2000:  79%|#######8  | 3936/5000 [01:26<00:25, 41.87it/s]
    error:  1.0170, value:  2.2165, test return:  2.2000:  79%|#######9  | 3968/5000 [01:27<00:24, 42.92it/s]
    error:  1.0170, value:  2.2165, test return:  2.2000:  80%|########  | 4000/5000 [01:27<00:22, 44.26it/s]
    error:  1.0170, value:  2.2165, test return:  2.2000:  81%|########  | 4032/5000 [01:28<00:21, 45.13it/s]
    error:  1.0170, value:  2.2165, test return:  2.2000:  81%|########1 | 4064/5000 [01:29<00:20, 46.55it/s]
    error:  1.0170, value:  2.2165, test return:  2.2000:  82%|########1 | 4096/5000 [01:29<00:19, 46.95it/s]
    error:  1.0170, value:  2.2165, test return:  2.2000:  83%|########2 | 4128/5000 [01:30<00:18, 46.85it/s]
    error:  1.0170, value:  2.2165, test return:  2.2000:  83%|########3 | 4160/5000 [01:31<00:17, 47.35it/s]
    error:  1.0170, value:  2.2165, test return:  2.2000:  84%|########3 | 4192/5000 [01:31<00:16, 47.99it/s]
    error:  0.1595, value:  0.9605, test return:  8.3000:  84%|########3 | 4192/5000 [01:33<00:16, 47.99it/s]
    error:  0.1595, value:  0.9605, test return:  8.3000:  84%|########4 | 4224/5000 [01:33<00:21, 35.98it/s]
    error:  0.1595, value:  0.9605, test return:  8.3000:  85%|########5 | 4256/5000 [01:34<00:21, 34.58it/s]
    error:  0.1595, value:  0.9605, test return:  8.3000:  86%|########5 | 4288/5000 [01:34<00:19, 37.20it/s]
    error:  0.1595, value:  0.9605, test return:  8.3000:  86%|########6 | 4320/5000 [01:35<00:17, 38.70it/s]
    error:  0.1595, value:  0.9605, test return:  8.3000:  87%|########7 | 4352/5000 [01:36<00:15, 41.86it/s]
    error:  0.1595, value:  0.9605, test return:  8.3000:  88%|########7 | 4384/5000 [01:37<00:14, 42.68it/s]
    error:  0.1595, value:  0.9605, test return:  8.3000:  88%|########8 | 4416/5000 [01:37<00:13, 44.83it/s]
    error:  0.1595, value:  0.9605, test return:  8.3000:  89%|########8 | 4448/5000 [01:38<00:12, 44.53it/s]
    error:  0.1595, value:  0.9605, test return:  8.3000:  90%|########9 | 4480/5000 [01:39<00:11, 45.50it/s]
    error:  0.1595, value:  0.9605, test return:  8.3000:  90%|######### | 4512/5000 [01:39<00:10, 46.46it/s]
    error:  0.2171, value:  2.5341, test return:  3.7000:  90%|######### | 4512/5000 [01:40<00:10, 46.46it/s]
    error:  0.2171, value:  2.5341, test return:  3.7000:  91%|######### | 4544/5000 [01:40<00:10, 41.48it/s]
    error:  0.2171, value:  2.5341, test return:  3.7000:  92%|#########1| 4576/5000 [01:41<00:09, 43.43it/s]
    error:  0.2171, value:  2.5341, test return:  3.7000:  92%|#########2| 4608/5000 [01:41<00:08, 44.58it/s]
    error:  0.2171, value:  2.5341, test return:  3.7000:  93%|#########2| 4640/5000 [01:42<00:07, 45.79it/s]
    error:  0.2171, value:  2.5341, test return:  3.7000:  93%|#########3| 4672/5000 [01:43<00:06, 46.95it/s]
    error:  0.2171, value:  2.5341, test return:  3.7000:  94%|#########4| 4704/5000 [01:43<00:06, 48.08it/s]
    error:  0.2171, value:  2.5341, test return:  3.7000:  95%|#########4| 4736/5000 [01:44<00:05, 47.75it/s]
    error:  0.2171, value:  2.5341, test return:  3.7000:  95%|#########5| 4768/5000 [01:45<00:04, 46.86it/s]
    error:  0.2171, value:  2.5341, test return:  3.7000:  96%|#########6| 4800/5000 [01:46<00:04, 46.30it/s]
    error:  0.2171, value:  2.5341, test return:  3.7000:  97%|#########6| 4832/5000 [01:46<00:03, 45.57it/s]
    error:  0.6576, value:  1.7512, test return:  1.8000:  97%|#########6| 4832/5000 [01:47<00:03, 45.57it/s]
    error:  0.6576, value:  1.7512, test return:  1.8000:  97%|#########7| 4864/5000 [01:47<00:03, 43.17it/s]
    error:  0.6576, value:  1.7512, test return:  1.8000:  98%|#########7| 4896/5000 [01:48<00:02, 43.92it/s]
    error:  0.6576, value:  1.7512, test return:  1.8000:  99%|#########8| 4928/5000 [01:48<00:01, 45.39it/s]
    error:  0.6576, value:  1.7512, test return:  1.8000:  99%|#########9| 4960/5000 [01:49<00:00, 46.75it/s]
    error:  0.6576, value:  1.7512, test return:  1.8000: 100%|#########9| 4992/5000 [01:50<00:00, 47.08it/s]
    error:  0.6576, value:  1.7512, test return:  1.8000: : 5024it [01:50, 47.70it/s]                        



.. GENERATED FROM PYTHON SOURCE LINES 883-893

TD(:math:`\lambda`) performs significantly better than TD(0) because it
retrieves a much less biased estimate of the state-action value.

.. code-block:: python

   plot(logs_exp2, "dqn_tdlambda.png")

.. figure:: /_static/img/dqn_tdlambda.png
   :alt: Cart Pole results with TD(lambda)


.. GENERATED FROM PYTHON SOURCE LINES 893-899

.. code-block:: default



    print("shutting down")
    data_collector.shutdown()
    del data_collector





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    shutting down




.. GENERATED FROM PYTHON SOURCE LINES 900-906

Let's compare the results on a single plot. Because the TD(lambda) version
works better, we'll have fewer episodes collected for a given number of
frames (as there are more frames per episode).

**Note**: As already mentioned above, to get a more reasonable performance,
use a greater value for ``total_frames`` e.g. 500000.

.. GENERATED FROM PYTHON SOURCE LINES 906-961

.. code-block:: default



    def plot_both():
        frames_td0 = logs_exp1["frames"]
        frames_tdlambda = logs_exp2["frames"]
        evals_td0 = logs_exp1["evals"]
        evals_tdlambda = logs_exp2["evals"]
        mavgs_td0 = logs_exp1["mavgs"]
        mavgs_tdlambda = logs_exp2["mavgs"]
        traj_count_td0 = logs_exp1["traj_count_eval"]
        traj_count_tdlambda = logs_exp2["traj_count_eval"]

        plt.figure(figsize=(15, 10))
        plt.subplot(1, 2, 1)
        plt.plot(frames_td0[-len(evals_td0) :], evals_td0, label="return (td0)", alpha=0.5)
        plt.plot(
            frames_tdlambda[-len(evals_tdlambda) :],
            evals_tdlambda,
            label="return (td(lambda))",
            alpha=0.5,
        )
        plt.plot(frames_td0[-len(mavgs_td0) :], mavgs_td0, label="mavg (td0)")
        plt.plot(
            frames_tdlambda[-len(mavgs_tdlambda) :],
            mavgs_tdlambda,
            label="mavg (td(lambda))",
        )
        plt.xlabel("frames collected")
        plt.ylabel("trajectory length (= return)")

        plt.subplot(1, 2, 2)
        plt.plot(
            traj_count_td0[-len(evals_td0) :],
            evals_td0,
            label="return (td0)",
            alpha=0.5,
        )
        plt.plot(
            traj_count_tdlambda[-len(evals_tdlambda) :],
            evals_tdlambda,
            label="return (td(lambda))",
            alpha=0.5,
        )
        plt.plot(traj_count_td0[-len(mavgs_td0) :], mavgs_td0, label="mavg (td0)")
        plt.plot(
            traj_count_tdlambda[-len(mavgs_tdlambda) :],
            mavgs_tdlambda,
            label="mavg (td(lambda))",
        )
        plt.xlabel("trajectories collected")
        plt.legend()

        plt.savefig("dqn.png")









.. GENERATED FROM PYTHON SOURCE LINES 962-979

.. code-block:: python

   plot_both()

.. figure:: /_static/img/dqn.png
   :alt: Cart Pole results from the TD(:math:`lambda`) trained policy.

Finally, we generate a new video to check what the algorithm has learnt.
If all goes well, the duration should be significantly longer than with a
random rollout.

To get the raw pixels of the rollout, we insert a
:class:`torchrl.envs.CatTensors` transform that precedes all others and copies
the ``"pixels"`` key onto a ``"pixels_save"`` key. This is necessary because
the other transforms that modify this key will update its value in-place in
the output tensordict.


.. GENERATED FROM PYTHON SOURCE LINES 979-986

.. code-block:: default


    test_env.transform.insert(0, CatTensors(["pixels"], "pixels_save", del_keys=False))
    eval_rollout = test_env.rollout(max_steps=10000, policy=actor, auto_reset=True).cpu()


    del test_env








.. GENERATED FROM PYTHON SOURCE LINES 992-1001

The video of the rollout can be saved using the imageio package:

.. code-block::

  import imageio
  imageio.mimwrite('cartpole.mp4', eval_rollout["pixels_save"].numpy(), fps=30);

.. figure:: /_static/img/cartpole.gif
   :alt: Cart Pole results from the TD(:math:`\lambda`) trained policy.

.. GENERATED FROM PYTHON SOURCE LINES 1003-1039

Conclusion and possible improvements
------------------------------------

In this tutorial we have learnt:

- How to train a policy that read pixel-based states, what transforms to
  include and how to normalize the data;
- How to create a policy that picks up the action with the highest value
  with :class:`torchrl.modules.QValueNetwork`;
- How to build a multiprocessed data collector;
- How to train a DQN with TD(:math:`\lambda`) returns.

We have seen that using TD(:math:`\lambda`) greatly improved the performance
of DQN. Other possible improvements could include:

- Using the Multi-Step post-processing. Multi-step will project an action
  to the nth following step, and create a discounted sum of the rewards in
  between. This trick can make the algorithm noticebly less myopic. To use
  this, simply create the collector with

      from torchrl.data.postprocs.postprocs import MultiStep
      collector = CollectorClass(..., postproc=MultiStep(gamma, n))

  where ``n`` is the number of looking-forward steps. Pay attention to the
  fact that the ``gamma`` factor has to be corrected by the number of
  steps till the next observation when being passed to
  ``vec_td_lambda_advantage_estimate``:

      gamma = gamma ** tensordict["steps_to_next_obs"]
- A prioritized replay buffer could also be used. This will give a
  higher priority to samples that have the worst value accuracy.
- A distributional loss (see ``torchrl.objectives.DistributionalDQNLoss``
  for more information).
- More fancy exploration techniques, such as NoisyLinear layers and such
  (check ``torchrl.modules.NoisyLinear``, which is fully compatible with the
  ``MLP`` class used in our Dueling DQN).


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 5 minutes  26.315 seconds)

**Estimated memory usage:**  1574 MB


.. _sphx_glr_download_tutorials_coding_dqn.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: coding_dqn.py <coding_dqn.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: coding_dqn.ipynb <coding_dqn.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
