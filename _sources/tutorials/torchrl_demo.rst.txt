
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorials/torchrl_demo.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_tutorials_torchrl_demo.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorials_torchrl_demo.py:


Introduction to TorchRL
============================
This demo was presented at ICML 2022 on the industry demo day.

.. GENERATED FROM PYTHON SOURCE LINES 8-115

It gives a good overview of TorchRL functionalities. Feel free to reach out
to vmoens@fb.com or submit issues if you have questions or comments about
it.

TorchRL is an open-source Reinforcement Learning (RL) library for PyTorch.

https://github.com/pytorch/rl

The PyTorch ecosystem team (Meta) has decided to invest in that library to
provide a leading platform to develop RL solutions in research settings.

It provides pytorch and **python-first**, low and high level
**abstractions** # for RL that are intended to be efficient, documented and
properly tested.
The code is aimed at supporting research in RL. Most of it is written in
python in a highly modular way, such that researchers can easily swap
components, transform them or write new ones with little effort.

This repo attempts to align with the existing pytorch ecosystem libraries
in that it has a dataset pillar (torchrl/envs), transforms, models, data
utilities (e.g. collectors and containers), etc. TorchRL aims at having as
few dependencies as possible (python standard library, numpy and pytorch).
Common environment libraries (e.g. OpenAI gym) are only optional.

**Content**: TODO: should the presentation get changed ?

torchrl

- collectors
   - collectors.py
- data
   - tensor_specs.py
   - postprocs
      - postprocs.py
   - replay_buffers
      - replay_buffers.py
      - storages.py
- envs
   - common.py
   - env_creator.py
   - gym_like.py
   - vec_env.py
   - libs
      - dm_control.py
      - gym.py
   - transforms
      - functional.py
      - transforms.py
- modules
   - distributions
      - continuous.py
      - discrete.py
   - models
      - models.py
      - exploration.py
   - tensordict_module
      - actors.py
      - common.py
      - exploration.py
      - probabilistic.py
      - sequence.py
- objectives
   - common.py
   - ddpg.py
   - dqn.py
   - functional.py
   - ppo.py
   - redq.py
   - reinforce.py
   - sac.py
   - utils.py
   - value
      - advantages.py
      - functional.py
      - pg.py
      - utils.py
      - vtrace.py
- record
      - recorder.py
- trainers
   - loggers
   - trainers.py
   - helpers
      - collectors.py
      - envs.py
      - loggers.py
      - losses.py
      - models.py
      - replay_buffer.py
      - trainers.py

Unlike other domains, RL is less about media than *algorithms*. As such, it
is harder to make truly independent components.

What TorchRL is not:
- a collection of algorithms: we do not intend to provide SOTA implementations of RL algorithms,
  but we provide these algorithms only as examples of how to use the library.
- a research framework

TorchRL has very few core dependencies, mostly PyTorch and functorch. All
other dependencies (gym, torchvision, wandb / tensorboard) are optional.

Data
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

TensorDict
------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 115-119

.. code-block:: default


    import torch
    from tensordict import TensorDict








.. GENERATED FROM PYTHON SOURCE LINES 120-121

Let's create a TensorDict.

.. GENERATED FROM PYTHON SOURCE LINES 121-132

.. code-block:: default


    batch_size = 5
    tensordict = TensorDict(
        source={
            "key 1": torch.zeros(batch_size, 3),
            "key 2": torch.zeros(batch_size, 5, 6, dtype=torch.bool),
        },
        batch_size=[batch_size],
    )
    print(tensordict)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    TensorDict(
        fields={
            key 1: Tensor(torch.Size([5, 3]), dtype=torch.float32),
            key 2: Tensor(torch.Size([5, 5, 6]), dtype=torch.bool)},
        batch_size=torch.Size([5]),
        device=None,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 133-134

You can index a TensorDict as well as query keys.

.. GENERATED FROM PYTHON SOURCE LINES 134-138

.. code-block:: default


    print(tensordict[2])
    print(tensordict["key 1"] is tensordict.get("key 1"))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    TensorDict(
        fields={
            key 1: Tensor(torch.Size([3]), dtype=torch.float32),
            key 2: Tensor(torch.Size([5, 6]), dtype=torch.bool)},
        batch_size=torch.Size([]),
        device=None,
        is_shared=False)
    True




.. GENERATED FROM PYTHON SOURCE LINES 139-140

The following shows how to stack multiple TensorDicts.

.. GENERATED FROM PYTHON SOURCE LINES 140-160

.. code-block:: default


    tensordict1 = TensorDict(
        source={
            "key 1": torch.zeros(batch_size, 1),
            "key 2": torch.zeros(batch_size, 5, 6, dtype=torch.bool),
        },
        batch_size=[batch_size],
    )

    tensordict2 = TensorDict(
        source={
            "key 1": torch.ones(batch_size, 1),
            "key 2": torch.ones(batch_size, 5, 6, dtype=torch.bool),
        },
        batch_size=[batch_size],
    )

    tensordict = torch.stack([tensordict1, tensordict2], 0)
    tensordict.batch_size, tensordict["key 1"]





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    (torch.Size([2, 5]), tensor([[[0.],
             [0.],
             [0.],
             [0.],
             [0.]],

            [[1.],
             [1.],
             [1.],
             [1.],
             [1.]]]))



.. GENERATED FROM PYTHON SOURCE LINES 161-162

Here are some other functionalities of TensorDict.

.. GENERATED FROM PYTHON SOURCE LINES 162-185

.. code-block:: default


    print(
        "view(-1): ", tensordict.view(-1).batch_size, tensordict.view(-1).get("key 1").shape
    )

    print("to device: ", tensordict.to("cpu"))

    # print("pin_memory: ", tensordict.pin_memory())

    print("share memory: ", tensordict.share_memory_())

    print(
        "permute(1, 0): ",
        tensordict.permute(1, 0).batch_size,
        tensordict.permute(1, 0).get("key 1").shape,
    )

    print(
        "expand: ",
        tensordict.expand(3, *tensordict.batch_size).batch_size,
        tensordict.expand(3, *tensordict.batch_size).get("key 1").shape,
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    view(-1):  torch.Size([10]) torch.Size([10, 1])
    to device:  TensorDict(
        fields={
            key 1: Tensor(torch.Size([2, 5, 1]), dtype=torch.float32),
            key 2: Tensor(torch.Size([2, 5, 5, 6]), dtype=torch.bool)},
        batch_size=torch.Size([2, 5]),
        device=cpu,
        is_shared=False)
    share memory:  LazyStackedTensorDict(
        fields={
            key 1: Tensor(torch.Size([2, 5, 1]), dtype=torch.float32),
            key 2: Tensor(torch.Size([2, 5, 5, 6]), dtype=torch.bool)},
        batch_size=torch.Size([2, 5]),
        device=None,
        is_shared=True)
    permute(1, 0):  torch.Size([5, 2]) torch.Size([5, 2, 1])
    expand:  torch.Size([3, 2, 5]) torch.Size([3, 2, 5, 1])




.. GENERATED FROM PYTHON SOURCE LINES 186-187

You can create a **nested TensorDict** as well.

.. GENERATED FROM PYTHON SOURCE LINES 187-200

.. code-block:: default


    tensordict = TensorDict(
        source={
            "key 1": torch.zeros(batch_size, 3),
            "key 2": TensorDict(
                source={"sub-key 1": torch.zeros(batch_size, 2, 1)},
                batch_size=[batch_size, 2],
            ),
        },
        batch_size=[batch_size],
    )
    tensordict





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    TensorDict(
        fields={
            key 1: Tensor(torch.Size([5, 3]), dtype=torch.float32),
            key 2: TensorDict(
                fields={
                    sub-key 1: Tensor(torch.Size([5, 2, 1]), dtype=torch.float32)},
                batch_size=torch.Size([5, 2]),
                device=None,
                is_shared=False)},
        batch_size=torch.Size([5]),
        device=None,
        is_shared=False)



.. GENERATED FROM PYTHON SOURCE LINES 201-203

Replay buffers
------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 203-206

.. code-block:: default


    from torchrl.data import PrioritizedReplayBuffer, ReplayBuffer








.. GENERATED FROM PYTHON SOURCE LINES 207-212

.. code-block:: default


    rb = ReplayBuffer(100, collate_fn=lambda x: x)
    rb.add(1)
    rb.sample(1)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    [1]



.. GENERATED FROM PYTHON SOURCE LINES 213-217

.. code-block:: default


    rb.extend([2, 3])
    rb.sample(3)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    [3, 2, 2]



.. GENERATED FROM PYTHON SOURCE LINES 218-224

.. code-block:: default


    rb = PrioritizedReplayBuffer(100, alpha=0.7, beta=1.1, collate_fn=lambda x: x)
    rb.add(1)
    rb.sample(1)
    rb.update_priority(1, 0.5)








.. GENERATED FROM PYTHON SOURCE LINES 225-226

Here are examples of using a replaybuffer with tensordicts.

.. GENERATED FROM PYTHON SOURCE LINES 226-232

.. code-block:: default


    collate_fn = torch.stack
    rb = ReplayBuffer(100, collate_fn=collate_fn)
    rb.add(TensorDict({"a": torch.randn(3)}, batch_size=[]))
    len(rb)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    1



.. GENERATED FROM PYTHON SOURCE LINES 233-239

.. code-block:: default


    rb.extend(TensorDict({"a": torch.randn(2, 3)}, batch_size=[2]))
    print(len(rb))
    print(rb.sample(10))
    print(rb.sample(2).contiguous())





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    3
    LazyStackedTensorDict(
        fields={
            a: Tensor(torch.Size([10, 3]), dtype=torch.float32)},
        batch_size=torch.Size([10]),
        device=None,
        is_shared=False)
    TensorDict(
        fields={
            a: Tensor(torch.Size([2, 3]), dtype=torch.float32)},
        batch_size=torch.Size([2]),
        device=None,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 240-251

.. code-block:: default


    torch.manual_seed(0)
    from torchrl.data import TensorDictPrioritizedReplayBuffer

    rb = TensorDictPrioritizedReplayBuffer(
        100, alpha=0.7, beta=1.1, priority_key="td_error"
    )
    rb.extend(TensorDict({"a": torch.randn(2, 3)}, batch_size=[2]))
    tensordict_sample = rb.sample(2).contiguous()
    tensordict_sample





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    TensorDict(
        fields={
            a: Tensor(torch.Size([2, 3]), dtype=torch.float32),
            index: Tensor(torch.Size([2, 1]), dtype=torch.int32)},
        batch_size=torch.Size([2]),
        device=None,
        is_shared=False)



.. GENERATED FROM PYTHON SOURCE LINES 252-255

.. code-block:: default


    tensordict_sample["index"]





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    tensor([[1],
            [0]], dtype=torch.int32)



.. GENERATED FROM PYTHON SOURCE LINES 256-267

.. code-block:: default


    tensordict_sample["td_error"] = torch.rand(2)
    rb.update_priority(tensordict_sample)

    for i, val in enumerate(rb._sum_tree):
        print(i, val)
        if i == len(rb):
            break

    import gym





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    0 0.28791671991348267
    1 0.06984968483448029
    2 0.0




.. GENERATED FROM PYTHON SOURCE LINES 268-270

Envs
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. GENERATED FROM PYTHON SOURCE LINES 270-277

.. code-block:: default


    from torchrl.envs.libs.gym import GymEnv, GymWrapper

    gym_env = gym.make("Pendulum-v1")
    env = GymWrapper(gym_env)
    env = GymEnv("Pendulum-v1")








.. GENERATED FROM PYTHON SOURCE LINES 278-282

.. code-block:: default


    tensordict = env.reset()
    env.rand_step(tensordict)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    TensorDict(
        fields={
            action: Tensor(torch.Size([1]), dtype=torch.float32),
            done: Tensor(torch.Size([1]), dtype=torch.bool),
            next: TensorDict(
                fields={
                    observation: Tensor(torch.Size([3]), dtype=torch.float32)},
                batch_size=torch.Size([]),
                device=cpu,
                is_shared=False),
            observation: Tensor(torch.Size([3]), dtype=torch.float32),
            reward: Tensor(torch.Size([1]), dtype=torch.float32)},
        batch_size=torch.Size([]),
        device=cpu,
        is_shared=False)



.. GENERATED FROM PYTHON SOURCE LINES 283-285

Changing environments config
------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 285-289

.. code-block:: default


    env = GymEnv("Pendulum-v1", frame_skip=3, from_pixels=True, pixels_only=False)
    env.reset()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    TensorDict(
        fields={
            done: Tensor(torch.Size([1]), dtype=torch.bool),
            pixels: Tensor(torch.Size([500, 500, 3]), dtype=torch.uint8),
            state: Tensor(torch.Size([3]), dtype=torch.float32)},
        batch_size=torch.Size([]),
        device=cpu,
        is_shared=False)



.. GENERATED FROM PYTHON SOURCE LINES 290-294

.. code-block:: default


    env.close()
    del env








.. GENERATED FROM PYTHON SOURCE LINES 295-308

.. code-block:: default


    from torchrl.envs import (
        Compose,
        NoopResetEnv,
        ObservationNorm,
        ToTensorImage,
        TransformedEnv,
    )

    base_env = GymEnv("Pendulum-v1", frame_skip=3, from_pixels=True, pixels_only=False)
    env = TransformedEnv(base_env, Compose(NoopResetEnv(3), ToTensorImage()))
    env.append_transform(ObservationNorm(in_keys=["pixels"], loc=2, scale=1))








.. GENERATED FROM PYTHON SOURCE LINES 309-311

Transforms
------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 311-324

.. code-block:: default


    from torchrl.envs import (
        Compose,
        NoopResetEnv,
        ObservationNorm,
        ToTensorImage,
        TransformedEnv,
    )

    base_env = GymEnv("Pendulum-v1", frame_skip=3, from_pixels=True, pixels_only=False)
    env = TransformedEnv(base_env, Compose(NoopResetEnv(3), ToTensorImage()))
    env.append_transform(ObservationNorm(in_keys=["pixels"], loc=2, scale=1))








.. GENERATED FROM PYTHON SOURCE LINES 325-328

.. code-block:: default


    env.reset()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    TensorDict(
        fields={
            pixels: Tensor(torch.Size([3, 500, 500]), dtype=torch.float32),
            state: Tensor(torch.Size([3]), dtype=torch.float32)},
        batch_size=torch.Size([]),
        device=cpu,
        is_shared=False)



.. GENERATED FROM PYTHON SOURCE LINES 329-333

.. code-block:: default


    print("env: ", env)
    print("last transform parent: ", env.transform[2].parent)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    env:  TransformedEnv(
        env=GymEnv(env=Pendulum-v1, batch_size=torch.Size([]), device=cpu),
        transform=Compose(
                NoopResetEnv(noops=3, random=True),
                ToTensorImage(keys=['pixels']),
                ObservationNorm(loc=2.0000, scale=1.0000, keys=['pixels'])))
    last transform parent:  TransformedEnv(
        env=GymEnv(env=Pendulum-v1, batch_size=torch.Size([]), device=cpu),
        transform=Compose(
                NoopResetEnv(noops=3, random=True),
                ToTensorImage(keys=['pixels'])))




.. GENERATED FROM PYTHON SOURCE LINES 334-336

Vectorized Environments
------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 336-348

.. code-block:: default


    from torchrl.envs import ParallelEnv

    base_env = ParallelEnv(
        4, lambda: GymEnv("Pendulum-v1", frame_skip=3, from_pixels=True, pixels_only=False)
    )
    env = TransformedEnv(
        base_env, Compose(NoopResetEnv(3), ToTensorImage())
    )  # applies transforms on batch of envs
    env.append_transform(ObservationNorm(in_keys=["pixels"], loc=2, scale=1))
    env.reset()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    TensorDict(
        fields={
            pixels: Tensor(torch.Size([4, 3, 500, 500]), dtype=torch.float32),
            state: Tensor(torch.Size([4, 3]), dtype=torch.float32)},
        batch_size=torch.Size([4]),
        device=cpu,
        is_shared=False)



.. GENERATED FROM PYTHON SOURCE LINES 349-352

.. code-block:: default


    env.action_spec





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    NdBoundedTensorSpec(
         shape=torch.Size([1]), space=ContinuousBox(minimum=tensor([-2.]), maximum=tensor([2.])), device=cpu, dtype=torch.float32, domain=continuous)



.. GENERATED FROM PYTHON SOURCE LINES 353-360

Modules
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Models
------------------------------

Example of a MLP model:

.. GENERATED FROM PYTHON SOURCE LINES 360-363

.. code-block:: default


    from torch import nn








.. GENERATED FROM PYTHON SOURCE LINES 364-372

.. code-block:: default


    from torchrl.modules import ConvNet, MLP
    from torchrl.modules.models.utils import SquashDims

    net = MLP(num_cells=[32, 64], out_features=4, activation_class=nn.ELU)
    print(net)
    print(net(torch.randn(10, 3)).shape)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    MLP(
      (0): LazyLinear(in_features=0, out_features=32, bias=True)
      (1): ELU(alpha=1.0)
      (2): Linear(in_features=32, out_features=64, bias=True)
      (3): ELU(alpha=1.0)
      (4): Linear(in_features=64, out_features=4, bias=True)
    )
    torch.Size([10, 4])




.. GENERATED FROM PYTHON SOURCE LINES 373-374

Example of a CNN model:

.. GENERATED FROM PYTHON SOURCE LINES 374-381

.. code-block:: default


    cnn = ConvNet(
        num_cells=[32, 64], kernel_sizes=[8, 4], strides=[2, 1], aggregator_class=SquashDims
    )
    print(cnn)
    print(cnn(torch.randn(10, 3, 32, 32)).shape)  # last tensor is squashed





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ConvNet(
      (0): LazyConv2d(0, 32, kernel_size=(8, 8), stride=(2, 2))
      (1): ELU(alpha=1.0)
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(1, 1))
      (3): ELU(alpha=1.0)
      (4): SquashDims()
    )
    torch.Size([10, 6400])




.. GENERATED FROM PYTHON SOURCE LINES 382-384

TensorDictModules
------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 384-393

.. code-block:: default


    from torchrl.modules import TensorDictModule

    tensordict = TensorDict({"key 1": torch.randn(10, 3)}, batch_size=[10])
    module = nn.Linear(3, 4)
    td_module = TensorDictModule(module, in_keys=["key 1"], out_keys=["key 2"])
    td_module(tensordict)
    print(tensordict)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    TensorDict(
        fields={
            key 1: Tensor(torch.Size([10, 3]), dtype=torch.float32),
            key 2: Tensor(torch.Size([10, 4]), dtype=torch.float32)},
        batch_size=torch.Size([10]),
        device=None,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 394-396

Sequences of Modules
------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 396-411

.. code-block:: default


    from torchrl.modules import TensorDictSequential

    backbone_module = nn.Linear(5, 3)
    backbone = TensorDictModule(
        backbone_module, in_keys=["observation"], out_keys=["hidden"]
    )
    actor_module = nn.Linear(3, 4)
    actor = TensorDictModule(actor_module, in_keys=["hidden"], out_keys=["action"])
    value_module = MLP(out_features=1, num_cells=[4, 5])
    value = TensorDictModule(value_module, in_keys=["hidden", "action"], out_keys=["value"])

    sequence = TensorDictSequential(backbone, actor, value)
    print(sequence)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    TensorDictSequential(
        module=ModuleList(
          (0): TensorDictModule(
              module=Linear(in_features=5, out_features=3, bias=True), 
              device=cpu, 
              in_keys=['observation'], 
              out_keys=['hidden'])
          (1): TensorDictModule(
              module=Linear(in_features=3, out_features=4, bias=True), 
              device=cpu, 
              in_keys=['hidden'], 
              out_keys=['action'])
          (2): TensorDictModule(
              module=MLP(
                (0): LazyLinear(in_features=0, out_features=4, bias=True)
                (1): Tanh()
                (2): Linear(in_features=4, out_features=5, bias=True)
                (3): Tanh()
                (4): Linear(in_features=5, out_features=1, bias=True)
              ), 
              device=cpu, 
              in_keys=['hidden', 'action'], 
              out_keys=['value'])
        ), 
        device=cpu, 
        in_keys=['observation'], 
        out_keys=['hidden', 'action', 'value'])




.. GENERATED FROM PYTHON SOURCE LINES 412-415

.. code-block:: default


    print(sequence.in_keys, sequence.out_keys)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ['observation'] ['hidden', 'action', 'value']




.. GENERATED FROM PYTHON SOURCE LINES 416-425

.. code-block:: default


    tensordict = TensorDict(
        {"observation": torch.randn(3, 5)},
        [3],
    )
    backbone(tensordict)
    actor(tensordict)
    value(tensordict)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    TensorDict(
        fields={
            action: Tensor(torch.Size([3, 4]), dtype=torch.float32),
            hidden: Tensor(torch.Size([3, 3]), dtype=torch.float32),
            observation: Tensor(torch.Size([3, 5]), dtype=torch.float32),
            value: Tensor(torch.Size([3, 1]), dtype=torch.float32)},
        batch_size=torch.Size([3]),
        device=None,
        is_shared=False)



.. GENERATED FROM PYTHON SOURCE LINES 426-434

.. code-block:: default


    tensordict = TensorDict(
        {"observation": torch.randn(3, 5)},
        [3],
    )
    sequence(tensordict)
    print(tensordict)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    TensorDict(
        fields={
            action: Tensor(torch.Size([3, 4]), dtype=torch.float32),
            hidden: Tensor(torch.Size([3, 3]), dtype=torch.float32),
            observation: Tensor(torch.Size([3, 5]), dtype=torch.float32),
            value: Tensor(torch.Size([3, 1]), dtype=torch.float32)},
        batch_size=torch.Size([3]),
        device=None,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 435-437

Functional Programming (Ensembling / Meta-RL)
----------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 437-441

.. code-block:: default


    fsequence, (params, buffers) = sequence.make_functional_with_buffers()
    len(list(fsequence.parameters()))  # functional modules have no parameters





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    0



.. GENERATED FROM PYTHON SOURCE LINES 442-445

.. code-block:: default


    fsequence(tensordict, params=params, buffers=buffers)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    TensorDict(
        fields={
            action: Tensor(torch.Size([3, 4]), dtype=torch.float32),
            hidden: Tensor(torch.Size([3, 3]), dtype=torch.float32),
            observation: Tensor(torch.Size([3, 5]), dtype=torch.float32),
            value: Tensor(torch.Size([3, 1]), dtype=torch.float32)},
        batch_size=torch.Size([3]),
        device=None,
        is_shared=False)



.. GENERATED FROM PYTHON SOURCE LINES 446-454

.. code-block:: default


    params_expand = [p.expand(4, *p.shape) for p in params]
    buffers_expand = [b.expand(4, *b.shape) for b in buffers]
    tensordict_exp = fsequence(
        tensordict, params=params_expand, buffers=buffers, vmap=(0, 0, None)
    )
    print(tensordict_exp)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    TensorDict(
        fields={
            action: Tensor(torch.Size([4, 3, 4]), dtype=torch.float32),
            hidden: Tensor(torch.Size([4, 3, 3]), dtype=torch.float32),
            observation: Tensor(torch.Size([4, 3, 5]), dtype=torch.float32),
            value: Tensor(torch.Size([4, 3, 1]), dtype=torch.float32)},
        batch_size=torch.Size([4, 3]),
        device=None,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 455-457

Specialized Classes
------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 457-469

.. code-block:: default


    torch.manual_seed(0)
    from torchrl.data import NdBoundedTensorSpec

    spec = NdBoundedTensorSpec(-torch.ones(3), torch.ones(3))
    base_module = nn.Linear(5, 3)
    module = TensorDictModule(
        module=base_module, spec=spec, in_keys=["obs"], out_keys=["action"], safe=True
    )
    tensordict = TensorDict({"obs": torch.randn(5)}, batch_size=[])
    module(tensordict)["action"]





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    tensor([-0.0137,  0.1524, -0.0641], grad_fn=<AddBackward0>)



.. GENERATED FROM PYTHON SOURCE LINES 470-474

.. code-block:: default


    tensordict = TensorDict({"obs": torch.randn(5) * 100}, batch_size=[])
    module(tensordict)["action"]  # safe=True projects the result within the set





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    tensor([-1.,  1., -1.], grad_fn=<IndexPutBackward0>)



.. GENERATED FROM PYTHON SOURCE LINES 475-483

.. code-block:: default


    from torchrl.modules import Actor

    base_module = nn.Linear(5, 3)
    actor = Actor(base_module, in_keys=["obs"])
    tensordict = TensorDict({"obs": torch.randn(5)}, batch_size=[])
    actor(tensordict)  # action is the default value





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    TensorDict(
        fields={
            action: Tensor(torch.Size([3]), dtype=torch.float32),
            obs: Tensor(torch.Size([5]), dtype=torch.float32)},
        batch_size=torch.Size([]),
        device=None,
        is_shared=False)



.. GENERATED FROM PYTHON SOURCE LINES 484-510

.. code-block:: default


    # Probabilistic modules
    from torchrl.modules import (
        NormalParamWrapper,
        ProbabilisticTensorDictModule,
        TanhNormal,
    )

    td = TensorDict(
        {"input": torch.randn(3, 5)},
        [
            3,
        ],
    )
    net = NormalParamWrapper(nn.Linear(5, 4))  # splits the output in loc and scale
    module = TensorDictModule(net, in_keys=["input"], out_keys=["loc", "scale"])
    td_module = ProbabilisticTensorDictModule(
        module=module,
        dist_in_keys=["loc", "scale"],
        sample_out_key=["action"],
        distribution_class=TanhNormal,
        return_log_prob=False,
    )
    td_module(td)
    print(td)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    TensorDict(
        fields={
            action: Tensor(torch.Size([3, 2]), dtype=torch.float32),
            input: Tensor(torch.Size([3, 5]), dtype=torch.float32),
            loc: Tensor(torch.Size([3, 2]), dtype=torch.float32),
            scale: Tensor(torch.Size([3, 2]), dtype=torch.float32)},
        batch_size=torch.Size([3]),
        device=None,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 511-529

.. code-block:: default


    # returning the log-probability
    td = TensorDict(
        {"input": torch.randn(3, 5)},
        [
            3,
        ],
    )
    td_module = ProbabilisticTensorDictModule(
        module=module,
        dist_in_keys=["loc", "scale"],
        sample_out_key=["action"],
        distribution_class=TanhNormal,
        return_log_prob=True,
    )
    td_module(td)
    print(td)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    TensorDict(
        fields={
            action: Tensor(torch.Size([3, 2]), dtype=torch.float32),
            input: Tensor(torch.Size([3, 5]), dtype=torch.float32),
            loc: Tensor(torch.Size([3, 2]), dtype=torch.float32),
            sample_log_prob: Tensor(torch.Size([3, 1]), dtype=torch.float32),
            scale: Tensor(torch.Size([3, 2]), dtype=torch.float32)},
        batch_size=torch.Size([3]),
        device=None,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 530-554

.. code-block:: default


    # Sampling vs mode / mean
    from torchrl.envs.utils import set_exploration_mode

    td = TensorDict(
        {"input": torch.randn(3, 5)},
        [
            3,
        ],
    )

    torch.manual_seed(0)
    with set_exploration_mode("random"):
        td_module(td)
        print("random:", td["action"])

    with set_exploration_mode("mode"):
        td_module(td)
        print("mode:", td["action"])

    with set_exploration_mode("mean"):
        td_module(td)
        print("mean:", td["action"])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    random: tensor([[ 0.8728, -0.1335],
            [-0.9833,  0.3497],
            [-0.6889, -0.6433]], grad_fn=<ClampBackward1>)
    mode: tensor([[-0.1131,  0.1761],
            [-0.3425, -0.2665],
            [ 0.2915,  0.6207]], grad_fn=<ClampBackward1>)
    mean: tensor([[-0.1131,  0.1441],
            [-0.2375, -0.1242],
            [ 0.1372,  0.3810]], grad_fn=<MeanBackward1>)




.. GENERATED FROM PYTHON SOURCE LINES 555-557

Using Environments and Modules
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. GENERATED FROM PYTHON SOURCE LINES 557-585

.. code-block:: default


    from torchrl.envs.utils import step_mdp

    env = GymEnv("Pendulum-v1")

    action_spec = env.action_spec
    actor_module = nn.Linear(3, 1)
    actor = TensorDictModule(
        actor_module, spec=action_spec, in_keys=["observation"], out_keys=["action"]
    )

    torch.manual_seed(0)
    env.set_seed(0)

    max_steps = 100
    tensordict = env.reset()
    tensordicts = TensorDict({}, [max_steps])
    for i in range(max_steps):
        actor(tensordict)
        tensordicts[i] = env.step(tensordict)
        tensordict = step_mdp(tensordict)  # roughly equivalent to obs = next_obs
        if env.is_done:
            break

    tensordicts_prealloc = tensordicts.clone()
    print("total steps:", i)
    print(tensordicts)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    total steps: 99
    TensorDict(
        fields={
            action: Tensor(torch.Size([100, 1]), dtype=torch.float32),
            done: Tensor(torch.Size([100, 1]), dtype=torch.bool),
            next: TensorDict(
                fields={
                    observation: Tensor(torch.Size([100, 3]), dtype=torch.float32)},
                batch_size=torch.Size([100]),
                device=None,
                is_shared=False),
            observation: Tensor(torch.Size([100, 3]), dtype=torch.float32),
            reward: Tensor(torch.Size([100, 1]), dtype=torch.float32)},
        batch_size=torch.Size([100]),
        device=None,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 586-604

.. code-block:: default


    # equivalent
    torch.manual_seed(0)
    env.set_seed(0)

    max_steps = 100
    tensordict = env.reset()
    tensordicts = []
    for _ in range(max_steps):
        actor(tensordict)
        tensordicts.append(env.step(tensordict))
        tensordict = step_mdp(tensordict)  # roughly equivalent to obs = next_obs
        if env.is_done:
            break
    tensordicts_stack = torch.stack(tensordicts, 0)
    print("total steps:", i)
    print(tensordicts_stack)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    total steps: 99
    LazyStackedTensorDict(
        fields={
            action: Tensor(torch.Size([100, 1]), dtype=torch.float32),
            done: Tensor(torch.Size([100, 1]), dtype=torch.bool),
            next: LazyStackedTensorDict(
                fields={
                    observation: Tensor(torch.Size([100, 3]), dtype=torch.float32)},
                batch_size=torch.Size([100]),
                device=cpu,
                is_shared=False),
            observation: Tensor(torch.Size([100, 3]), dtype=torch.float32),
            reward: Tensor(torch.Size([100, 1]), dtype=torch.float32)},
        batch_size=torch.Size([100]),
        device=cpu,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 605-608

.. code-block:: default


    (tensordicts_stack == tensordicts_prealloc).all()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    True



.. GENERATED FROM PYTHON SOURCE LINES 609-616

.. code-block:: default


    # helper
    torch.manual_seed(0)
    env.set_seed(0)
    tensordict_rollout = env.rollout(policy=actor, max_steps=max_steps)
    tensordict_rollout





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    TensorDict(
        fields={
            action: Tensor(torch.Size([100, 1]), dtype=torch.float32),
            done: Tensor(torch.Size([100, 1]), dtype=torch.bool),
            next: LazyStackedTensorDict(
                fields={
                    observation: Tensor(torch.Size([100, 3]), dtype=torch.float32)},
                batch_size=torch.Size([100]),
                device=cpu,
                is_shared=False),
            observation: Tensor(torch.Size([100, 3]), dtype=torch.float32),
            reward: Tensor(torch.Size([100, 1]), dtype=torch.float32)},
        batch_size=torch.Size([100]),
        device=cpu,
        is_shared=False)



.. GENERATED FROM PYTHON SOURCE LINES 617-625

.. code-block:: default


    (tensordict_rollout == tensordicts_prealloc).all()

    # Collectors
    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^

    from torchrl.collectors import MultiaSyncDataCollector, MultiSyncDataCollector








.. GENERATED FROM PYTHON SOURCE LINES 626-651

.. code-block:: default


    from torchrl.envs import EnvCreator, ParallelEnv
    from torchrl.envs.libs.gym import GymEnv
    from torchrl.modules import TensorDictModule

    # EnvCreator makes sure that we can send a lambda function from process to process
    parallel_env = ParallelEnv(3, EnvCreator(lambda: GymEnv("Pendulum-v1")))
    create_env_fn = [parallel_env, parallel_env]

    actor_module = nn.Linear(3, 1)
    actor = TensorDictModule(actor_module, in_keys=["observation"], out_keys=["action"])

    # Sync data collector
    devices = ["cpu", "cpu"]

    collector = MultiSyncDataCollector(
        create_env_fn=create_env_fn,  # either a list of functions or a ParallelEnv
        policy=actor,
        total_frames=240,
        max_frames_per_traj=-1,  # envs are terminating, we don't need to stop them early
        frames_per_batch=60,  # we want 60 frames at a time (we have 3 envs per sub-collector)
        passing_devices=devices,  # len must match len of env created
        devices=devices,
    )








.. GENERATED FROM PYTHON SOURCE LINES 652-659

.. code-block:: default


    for i, d in enumerate(collector):
        if i == 0:
            print(d)  # trajectories are split automatically in [6 workers x 10 steps]
        collector.update_policy_weights_()  # make sure that our policies have the latest weights if working on multiple devices
    print(i)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    TensorDict(
        fields={
            action: Tensor(torch.Size([6, 10, 1]), dtype=torch.float32),
            done: Tensor(torch.Size([6, 10, 1]), dtype=torch.bool),
            mask: Tensor(torch.Size([6, 10, 1]), dtype=torch.bool),
            next: TensorDict(
                fields={
                    observation: Tensor(torch.Size([6, 10, 3]), dtype=torch.float32)},
                batch_size=torch.Size([6, 10]),
                device=None,
                is_shared=False),
            observation: Tensor(torch.Size([6, 10, 3]), dtype=torch.float32),
            reward: Tensor(torch.Size([6, 10, 1]), dtype=torch.float32),
            step_count: Tensor(torch.Size([6, 10, 1]), dtype=torch.float32),
            traj_ids: Tensor(torch.Size([6, 10, 1]), dtype=torch.float32)},
        batch_size=torch.Size([6, 10]),
        device=None,
        is_shared=False)
    3




.. GENERATED FROM PYTHON SOURCE LINES 660-679

.. code-block:: default


    # async data collector: keeps working while you update your model
    collector = MultiaSyncDataCollector(
        create_env_fn=create_env_fn,  # either a list of functions or a ParallelEnv
        policy=actor,
        total_frames=240,
        max_frames_per_traj=-1,  # envs are terminating, we don't need to stop them early
        frames_per_batch=60,  # we want 60 frames at a time (we have 3 envs per sub-collector)
        passing_devices=devices,  # len must match len of env created
        devices=devices,
    )

    for i, d in enumerate(collector):
        if i == 0:
            print(d)  # trajectories are split automatically in [6 workers x 10 steps]
        collector.update_policy_weights_()  # make sure that our policies have the latest weights if working on multiple devices
    print(i)
    del collector





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    TensorDict(
        fields={
            action: Tensor(torch.Size([3, 20, 1]), dtype=torch.float32),
            done: Tensor(torch.Size([3, 20, 1]), dtype=torch.bool),
            mask: Tensor(torch.Size([3, 20, 1]), dtype=torch.bool),
            next: TensorDict(
                fields={
                    observation: Tensor(torch.Size([3, 20, 3]), dtype=torch.float32)},
                batch_size=torch.Size([3, 20]),
                device=cpu,
                is_shared=False),
            observation: Tensor(torch.Size([3, 20, 3]), dtype=torch.float32),
            reward: Tensor(torch.Size([3, 20, 1]), dtype=torch.float32),
            step_count: Tensor(torch.Size([3, 20, 1]), dtype=torch.float32),
            traj_ids: Tensor(torch.Size([3, 20, 1]), dtype=torch.float32)},
        batch_size=torch.Size([3, 20]),
        device=cpu,
        is_shared=False)
    3




.. GENERATED FROM PYTHON SOURCE LINES 680-682

Objectives
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. GENERATED FROM PYTHON SOURCE LINES 682-703

.. code-block:: default


    # TorchRL delivers meta-RL compatible loss functions
    # Disclaimer: This APi may change in the future
    from torchrl.objectives import DDPGLoss

    actor_module = nn.Linear(3, 1)
    actor = TensorDictModule(actor_module, in_keys=["observation"], out_keys=["action"])


    class ConcatModule(nn.Linear):
        def forward(self, obs, action):
            return super().forward(torch.cat([obs, action], -1))


    value_module = ConcatModule(4, 1)
    value = TensorDictModule(
        value_module, in_keys=["observation", "action"], out_keys=["state_action_value"]
    )

    loss_fn = DDPGLoss(actor, value, gamma=0.99)








.. GENERATED FROM PYTHON SOURCE LINES 704-718

.. code-block:: default


    tensordict = TensorDict(
        {
            "observation": torch.randn(10, 3),
            "next": {"observation": torch.randn(10, 3)},
            "reward": torch.randn(10, 1),
            "action": torch.randn(10, 1),
            "done": torch.zeros(10, 1, dtype=torch.bool),
        },
        batch_size=[10],
        device="cpu",
    )
    loss_td = loss_fn(tensordict)








.. GENERATED FROM PYTHON SOURCE LINES 719-722

.. code-block:: default


    loss_td





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    TensorDict(
        fields={
            loss_actor: Tensor(torch.Size([1]), dtype=torch.float32),
            loss_value: Tensor(torch.Size([1]), dtype=torch.float32),
            pred_value: Tensor(torch.Size([1]), dtype=torch.float32),
            pred_value_max: Tensor(torch.Size([1]), dtype=torch.float32),
            target_value: Tensor(torch.Size([1]), dtype=torch.float32),
            target_value_max: Tensor(torch.Size([1]), dtype=torch.float32)},
        batch_size=torch.Size([]),
        device=None,
        is_shared=False)



.. GENERATED FROM PYTHON SOURCE LINES 723-726

.. code-block:: default


    tensordict





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    TensorDict(
        fields={
            action: Tensor(torch.Size([10, 1]), dtype=torch.float32),
            done: Tensor(torch.Size([10, 1]), dtype=torch.bool),
            next: TensorDict(
                fields={
                    observation: Tensor(torch.Size([10, 3]), dtype=torch.float32)},
                batch_size=torch.Size([10]),
                device=cpu,
                is_shared=False),
            observation: Tensor(torch.Size([10, 3]), dtype=torch.float32),
            reward: Tensor(torch.Size([10, 1]), dtype=torch.float32),
            td_error: Tensor(torch.Size([10, 1]), dtype=torch.float32)},
        batch_size=torch.Size([10]),
        device=cpu,
        is_shared=False)



.. GENERATED FROM PYTHON SOURCE LINES 727-746

State of the Library
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

TorchRL is currently an **alpha-release**: there may be bugs and there is no
guarantee about BC-breaking changes. We should be able to move to a beta-release
by the end of the year. Our roadmap to get there comprises:

- Distributed solutions
- Offline RL
- Greater support for meta-RL
- Multi-task and hierarchical RL

Contributing
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We are actively looking for contributors and early users. If you're working in
RL (or just curious), try it! Give us feedback: what will make the success of
TorchRL is how well it covers researchers needs. To do that, we need their input!
Since the library is nascent, it is a great time for you to shape it the way you want!

.. GENERATED FROM PYTHON SOURCE LINES 748-752

Installing the Library
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The library is on PyPI: *pip install torchrl*


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  38.307 seconds)


.. _sphx_glr_download_tutorials_torchrl_demo.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: torchrl_demo.py <torchrl_demo.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: torchrl_demo.ipynb <torchrl_demo.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
