
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorials/pretrained_models.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_tutorials_pretrained_models.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorials_pretrained_models.py:


Using pretrained models
=======================
This tutorial explains how to use pretrained models in TorchRL.

.. GENERATED FROM PYTHON SOURCE LINES 8-16

At the end of this tutorial, you will be capable of using pretrained models
for efficient image representation, and fine-tune them.

TorchRL provides pretrained models that are to be used either as transforms or as
components of the policy. As the sematic is the same, they can be used interchangeably
in one or the other context. In this tutorial, we will be using R3M (https://arxiv.org/abs/2203.12601),
but other models (e.g. VIP) will work equally well.


.. GENERATED FROM PYTHON SOURCE LINES 16-25

.. code-block:: default

    import torch.cuda
    from tensordict.nn import TensorDictSequential
    from torch import nn
    from torchrl.envs import R3MTransform, TransformedEnv
    from torchrl.envs.libs.gym import GymEnv
    from torchrl.modules import Actor

    device = "cuda:0" if torch.cuda.device_count() else "cpu"








.. GENERATED FROM PYTHON SOURCE LINES 26-30

Let us first create an environment. For the sake of simplicity, we will be using
a common gym environment. In practice, this will work in more challenging, embodied
AI contexts (e.g. have a look at our Habitat wrappers).


.. GENERATED FROM PYTHON SOURCE LINES 30-32

.. code-block:: default

    base_env = GymEnv("Ant-v4", from_pixels=True, device=device)








.. GENERATED FROM PYTHON SOURCE LINES 33-40

Let us fetch our pretrained model. We ask for the pretrained version of the model through the
download=True flag. By default this is turned off.
Next, we will append our transform to the environment. In practice, what will happen is that
each batch of data collected will go through the transform and be mapped on a "r3m_vec" entry
in the output tensordict. Our policy, consisting of a single layer MLP, will then read this vector and compute
the corresponding action.


.. GENERATED FROM PYTHON SOURCE LINES 40-47

.. code-block:: default

    r3m = R3MTransform("resnet50", in_keys=["pixels"], download=True).to(device)
    env_transformed = TransformedEnv(base_env, r3m)
    net = nn.Sequential(
        nn.LazyLinear(128), nn.Tanh(), nn.Linear(128, base_env.action_spec.shape[-1])
    )
    policy = Actor(net, in_keys=["r3m_vec"])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading: "https://pytorch.s3.amazonaws.com/models/rl/r3m/r3m_50.pt" to /github/home/.cache/torch/hub/checkpoints/r3m_50.pt
      0%|          | 0.00/374M [00:00<?, ?B/s]      0%|          | 56.0k/374M [00:00<15:32, 421kB/s]      0%|          | 160k/374M [00:00<10:34, 618kB/s]       0%|          | 688k/374M [00:00<03:07, 2.09MB/s]      1%|          | 2.70M/374M [00:00<00:53, 7.23MB/s]      2%|2         | 7.64M/374M [00:00<00:21, 17.9MB/s]      3%|3         | 12.0M/374M [00:00<00:16, 22.5MB/s]      4%|4         | 16.8M/374M [00:00<00:13, 26.9MB/s]      6%|5         | 22.1M/374M [00:01<00:12, 30.7MB/s]      7%|7         | 27.7M/374M [00:01<00:10, 34.1MB/s]      9%|8         | 33.3M/374M [00:01<00:09, 36.4MB/s]     10%|#         | 38.8M/374M [00:01<00:08, 39.5MB/s]     11%|#1        | 43.0M/374M [00:01<00:08, 40.6MB/s]     13%|#2        | 47.5M/374M [00:01<00:08, 39.1MB/s]     14%|#4        | 53.1M/374M [00:01<00:08, 39.9MB/s]     16%|#5        | 59.0M/374M [00:02<00:08, 41.2MB/s]     17%|#7        | 64.5M/374M [00:02<00:07, 41.1MB/s]     18%|#8        | 68.5M/374M [00:02<00:08, 36.7MB/s]     20%|#9        | 73.3M/374M [00:02<00:08, 36.5MB/s]     21%|##1       | 78.6M/374M [00:02<00:08, 37.6MB/s]     22%|##2       | 84.2M/374M [00:02<00:07, 38.6MB/s]     24%|##3       | 88.9M/374M [00:02<00:08, 36.6MB/s]     25%|##5       | 93.8M/374M [00:03<00:08, 36.4MB/s]     26%|##6       | 98.8M/374M [00:03<00:07, 36.6MB/s]     28%|##7       | 104M/374M [00:03<00:07, 37.9MB/s]      29%|##9       | 110M/374M [00:03<00:07, 38.5MB/s]     31%|###       | 115M/374M [00:03<00:06, 39.3MB/s]     32%|###2      | 121M/374M [00:03<00:06, 39.6MB/s]     34%|###3      | 126M/374M [00:03<00:06, 40.3MB/s]     35%|###5      | 132M/374M [00:04<00:06, 40.4MB/s]     37%|###6      | 137M/374M [00:04<00:06, 40.4MB/s]     38%|###8      | 143M/374M [00:04<00:05, 40.7MB/s]     40%|###9      | 149M/374M [00:04<00:05, 41.1MB/s]     41%|####1     | 154M/374M [00:04<00:06, 37.9MB/s]     42%|####2     | 158M/374M [00:04<00:06, 37.5MB/s]     43%|####3     | 163M/374M [00:04<00:06, 35.9MB/s]     45%|####4     | 167M/374M [00:05<00:06, 34.8MB/s]     46%|####5     | 172M/374M [00:05<00:06, 34.2MB/s]     47%|####7     | 176M/374M [00:05<00:06, 33.9MB/s]     48%|####8     | 180M/374M [00:05<00:06, 33.8MB/s]     49%|####9     | 185M/374M [00:05<00:05, 33.6MB/s]     51%|#####     | 189M/374M [00:05<00:05, 34.7MB/s]     52%|#####1    | 193M/374M [00:05<00:05, 34.8MB/s]     52%|#####2    | 196M/374M [00:05<00:05, 34.2MB/s]     53%|#####3    | 200M/374M [00:06<00:05, 34.5MB/s]     54%|#####4    | 203M/374M [00:06<00:05, 34.4MB/s]     55%|#####5    | 206M/374M [00:06<00:05, 34.3MB/s]     56%|#####6    | 210M/374M [00:06<00:04, 34.5MB/s]     57%|#####7    | 213M/374M [00:06<00:04, 34.4MB/s]     58%|#####8    | 217M/374M [00:06<00:04, 34.6MB/s]     59%|#####8    | 220M/374M [00:06<00:04, 34.4MB/s]     60%|#####9    | 224M/374M [00:06<00:04, 35.0MB/s]     61%|######    | 228M/374M [00:06<00:04, 34.8MB/s]     62%|######1   | 231M/374M [00:07<00:04, 35.3MB/s]     63%|######2   | 235M/374M [00:07<00:04, 35.0MB/s]     64%|######3   | 239M/374M [00:07<00:04, 35.5MB/s]     65%|######4   | 242M/374M [00:07<00:03, 35.1MB/s]     66%|######5   | 246M/374M [00:07<00:03, 35.9MB/s]     67%|######6   | 249M/374M [00:07<00:03, 35.4MB/s]     68%|######7   | 253M/374M [00:07<00:03, 35.9MB/s]     69%|######8   | 256M/374M [00:07<00:03, 35.5MB/s]     70%|######9   | 260M/374M [00:07<00:03, 36.4MB/s]     71%|#######   | 264M/374M [00:07<00:03, 36.0MB/s]     72%|#######1  | 268M/374M [00:08<00:03, 36.7MB/s]     73%|#######2  | 271M/374M [00:08<00:03, 35.9MB/s]     74%|#######3  | 275M/374M [00:08<00:02, 36.8MB/s]     74%|#######4  | 279M/374M [00:08<00:02, 36.0MB/s]     76%|#######5  | 283M/374M [00:08<00:02, 37.1MB/s]     76%|#######6  | 286M/374M [00:08<00:02, 36.2MB/s]     78%|#######7  | 290M/374M [00:08<00:02, 37.3MB/s]     78%|#######8  | 294M/374M [00:08<00:02, 36.4MB/s]     80%|#######9  | 298M/374M [00:08<00:02, 37.6MB/s]     81%|########  | 301M/374M [00:09<00:02, 36.6MB/s]     82%|########1 | 305M/374M [00:09<00:01, 37.8MB/s]     83%|########2 | 309M/374M [00:09<00:01, 36.8MB/s]     84%|########3 | 313M/374M [00:09<00:01, 37.9MB/s]     85%|########4 | 316M/374M [00:09<00:01, 30.5MB/s]     86%|########5 | 321M/374M [00:09<00:01, 31.2MB/s]     87%|########6 | 324M/374M [00:09<00:01, 29.8MB/s]     88%|########7 | 328M/374M [00:09<00:01, 28.9MB/s]     89%|########8 | 332M/374M [00:10<00:01, 28.4MB/s]     90%|########9 | 335M/374M [00:10<00:01, 28.1MB/s]     91%|######### | 339M/374M [00:10<00:01, 28.0MB/s]     92%|#########1| 343M/374M [00:10<00:01, 28.0MB/s]     93%|#########2| 347M/374M [00:10<00:01, 28.0MB/s]     94%|#########3| 350M/374M [00:10<00:00, 28.2MB/s]     95%|#########4| 354M/374M [00:10<00:00, 28.0MB/s]     95%|#########5| 357M/374M [00:11<00:00, 25.6MB/s]     96%|#########6| 360M/374M [00:11<00:00, 24.0MB/s]     97%|#########6| 362M/374M [00:11<00:00, 23.0MB/s]     98%|#########7| 365M/374M [00:11<00:00, 22.3MB/s]     98%|#########8| 368M/374M [00:11<00:00, 21.9MB/s]     99%|#########9| 371M/374M [00:11<00:00, 21.4MB/s]    100%|#########9| 374M/374M [00:11<00:00, 21.4MB/s]    100%|##########| 374M/374M [00:11<00:00, 32.8MB/s]




.. GENERATED FROM PYTHON SOURCE LINES 48-50

Let's check the number of parameters of the policy:


.. GENERATED FROM PYTHON SOURCE LINES 50-52

.. code-block:: default

    print("number of params:", len(list(policy.parameters())))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    number of params: 4




.. GENERATED FROM PYTHON SOURCE LINES 53-55

We collect a rollout of 32 steps and print its output:


.. GENERATED FROM PYTHON SOURCE LINES 55-58

.. code-block:: default

    rollout = env_transformed.rollout(32, policy)
    print("rollout with transform:", rollout)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    rollout with transform: TensorDict(
        fields={
            action: Tensor(shape=torch.Size([32, 8]), device=cpu, dtype=torch.float32, is_shared=False),
            done: Tensor(shape=torch.Size([32, 1]), device=cpu, dtype=torch.bool, is_shared=False),
            next: TensorDict(
                fields={
                    r3m_vec: Tensor(shape=torch.Size([32, 2048]), device=cpu, dtype=torch.float32, is_shared=False)},
                batch_size=torch.Size([32]),
                device=cpu,
                is_shared=False),
            r3m_vec: Tensor(shape=torch.Size([32, 2048]), device=cpu, dtype=torch.float32, is_shared=False),
            reward: Tensor(shape=torch.Size([32, 1]), device=cpu, dtype=torch.float32, is_shared=False)},
        batch_size=torch.Size([32]),
        device=cpu,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 59-63

For fine tuning, we integrate the transform in the policy after making the parameters
trainable. In practice, it may be wiser to restrict this to a subset of the parameters (say the last layer
of the MLP).


.. GENERATED FROM PYTHON SOURCE LINES 63-67

.. code-block:: default

    r3m.train()
    policy = TensorDictSequential(r3m, policy)
    print("number of params after r3m is integrated:", len(list(policy.parameters())))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    number of params after r3m is integrated: 163




.. GENERATED FROM PYTHON SOURCE LINES 68-72

Again, we collect a rollout with R3M. The structure of the output has changed slightly, as now
the environment returns pixels (and not an embedding). The embedding "r3m_vec" is an intermediate
result of our policy.


.. GENERATED FROM PYTHON SOURCE LINES 72-75

.. code-block:: default

    rollout = base_env.rollout(32, policy)
    print("rollout, fine tuning:", rollout)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    rollout, fine tuning: TensorDict(
        fields={
            action: Tensor(shape=torch.Size([32, 8]), device=cpu, dtype=torch.float32, is_shared=False),
            done: Tensor(shape=torch.Size([32, 1]), device=cpu, dtype=torch.bool, is_shared=False),
            next: TensorDict(
                fields={
                    pixels: Tensor(shape=torch.Size([32, 480, 480, 3]), device=cpu, dtype=torch.uint8, is_shared=False)},
                batch_size=torch.Size([32]),
                device=cpu,
                is_shared=False),
            r3m_vec: Tensor(shape=torch.Size([32, 2048]), device=cpu, dtype=torch.float32, is_shared=False),
            reward: Tensor(shape=torch.Size([32, 1]), device=cpu, dtype=torch.float32, is_shared=False)},
        batch_size=torch.Size([32]),
        device=cpu,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 76-83

The easiness with which we have swapped the transform from the env to the policy
is due to the fact that both behave like TensorDictModule: they have a set of `"in_keys"` and
`"out_keys"` that make it easy to read and write output in different context.

To conclude this tutorial, let's have a look at how we could use R3M to read
images stored in a replay buffer (e.g. in an offline RL context). First, let's build our dataset:


.. GENERATED FROM PYTHON SOURCE LINES 83-88

.. code-block:: default

    from torchrl.data import LazyMemmapStorage, ReplayBuffer

    storage = LazyMemmapStorage(1000)
    rb = ReplayBuffer(storage=storage, transform=r3m)








.. GENERATED FROM PYTHON SOURCE LINES 89-92

We can now collect the data (random rollouts for our purpose) and fill the replay
buffer with it:


.. GENERATED FROM PYTHON SOURCE LINES 92-98

.. code-block:: default

    total = 0
    while total < 1000:
        tensordict = base_env.rollout(1000)
        rb.extend(tensordict)
        total += tensordict.numel()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Creating a MemmapStorage...
    The storage is being created: 
            action: /tmp/tmpthg35llc, 0.030517578125 Mb of storage (size: torch.Size([1000, 8])).
            done: /tmp/tmpf5b5eprx, 0.00095367431640625 Mb of storage (size: torch.Size([1000, 1])).
            pixels: /tmp/tmpmj6pboz0, 659.1796875 Mb of storage (size: torch.Size([1000, 480, 480, 3])).
            reward: /tmp/tmpmt5nw4wj, 0.003814697265625 Mb of storage (size: torch.Size([1000, 1])).
            ('next', 'pixels'): /tmp/tmpqnk84z3y, 659.1796875 Mb of storage (size: torch.Size([1000, 480, 480, 3])).




.. GENERATED FROM PYTHON SOURCE LINES 99-101

Let's check what our replay buffer storage looks like. It should not contain the "r3m_vec" entry
since we haven't used it yet:

.. GENERATED FROM PYTHON SOURCE LINES 101-103

.. code-block:: default

    print("stored data:", storage._storage)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    stored data: TensorDict(
        fields={
            action: MemmapTensor(shape=torch.Size([1000, 8]), device=cpu, dtype=torch.float32, is_shared=False),
            done: MemmapTensor(shape=torch.Size([1000, 1]), device=cpu, dtype=torch.bool, is_shared=False),
            next: TensorDict(
                fields={
                    pixels: MemmapTensor(shape=torch.Size([1000, 480, 480, 3]), device=cpu, dtype=torch.uint8, is_shared=False)},
                batch_size=torch.Size([1000]),
                device=cpu,
                is_shared=False),
            pixels: MemmapTensor(shape=torch.Size([1000, 480, 480, 3]), device=cpu, dtype=torch.uint8, is_shared=False),
            reward: MemmapTensor(shape=torch.Size([1000, 1]), device=cpu, dtype=torch.float32, is_shared=False)},
        batch_size=torch.Size([1000]),
        device=cpu,
        is_shared=False)




.. GENERATED FROM PYTHON SOURCE LINES 104-107

When sampling, the data will go through the R3M transform, giving us the processed data that we wanted.
In this way, we can train an algorithm offline on a dataset made of images:


.. GENERATED FROM PYTHON SOURCE LINES 107-109

.. code-block:: default

    batch = rb.sample(32)
    print("data after sampling:", batch)




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    data after sampling: TensorDict(
        fields={
            action: Tensor(shape=torch.Size([32, 8]), device=cpu, dtype=torch.float32, is_shared=False),
            done: Tensor(shape=torch.Size([32, 1]), device=cpu, dtype=torch.bool, is_shared=False),
            next: TensorDict(
                fields={
                    pixels: Tensor(shape=torch.Size([32, 480, 480, 3]), device=cpu, dtype=torch.uint8, is_shared=False)},
                batch_size=torch.Size([32]),
                device=cpu,
                is_shared=False),
            r3m_vec: Tensor(shape=torch.Size([32, 2048]), device=cpu, dtype=torch.float32, is_shared=False),
            reward: Tensor(shape=torch.Size([32, 1]), device=cpu, dtype=torch.float32, is_shared=False)},
        batch_size=torch.Size([32]),
        device=cpu,
        is_shared=False)





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 1 minutes  21.756 seconds)


.. _sphx_glr_download_tutorials_pretrained_models.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: pretrained_models.py <pretrained_models.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: pretrained_models.ipynb <pretrained_models.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
