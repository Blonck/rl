{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Coding a pixel-based DQN using TorchRL\n**Author**: [Vincent Moens](https://github.com/vmoens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This tutorial will guide you through the steps to code DQN to solve the\nCartPole task from scratch. DQN\n([Deep Q-Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)) was\nthe founding work in deep reinforcement learning.\nOn a high level, the algorithm is quite simple: Q-learning consists in learning a table of\nstate-action values in such a way that, when encountering any particular state,\nwe know which action to pick just by searching for the action with the\nhighest value. This simple setting requires the actions and states to be\ndiscrete, otherwise a lookup table cannot be built.\n\nDQN uses a neural network that encodes a map from the state-action space to\na value (scalar) space, which amortizes the cost of storing and exploring all\nthe possible state-action combinations: if a state has not been seen in the\npast, we can still pass it in conjunction with the various actions available\nthrough our neural network and get an interpolated value for each of the\nactions available.\n\nWe will solve the classic control problem of the cart pole. From the\nGymnasium doc from where this environment is retrieved:\n\n| A pole is attached by an un-actuated joint to a cart, which moves along a\n| frictionless track. The pendulum is placed upright on the cart and the goal\n| is to balance the pole by applying forces in the left and right direction\n| on the cart.\n\n.. figure:: /_static/img/cartpole_demo.gif\n   :alt: Cart Pole\n\n**Prerequisites**: We encourage you to get familiar with torchrl through the\n[PPO tutorial](https://pytorch.org/rl/tutorials/coding_ppo.html) first.\nThis tutorial is more complex and full-fleshed, but it may be .\n\nIn this tutorial, you will learn:\n\n- how to build an environment in TorchRL, including transforms (e.g. data\n  normalization, frame concatenation, resizing and turning to grayscale)\n  and parallel execution. Unlike what we did in the\n  [DDPG tutorial](https://pytorch.org/rl/tutorials/coding_ddpg.html), we\n  will normalize the pixels and not the state vector.\n- how to design a QValue actor, i.e. an actor that estimates the action\n  values and picks up the action with the highest estimated return;\n- how to collect data from your environment efficiently and store them\n  in a replay buffer;\n- how to store trajectories (and not transitions) in your replay buffer),\n  and how to estimate returns using TD(lambda);\n- how to make a module functional and use ;\n- and finally how to evaluate your model.\n\nThis tutorial assumes the reader is familiar with some of TorchRL\nprimitives, such as :class:`tensordict.TensorDict` and\n:class:`tensordict.TensorDictModules`, although it\nshould be sufficiently transparent to be understood without a deep\nunderstanding of these classes.\n\nWe do not aim at giving a SOTA implementation of the algorithm, but rather\nto provide a high-level illustration of TorchRL features in the context\nof this algorithm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport tqdm\nfrom functorch import vmap\nfrom matplotlib import pyplot as plt\nfrom tensordict import TensorDict\nfrom tensordict.nn import get_functional\nfrom torch import nn\nfrom torchrl.collectors import MultiaSyncDataCollector\nfrom torchrl.data import LazyMemmapStorage, TensorDictReplayBuffer\nfrom torchrl.envs import EnvCreator, ParallelEnv, RewardScaling\nfrom torchrl.envs.libs.gym import GymEnv\nfrom torchrl.envs.transforms import (\n    CatFrames,\n    CatTensors,\n    Compose,\n    GrayScale,\n    ObservationNorm,\n    Resize,\n    ToTensorImage,\n    TransformedEnv,\n)\nfrom torchrl.envs.utils import set_exploration_mode, step_mdp\nfrom torchrl.modules import DuelingCnnDQNet, EGreedyWrapper, QValueActor\n\n\ndef is_notebook() -> bool:\n    try:\n        shell = get_ipython().__class__.__name__\n        if shell == \"ZMQInteractiveShell\":\n            return True  # Jupyter notebook or qtconsole\n        elif shell == \"TerminalInteractiveShell\":\n            return False  # Terminal running IPython\n        else:\n            return False  # Other type (?)\n    except NameError:\n        return False  # Probably standard Python interpreter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters\n\nLet's start with our hyperparameters. The following setting should work well\nin practice, and the performance of the algorithm should hopefully not be\ntoo sensitive to slight variations of these.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = \"cuda:0\" if torch.cuda.device_count() > 0 else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimizer\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# the learning rate of the optimizer\nlr = 2e-3\n# the beta parameters of Adam\nbetas = (0.9, 0.999)\n# Optimization steps per batch collected (aka UPD or updates per data)\nn_optim = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DQN parameters\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "gamma decay factor\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gamma = 0.99"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "lambda decay factor (see second the part with TD($\\lambda$)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lmbda = 0.95"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Smooth target network update decay parameter.\nThis loosely corresponds to a 1/(1-tau) interval with hard target network\nupdate\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tau = 0.005"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data collection and replay buffer\nValues to be used for proper training have been commented.\n\nTotal frames collected in the environment. In other implementations, the\nuser defines a maximum number of episodes.\nThis is harder to do with our data collectors since they return batches\nof N collected frames, where N is a constant.\nHowever, one can easily get the same restriction on number of episodes by\nbreaking the training loop when a certain number\nepisodes has been collected.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "total_frames = 5000  # 500000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Random frames used to initialize the replay buffer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "init_random_frames = 100  # 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Frames in each batch collected.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "frames_per_batch = 32  # 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Frames sampled from the replay buffer at each optimization step\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 32  # 256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Size of the replay buffer in terms of frames\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "buffer_size = min(total_frames, 100000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Number of environments run in parallel in each data collector\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_workers = 2  # 8\nnum_collectors = 2  # 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment and exploration\n\nWe set the initial and final value of the epsilon factor in Epsilon-greedy\nexploration.\nSince our policy is deterministic, exploration is crucial: without it, the\nonly source of randomness would be the environment reset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "eps_greedy_val = 0.1\neps_greedy_val_env = 0.005"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To speed up learning, we set the bias of the last layer of our value network\nto a predefined value (this is not mandatory)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "init_bias = 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note**: for fast rendering of the tutorial ``total_frames`` hyperparameter\nwas set to a very low number. To get a reasonable performance, use a greater\nvalue e.g. 500000\n\n## Building the environment\n\nOur environment builder has two arguments:\n\n- ``parallel``: determines whether multiple environments have to be run in\n  parallel. We stack the transforms after the\n  :class:`torchrl.envs.ParallelEnv` to take advantage\n  of vectorization of the operations on device, although this would\n  technically work with every single environment attached to its own set of\n  transforms.\n- ``observation_norm_state_dict`` will contain the normalizing constants for\n  the :class:`torchrl.envs.ObservationNorm` tranform.\n\nWe will be using five transforms:\n\n- :class:`torchrl.envs.ToTensorImage` will convert a ``[W, H, C]`` uint8\n  tensor in a floating point tensor in the ``[0, 1]`` space with shape\n  ``[C, W, H]``;\n- :class:`torchrl.envs.RewardScaling` to reduce the scale of the return;\n- :class:`torchrl.envs.GrayScale` will turn our image into grayscale;\n- :class:`torchrl.envs.Resize` will resize the image in a 64x64 format;\n- :class:`torchrl.envs.CatFrames` will concatenate an arbitrary number of\n  successive frames (``N=4``) in a single tensor along the channel dimension.\n  This is useful as a single image does not carry information about the\n  motion of the cartpole. Some memory about past observations and actions\n  is needed, either via a recurrent neural network or using a stack of\n  frames.\n- :class:`torchrl.envs.ObservationNorm` which will normalize our observations\n  given some custom summary statistics.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_env(parallel=False, observation_norm_state_dict=None):\n    if observation_norm_state_dict is None:\n        observation_norm_state_dict = {\"standard_normal\": True}\n    if parallel:\n        base_env = ParallelEnv(\n            num_workers,\n            EnvCreator(\n                lambda: GymEnv(\n                    \"CartPole-v1\", from_pixels=True, pixels_only=True, device=device\n                )\n            ),\n        )\n    else:\n        base_env = GymEnv(\n            \"CartPole-v1\", from_pixels=True, pixels_only=True, device=device\n        )\n\n    env = TransformedEnv(\n        base_env,\n        Compose(\n            ToTensorImage(),\n            RewardScaling(loc=0.0, scale=0.1),\n            GrayScale(),\n            Resize(64, 64),\n            CatFrames(4, in_keys=[\"pixels\"], dim=-3),\n            ObservationNorm(in_keys=[\"pixels\"], **observation_norm_state_dict),\n        ),\n    )\n    return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compute normalizing constants\n\nTo normalize images, we don't want to normalize each pixel independently\nwith a full ``[C, W, H]`` normalizing mask, but with simpler ``[C, 1, 1]``\nshaped loc and scale parameters. We will be using the ``reduce_dim`` argument\nof :func:`torchrl.envs.ObservationNorm.init_stats` to instruct which\ndimensions must be reduced, and the ``keep_dims`` parameter to ensure that\nnot all dimensions disappear in the process:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "test_env = make_env()\ntest_env.transform[-1].init_stats(\n    num_iter=1000, cat_dim=0, reduce_dim=[-1, -2, -4], keep_dims=(-1, -2)\n)\nobservation_norm_state_dict = test_env.transform[-1].state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "let's check that normalizing constants have a size of ``[C, 1, 1]`` where\n``C=4`` (because of :class:`torchrl.envs.CatFrames`).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(observation_norm_state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building the model (Deep Q-network)\n\nThe following function builds a :class:`torchrl.modules.DuelingCnnDQNet`\nobject which is a simple CNN followed by a two-layer MLP. The only trick used\nhere is that the action values (i.e. left and right action value) are\ncomputed using\n\n\\begin{align}val = b(obs) + v(obs) - \\mathbb{E}[v(obs)]\\end{align}\n\nwhere $b$ is a $\\# obs \\rightarrow 1$ function and $v$ is a\n$\\# obs \\rightarrow num_actions$ function.\n\nOur network is wrapped in a :class:`torchrl.modules.QValueActor`, which will read the state-action\nvalues, pick up the one with the maximum value and write all those results\nin the input :class:`tensordict.TensorDict`.\n\n### Target parameters\n\nMany off-policy RL algorithms use the concept of \"target parameters\" when it\ncomes to estimate the value of the ``t+1`` state or state-action pair.\nThe target parameters are lagged copies of the model parameters. Because\ntheir predictions mismatch those of the current model configuration, they\nhelp learning by putting a pessimistic bound on the value being estimated.\nThis is a powerful trick (known as \"Double Q-Learning\") that is ubiquitous\nin similar algorithms.\n\n### Functionalizing modules\n\nOne of the features of torchrl is its usage of functional modules: as the\nsame architecture is often used with multiple sets of parameters (e.g.\ntrainable and target parameters), we functionalize the modules and isolate\nthe various sets of parameters in separate tensordicts.\n\nTo this aim, we use :func:`tensordict.nn.get_functional`, which augments\nour modules with some extra feature that make them compatible with parameters\npassed in the ``TensorDict`` format.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_model(dummy_env):\n    cnn_kwargs = {\n        \"num_cells\": [32, 64, 64],\n        \"kernel_sizes\": [6, 4, 3],\n        \"strides\": [2, 2, 1],\n        \"activation_class\": nn.ELU,\n        # This can be used to reduce the size of the last layer of the CNN\n        # \"squeeze_output\": True,\n        # \"aggregator_class\": nn.AdaptiveAvgPool2d,\n        # \"aggregator_kwargs\": {\"output_size\": (1, 1)},\n    }\n    mlp_kwargs = {\n        \"depth\": 2,\n        \"num_cells\": [\n            64,\n            64,\n        ],\n        \"activation_class\": nn.ELU,\n    }\n    net = DuelingCnnDQNet(\n        dummy_env.action_spec.shape[-1], 1, cnn_kwargs, mlp_kwargs\n    ).to(device)\n    net.value[-1].bias.data.fill_(init_bias)\n\n    actor = QValueActor(net, in_keys=[\"pixels\"], spec=dummy_env.action_spec).to(device)\n    # init actor: because the model is composed of lazy conv/linear layers,\n    # we must pass a fake batch of data through it to instantiate them.\n    tensordict = dummy_env.fake_tensordict()\n    actor(tensordict)\n\n    # Make functional:\n    # here's an explicit way of creating the parameters and buffer tensordict.\n    # Alternatively, we could have used `params = make_functional(actor)` from\n    # tensordict.nn\n    params = TensorDict({k: v for k, v in actor.named_parameters()}, [])\n    buffers = TensorDict({k: v for k, v in actor.named_buffers()}, [])\n    params = params.update(buffers)\n    params = params.unflatten_keys(\".\")  # creates a nested TensorDict\n    factor = get_functional(actor)\n\n    # creating the target parameters is fairly easy with tensordict:\n    params_target = params.clone().detach()\n\n    # we wrap our actor in an EGreedyWrapper for data collection\n    actor_explore = EGreedyWrapper(\n        actor,\n        annealing_num_steps=total_frames,\n        eps_init=eps_greedy_val,\n        eps_end=eps_greedy_val_env,\n    )\n\n    return factor, actor, actor_explore, params, params_target\n\n\n(\n    factor,\n    actor,\n    actor_explore,\n    params,\n    params_target,\n) = make_model(test_env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We represent the parameters and targets as flat structures, but unflattening\nthem is quite easy:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "params_flat = params.flatten_keys(\".\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will be using the adam optimizer:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optim = torch.optim.Adam(list(params_flat.values()), lr, betas=betas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create a test environment for evaluation of the policy:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "test_env = make_env(\n    parallel=False, observation_norm_state_dict=observation_norm_state_dict\n)\n# sanity check:\nprint(actor_explore(test_env.reset()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Collecting and storing data\n\n### Replay buffers\n\nReplay buffers play a central role in off-policy RL algorithms such as DQN.\nThey constitute the dataset we will be sampling from during training.\n\nHere, we will use a regular sampling strategy, although a prioritized RB\ncould improve the performance significantly.\n\nWe place the storage on disk using\n:class:`torchrl.data.replay_buffers.storages.LazyMemmapStorage` class. This\nstorage is created in a lazy manner: it will only be instantiated once the\nfirst batch of data is passed to it.\n\nThe only requirement of this storage is that the data passed to it at write\ntime must always have the same shape.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "replay_buffer = TensorDictReplayBuffer(\n    storage=LazyMemmapStorage(buffer_size),\n    prefetch=n_optim,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data collector\n\nAs in `PPO <https://pytorch.org/rl/tutorials/coding_ppo.html>` and\n`DDPG <https://pytorch.org/rl/tutorials/coding_ddpg.html>`, we will be using\na data collector as a dataloader in the outer loop.\n\nWe choose the following configuration: we will be running a series of\nparallel environments synchronously in parallel in different collectors,\nthemselves running in parallel but asynchronously.\nThe advantage of this configuration is that we can balance the amount of\ncompute that is executed in batch with what we want to be executed\nasynchronously. We encourage the reader to experiment how the collection\nspeed is impacted by modifying the number of collectors (ie the number of\nenvironment constructors passed to the collector) and the number of\nenvironment executed in parallel in each collector (controlled by the\n``num_workers`` hyperparameter).\n\nWhen building the collector, we can choose on which device we want the\nenvironment and policy to execute the operations through the ``device``\nkeyword argument. The ``storing_devices`` argument will modify the\nlocation of the data being collected: if the batches that we are gathering\nhave a considerable size, we may want to store them on a different location\nthan the device where the computation is happening. For asynchronous data\ncollectors such as ours, different storing devices mean that the data that\nwe collect won't sit on the same device each time, which is something that\nout training loop must account for. For simplicity, we set the devices to\nthe same value for all sub-collectors.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_collector = MultiaSyncDataCollector(\n    # ``num_collectors`` collectors, each with an set of `num_workers` environments being run in parallel\n    [\n        make_env(\n            parallel=True, observation_norm_state_dict=observation_norm_state_dict\n        ),\n    ]\n    * num_collectors,\n    policy=actor_explore,\n    frames_per_batch=frames_per_batch,\n    total_frames=total_frames,\n    # this is the default behaviour: the collector runs in ``\"random\"`` (or explorative) mode\n    exploration_mode=\"random\",\n    # We set the all the devices to be identical. Below is an example of\n    # heterogeneous devices\n    devices=[device] * num_collectors,\n    storing_devices=[device] * num_collectors,\n    # devices=[f\"cuda:{i}\" for i in range(1, 1 + num_collectors)],\n    # storing_devices=[f\"cuda:{i}\" for i in range(1, 1 + num_collectors)],\n    split_trajs=False,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training loop of a regular DQN\n\nWe'll start with a simple implementation of DQN where the returns are\ncomputed without bootstrapping, i.e.\n\n\\begin{align}Q_{t}(s, a) = R(s, a) + \\gamma * V_{t+1}(s)\\end{align}\n\nwhere $Q(s, a)$ is the Q-value of the current state-action pair,\n$R(s, a)$ is the result of the reward function, and $V(s)$ is a\nvalue function that returns 0 for terminating states.\n\nWe store the logs in a defaultdict:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "logs_exp1 = defaultdict(list)\nprev_traj_count = 0\n\npbar = tqdm.tqdm(total=total_frames)\nfor j, data in enumerate(data_collector):\n    current_frames = data.numel()\n    pbar.update(current_frames)\n    data = data.view(-1)\n\n    # We store the values on the replay buffer, after placing them on CPU.\n    # When called for the first time, this will instantiate our storage\n    # object which will print its content.\n    replay_buffer.extend(data.cpu())\n\n    # some logging\n    if len(logs_exp1[\"frames\"]):\n        logs_exp1[\"frames\"].append(current_frames + logs_exp1[\"frames\"][-1])\n    else:\n        logs_exp1[\"frames\"].append(current_frames)\n\n    if data[\"done\"].any():\n        done = data[\"done\"].squeeze(-1)\n        logs_exp1[\"traj_lengths\"].append(\n            data[\"collector\", \"step_count\"][done].float().mean().item()\n        )\n\n    # check that we have enough data to start training\n    if sum(logs_exp1[\"frames\"]) > init_random_frames:\n        for _ in range(n_optim):\n            # sample from the RB and send to device\n            sampled_data = replay_buffer.sample(batch_size)\n            sampled_data = sampled_data.to(device, non_blocking=True)\n\n            # collect data from RB\n            reward = sampled_data[\"reward\"].squeeze(-1)\n            done = sampled_data[\"done\"].squeeze(-1).to(reward.dtype)\n            action = sampled_data[\"action\"].clone()\n\n            # Compute action value (of the action actually taken) at time t\n            # By default, TorchRL uses one-hot encodings for discrete actions\n            sampled_data_out = sampled_data.select(*actor.in_keys)\n            sampled_data_out = factor(sampled_data_out, params=params)\n            action_value = sampled_data_out[\"action_value\"]\n            action_value = (action_value * action.to(action_value.dtype)).sum(-1)\n            with torch.no_grad():\n                # compute best action value for the next step, using target parameters\n                tdstep = step_mdp(sampled_data)\n                next_value = factor(\n                    tdstep.select(*actor.in_keys),\n                    params=params_target,\n                )[\"chosen_action_value\"].squeeze(-1)\n                exp_value = reward + gamma * next_value * (1 - done)\n            assert exp_value.shape == action_value.shape\n            # we use MSE loss but L1 or smooth L1 should also work\n            error = nn.functional.mse_loss(exp_value, action_value).mean()\n            error.backward()\n\n            gv = nn.utils.clip_grad_norm_(list(params_flat.values()), 1)\n\n            optim.step()\n            optim.zero_grad()\n\n            # update of the target parameters\n            params_target.apply(\n                lambda p_target, p_orig: p_orig * tau + p_target * (1 - tau),\n                params.detach(),\n                inplace=True,\n            )\n\n        actor_explore.step(current_frames)\n\n        # Logging\n        logs_exp1[\"grad_vals\"].append(float(gv))\n        logs_exp1[\"losses\"].append(error.item())\n        logs_exp1[\"values\"].append(action_value.mean().item())\n        logs_exp1[\"traj_count\"].append(prev_traj_count + data[\"done\"].sum().item())\n        prev_traj_count = logs_exp1[\"traj_count\"][-1]\n\n        if j % 10 == 0:\n            with set_exploration_mode(\"mode\"), torch.no_grad():\n                # execute a rollout. The `set_exploration_mode(\"mode\")` has no effect here since the policy is deterministic, but we add it for completeness\n                eval_rollout = test_env.rollout(\n                    max_steps=10000,\n                    policy=actor,\n                ).cpu()\n            logs_exp1[\"traj_lengths_eval\"].append(eval_rollout.shape[-1])\n            logs_exp1[\"evals\"].append(eval_rollout[\"reward\"].sum().item())\n            if len(logs_exp1[\"mavgs\"]):\n                logs_exp1[\"mavgs\"].append(\n                    logs_exp1[\"evals\"][-1] * 0.05 + logs_exp1[\"mavgs\"][-1] * 0.95\n                )\n            else:\n                logs_exp1[\"mavgs\"].append(logs_exp1[\"evals\"][-1])\n            logs_exp1[\"traj_count_eval\"].append(logs_exp1[\"traj_count\"][-1])\n            pbar.set_description(\n                f\"error: {error: 4.4f}, value: {action_value.mean(): 4.4f}, test return: {logs_exp1['evals'][-1]: 4.4f}\"\n            )\n\n    # update policy weights\n    data_collector.update_policy_weights_()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We write a custom plot function to display the performance of our algorithm\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot(logs, name):\n    plt.figure(figsize=(15, 10))\n    plt.subplot(2, 3, 1)\n    plt.plot(\n        logs[\"frames\"][-len(logs[\"evals\"]) :],\n        logs[\"evals\"],\n        label=\"return (eval)\",\n    )\n    plt.plot(\n        logs[\"frames\"][-len(logs[\"mavgs\"]) :],\n        logs[\"mavgs\"],\n        label=\"mavg of returns (eval)\",\n    )\n    plt.xlabel(\"frames collected\")\n    plt.ylabel(\"trajectory length (= return)\")\n    plt.subplot(2, 3, 2)\n    plt.plot(\n        logs[\"traj_count\"][-len(logs[\"evals\"]) :],\n        logs[\"evals\"],\n        label=\"return\",\n    )\n    plt.plot(\n        logs[\"traj_count\"][-len(logs[\"mavgs\"]) :],\n        logs[\"mavgs\"],\n        label=\"mavg\",\n    )\n    plt.xlabel(\"trajectories collected\")\n    plt.legend()\n    plt.subplot(2, 3, 3)\n    plt.plot(logs[\"frames\"][-len(logs[\"losses\"]) :], logs[\"losses\"])\n    plt.xlabel(\"frames collected\")\n    plt.title(\"loss\")\n    plt.subplot(2, 3, 4)\n    plt.plot(logs[\"frames\"][-len(logs[\"values\"]) :], logs[\"values\"])\n    plt.xlabel(\"frames collected\")\n    plt.title(\"value\")\n    plt.subplot(2, 3, 5)\n    plt.plot(\n        logs[\"frames\"][-len(logs[\"grad_vals\"]) :],\n        logs[\"grad_vals\"],\n    )\n    plt.xlabel(\"frames collected\")\n    plt.title(\"grad norm\")\n    if len(logs[\"traj_lengths\"]):\n        plt.subplot(2, 3, 6)\n        plt.plot(logs[\"traj_lengths\"])\n        plt.xlabel(\"batches\")\n        plt.title(\"traj length (training)\")\n    plt.savefig(name)\n    if is_notebook():\n        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The performance of the policy can be measured as the length of trajectories.\nAs we can see on the results of the :func:`plot` function, the performance\nof the policy increases, albeit slowly.\n\n```python\nplot(logs_exp1, \"dqn_td0.png\")\n```\n.. figure:: /_static/img/dqn_td0.png\n   :alt: Cart Pole results with TD(0)\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"shutting down\")\ndata_collector.shutdown()\ndel data_collector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DQN with TD($\\lambda$)\n\nWe can improve the above algorithm by getting a better estimate of the\nreturn, using not only the next state value but the whole sequence of rewards\nand values that follow a particular step.\n\nTorchRL provides a vectorized version of TD(lambda) named\n:func:`torchrl.objectives.value.functional.vec_td_lambda_advantage_estimate`.\nWe'll use this to obtain a target value that the value network will be\ntrained to match.\n\nThe big difference in this implementation is that we'll store entire\ntrajectories and not single steps in the replay buffer. This will be done\nautomatically as long as we're not \"flattening\" the tensordict collected:\nby keeping a shape ``[Batch x timesteps]`` and giving this\nto the RB, we'll be creating a replay buffer of size\n``[Capacity x timesteps]``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.objectives.value.functional import vec_td_lambda_advantage_estimate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We reset the actor parameters:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "(\n    factor,\n    actor,\n    actor_explore,\n    params,\n    params_target,\n) = make_model(test_env)\nparams_flat = params.flatten_keys(\".\")\n\noptim = torch.optim.Adam(list(params_flat.values()), lr, betas=betas)\ntest_env = make_env(\n    parallel=False, observation_norm_state_dict=observation_norm_state_dict\n)\nprint(actor_explore(test_env.reset()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data: Replay buffer and collector\n\nWe need to build a new replay buffer of the appropriate size:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "max_size = frames_per_batch // num_workers\n\nreplay_buffer = TensorDictReplayBuffer(\n    storage=LazyMemmapStorage(-(-buffer_size // max_size)),\n    prefetch=n_optim,\n)\n\ndata_collector = MultiaSyncDataCollector(\n    [\n        make_env(\n            parallel=True, observation_norm_state_dict=observation_norm_state_dict\n        ),\n    ]\n    * num_collectors,\n    policy=actor_explore,\n    frames_per_batch=frames_per_batch,\n    total_frames=total_frames,\n    exploration_mode=\"random\",\n    devices=[device] * num_collectors,\n    storing_devices=[device] * num_collectors,\n    # devices=[f\"cuda:{i}\" for i in range(1, 1 + num_collectors)],\n    # storing_devices=[f\"cuda:{i}\" for i in range(1, 1 + num_collectors)],\n    split_trajs=False,\n)\n\n\nlogs_exp2 = defaultdict(list)\nprev_traj_count = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training loop\n\nThere are very few differences with the training loop above:\n\n- The tensordict received by the collector is used as-is, without being\n  flattened (recall the ``data.view(-1)`` above), to keep the temporal\n  relation between consecutive steps.\n- We use :func:`vec_td_lambda_advantage_estimate` to compute the target\n  value.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pbar = tqdm.tqdm(total=total_frames)\nfor j, data in enumerate(data_collector):\n    current_frames = data.numel()\n    pbar.update(current_frames)\n\n    replay_buffer.extend(data.cpu())\n    if len(logs_exp2[\"frames\"]):\n        logs_exp2[\"frames\"].append(current_frames + logs_exp2[\"frames\"][-1])\n    else:\n        logs_exp2[\"frames\"].append(current_frames)\n\n    if data[\"done\"].any():\n        done = data[\"done\"].squeeze(-1)\n        logs_exp2[\"traj_lengths\"].append(\n            data[\"collector\", \"step_count\"][done].float().mean().item()\n        )\n\n    if sum(logs_exp2[\"frames\"]) > init_random_frames:\n        for _ in range(n_optim):\n            sampled_data = replay_buffer.sample(batch_size // max_size)\n            sampled_data = sampled_data.clone().to(device, non_blocking=True)\n\n            reward = sampled_data[\"reward\"]\n            done = sampled_data[\"done\"].to(reward.dtype)\n            action = sampled_data[\"action\"].clone()\n\n            sampled_data_out = sampled_data.select(*actor.in_keys)\n            sampled_data_out = vmap(factor, (0, None))(sampled_data_out, params)\n            action_value = sampled_data_out[\"action_value\"]\n            action_value = (action_value * action.to(action_value.dtype)).sum(-1, True)\n            with torch.no_grad():\n                tdstep = step_mdp(sampled_data)\n                next_value = vmap(factor, (0, None))(\n                    tdstep.select(*actor.in_keys), params\n                )\n                next_value = next_value[\"chosen_action_value\"]\n            error = vec_td_lambda_advantage_estimate(\n                gamma,\n                lmbda,\n                action_value,\n                next_value,\n                reward,\n                done,\n            ).pow(2)\n            error = error.mean()\n            error.backward()\n\n            gv = nn.utils.clip_grad_norm_(list(params_flat.values()), 1)\n\n            optim.step()\n            optim.zero_grad()\n\n            # update of the target parameters\n            params_target.apply(\n                lambda p_target, p_orig: p_orig * tau + p_target * (1 - tau),\n                params.detach(),\n                inplace=True,\n            )\n\n        actor_explore.step(current_frames)\n\n        # Logging\n        logs_exp2[\"grad_vals\"].append(float(gv))\n\n        logs_exp2[\"losses\"].append(error.item())\n        logs_exp2[\"values\"].append(action_value.mean().item())\n        logs_exp2[\"traj_count\"].append(prev_traj_count + data[\"done\"].sum().item())\n        prev_traj_count = logs_exp2[\"traj_count\"][-1]\n        if j % 10 == 0:\n            with set_exploration_mode(\"mode\"), torch.no_grad():\n                # execute a rollout. The `set_exploration_mode(\"mode\")` has\n                # no effect here since the policy is deterministic, but we add\n                # it for completeness\n                eval_rollout = test_env.rollout(\n                    max_steps=10000,\n                    policy=actor,\n                ).cpu()\n            logs_exp2[\"traj_lengths_eval\"].append(eval_rollout.shape[-1])\n            logs_exp2[\"evals\"].append(eval_rollout[\"reward\"].sum().item())\n            if len(logs_exp2[\"mavgs\"]):\n                logs_exp2[\"mavgs\"].append(\n                    logs_exp2[\"evals\"][-1] * 0.05 + logs_exp2[\"mavgs\"][-1] * 0.95\n                )\n            else:\n                logs_exp2[\"mavgs\"].append(logs_exp2[\"evals\"][-1])\n            logs_exp2[\"traj_count_eval\"].append(logs_exp2[\"traj_count\"][-1])\n            pbar.set_description(\n                f\"error: {error: 4.4f}, value: {action_value.mean(): 4.4f}, test return: {logs_exp2['evals'][-1]: 4.4f}\"\n            )\n\n    # update policy weights\n    data_collector.update_policy_weights_()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TD($\\lambda$) performs significantly better than TD(0) because it\nretrieves a much less biased estimate of the state-action value.\n\n```python\nplot(logs_exp2, \"dqn_tdlambda.png\")\n```\n.. figure:: /_static/img/dqn_tdlambda.png\n   :alt: Cart Pole results with TD(lambda)\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"shutting down\")\ndata_collector.shutdown()\ndel data_collector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's compare the results on a single plot. Because the TD(lambda) version\nworks better, we'll have fewer episodes collected for a given number of\nframes (as there are more frames per episode).\n\n**Note**: As already mentioned above, to get a more reasonable performance,\nuse a greater value for ``total_frames`` e.g. 500000.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_both():\n    frames_td0 = logs_exp1[\"frames\"]\n    frames_tdlambda = logs_exp2[\"frames\"]\n    evals_td0 = logs_exp1[\"evals\"]\n    evals_tdlambda = logs_exp2[\"evals\"]\n    mavgs_td0 = logs_exp1[\"mavgs\"]\n    mavgs_tdlambda = logs_exp2[\"mavgs\"]\n    traj_count_td0 = logs_exp1[\"traj_count_eval\"]\n    traj_count_tdlambda = logs_exp2[\"traj_count_eval\"]\n\n    plt.figure(figsize=(15, 10))\n    plt.subplot(1, 2, 1)\n    plt.plot(frames_td0[-len(evals_td0) :], evals_td0, label=\"return (td0)\", alpha=0.5)\n    plt.plot(\n        frames_tdlambda[-len(evals_tdlambda) :],\n        evals_tdlambda,\n        label=\"return (td(lambda))\",\n        alpha=0.5,\n    )\n    plt.plot(frames_td0[-len(mavgs_td0) :], mavgs_td0, label=\"mavg (td0)\")\n    plt.plot(\n        frames_tdlambda[-len(mavgs_tdlambda) :],\n        mavgs_tdlambda,\n        label=\"mavg (td(lambda))\",\n    )\n    plt.xlabel(\"frames collected\")\n    plt.ylabel(\"trajectory length (= return)\")\n\n    plt.subplot(1, 2, 2)\n    plt.plot(\n        traj_count_td0[-len(evals_td0) :],\n        evals_td0,\n        label=\"return (td0)\",\n        alpha=0.5,\n    )\n    plt.plot(\n        traj_count_tdlambda[-len(evals_tdlambda) :],\n        evals_tdlambda,\n        label=\"return (td(lambda))\",\n        alpha=0.5,\n    )\n    plt.plot(traj_count_td0[-len(mavgs_td0) :], mavgs_td0, label=\"mavg (td0)\")\n    plt.plot(\n        traj_count_tdlambda[-len(mavgs_tdlambda) :],\n        mavgs_tdlambda,\n        label=\"mavg (td(lambda))\",\n    )\n    plt.xlabel(\"trajectories collected\")\n    plt.legend()\n\n    plt.savefig(\"dqn.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\nplot_both()\n```\n.. figure:: /_static/img/dqn.png\n   :alt: Cart Pole results from the TD($lambda$) trained policy.\n\nFinally, we generate a new video to check what the algorithm has learnt.\nIf all goes well, the duration should be significantly longer than with a\nrandom rollout.\n\nTo get the raw pixels of the rollout, we insert a\n:class:`torchrl.envs.CatTensors` transform that precedes all others and copies\nthe ``\"pixels\"`` key onto a ``\"pixels_save\"`` key. This is necessary because\nthe other transforms that modify this key will update its value in-place in\nthe output tensordict.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "test_env.transform.insert(0, CatTensors([\"pixels\"], \"pixels_save\", del_keys=False))\neval_rollout = test_env.rollout(max_steps=10000, policy=actor, auto_reset=True).cpu()\n\n\ndel test_env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The video of the rollout can be saved using the imageio package:\n\n```\nimport imageio\nimageio.mimwrite('cartpole.mp4', eval_rollout[\"pixels_save\"].numpy(), fps=30);\n```\n.. figure:: /_static/img/cartpole.gif\n   :alt: Cart Pole results from the TD($\\lambda$) trained policy.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion and possible improvements\n\nIn this tutorial we have learnt:\n\n- How to train a policy that read pixel-based states, what transforms to\n  include and how to normalize the data;\n- How to create a policy that picks up the action with the highest value\n  with :class:`torchrl.modules.QValueNetwork`;\n- How to build a multiprocessed data collector;\n- How to train a DQN with TD($\\lambda$) returns.\n\nWe have seen that using TD($\\lambda$) greatly improved the performance\nof DQN. Other possible improvements could include:\n\n- Using the Multi-Step post-processing. Multi-step will project an action\n  to the nth following step, and create a discounted sum of the rewards in\n  between. This trick can make the algorithm noticebly less myopic. To use\n  this, simply create the collector with\n\n      from torchrl.data.postprocs.postprocs import MultiStep\n      collector = CollectorClass(..., postproc=MultiStep(gamma, n))\n\n  where ``n`` is the number of looking-forward steps. Pay attention to the\n  fact that the ``gamma`` factor has to be corrected by the number of\n  steps till the next observation when being passed to\n  ``vec_td_lambda_advantage_estimate``:\n\n      gamma = gamma ** tensordict[\"steps_to_next_obs\"]\n- A prioritized replay buffer could also be used. This will give a\n  higher priority to samples that have the worst value accuracy.\n- A distributional loss (see ``torchrl.objectives.DistributionalDQNLoss``\n  for more information).\n- More fancy exploration techniques, such as NoisyLinear layers and such\n  (check ``torchrl.modules.NoisyLinear``, which is fully compatible with the\n  ``MLP`` class used in our Dueling DQN).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}