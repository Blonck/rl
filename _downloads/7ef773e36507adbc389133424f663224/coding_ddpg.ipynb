{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Coding DDPG using TorchRL\n**Author**: [Vincent Moens](https://github.com/vmoens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This tutorial will guide you through the steps to code DDPG from scratch.\n\nDDPG (`Deep Deterministic Policy Gradient <https://arxiv.org/abs/1509.02971>_`_)\nis a simple continuous control algorithm. It consists in learning a\nparametric value function for an action-observation pair, and\nthen learning a policy that outputs actions that maximise this value\nfunction given a certain observation.\n\nThis tutorial is more  than the PPO tutorial: it covers\nmultiple topics that were left aside. We strongly advise the reader to go\nthrough the PPO tutorial first before trying out this one. The goal is to\nshow how flexible torchrl is when it comes to writing scripts that can cover\nmultiple use cases.\n\nKey learnings:\n\n- how to build an environment in TorchRL, including transforms\n  (e.g. data normalization) and parallel execution;\n- how to design a policy and value network;\n- how to collect data from your environment efficiently and store them\n  in a replay buffer;\n- how to store trajectories (and not transitions) in your replay buffer);\n- and finally how to evaluate your model.\n\nThis tutorial assumes the reader is familiar with some of TorchRL primitives,\nsuch as :class:`tensordict.TensorDict` and\n:class:`tensordict.nn.TensorDictModules`, although it should be\nsufficiently transparent to be understood without a deep understanding of\nthese classes.\n\nWe do not aim at giving a SOTA implementation of the algorithm, but rather\nto provide a high-level illustration of TorchRL features in the context of\nthis algorithm.\n\n## Imports\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n\nimport numpy as np\nimport torch\nimport torch.cuda\nimport tqdm\nfrom matplotlib import pyplot as plt\nfrom tensordict.nn import TensorDictModule\nfrom torch import nn, optim\nfrom torchrl.collectors import MultiaSyncDataCollector\nfrom torchrl.data import CompositeSpec, TensorDictReplayBuffer\nfrom torchrl.data.postprocs import MultiStep\nfrom torchrl.data.replay_buffers.samplers import PrioritizedSampler, RandomSampler\nfrom torchrl.data.replay_buffers.storages import LazyMemmapStorage\nfrom torchrl.envs import (\n    CatTensors,\n    DoubleToFloat,\n    EnvCreator,\n    ObservationNorm,\n    ParallelEnv,\n)\nfrom torchrl.envs.libs.dm_control import DMControlEnv\nfrom torchrl.envs.libs.gym import GymEnv\nfrom torchrl.envs.transforms import RewardScaling, TransformedEnv\nfrom torchrl.envs.utils import set_exploration_mode, step_mdp\nfrom torchrl.modules import (\n    MLP,\n    OrnsteinUhlenbeckProcessWrapper,\n    ProbabilisticActor,\n    ValueOperator,\n)\nfrom torchrl.modules.distributions.continuous import TanhDelta\nfrom torchrl.objectives.utils import hold_out_net\nfrom torchrl.trainers import Recorder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment\n\nIn most algorithms, the first thing that needs to be taken care of is the\nconstruction of the environmet as it conditions the remainder of the\ntraining script.\n\nFor this example, we will be using the ``\"cheetah\"`` task. The goal is to make\na half-cheetah run as fast as possible.\n\nIn TorchRL, one can create such a task by relying on dm_control or gym:\n\n```python\nenv = GymEnv(\"HalfCheetah-v4\")\n```\nor\n\n```python\nenv = DMControlEnv(\"cheetah\", \"run\")\n```\nBy default, these environment disable rendering. Training from states is\nusually easier than training from images. To keep things simple, we focus\non learning from states only. To pass the pixels to the tensordicts that\nare collected by :func:`env.step()`, simply pass the ``from_pixels=True``\nargument to the constructor:\n\n```python\nenv = GymEnv(\"HalfCheetah-v4\", from_pixels=True, pixels_only=True)\n```\nWe write a :func:`make_env` helper funciton that will create an environment\nwith either one of the two backends considered above (dm-control or gym).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env_library = None\nenv_name = None\n\n\ndef make_env():\n    \"\"\"Create a base env.\"\"\"\n    global env_library\n    global env_name\n\n    if backend == \"dm_control\":\n        env_name = \"cheetah\"\n        env_task = \"run\"\n        env_args = (env_name, env_task)\n        env_library = DMControlEnv\n    elif backend == \"gym\":\n        env_name = \"HalfCheetah-v4\"\n        env_args = (env_name,)\n        env_library = GymEnv\n    else:\n        raise NotImplementedError\n\n    env_kwargs = {\n        \"device\": device,\n        \"frame_skip\": frame_skip,\n        \"from_pixels\": from_pixels,\n        \"pixels_only\": from_pixels,\n    }\n    env = env_library(*env_args, **env_kwargs)\n    return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transforms\n\nNow that we have a base environment, we may want to modify its representation\nto make it more policy-friendly. In TorchRL, transforms are appended to the\nbase environment in a specialized :class:`torchr.envs.TransformedEnv` class.\n\n- It is common in DDPG to rescale the reward using some heuristic value. We\n  will multiply the reward by 5 in this example.\n\n- If we are using :mod:`dm_control`, it is also important to build an interface\n  between the simulator which works with double precision numbers, and our\n  script which presumably uses single precision ones. This transformation goes\n  both ways: when calling :func:`env.step`, our actions will need to be\n  represented in double precision, and the output will need to be transformed\n  to single precision.\n  The :class:`torchrl.envs.DoubleToFloat` transform does exactly this: the\n  ``in_keys`` list refers to the keys that will need to be transformed from\n  double to float, while the ``in_keys_inv`` refers to those that need to\n  be transformed to double before being passed to the environment.\n\n- We concatenate the state keys together using the :class:`torchrl.envs.CatTensors`\n  transform.\n\n- Finally, we also leave the possibility of normalizing the states: we will\n  take care of computing the normalizing constants later on.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_transformed_env(\n    env,\n):\n    \"\"\"Apply transforms to the env (such as reward scaling and state normalization).\"\"\"\n\n    env = TransformedEnv(env)\n\n    # we append transforms one by one, although we might as well create the\n    # transformed environment using the `env = TransformedEnv(base_env, transforms)`\n    # syntax.\n    env.append_transform(RewardScaling(loc=0.0, scale=reward_scaling))\n\n    double_to_float_list = []\n    double_to_float_inv_list = []\n    if env_library is DMControlEnv:\n        # DMControl requires double-precision\n        double_to_float_list += [\n            \"reward\",\n            \"action\",\n        ]\n        double_to_float_inv_list += [\"action\"]\n\n    # We concatenate all states into a single \"observation_vector\"\n    # even if there is a single tensor, it'll be renamed in \"observation_vector\".\n    # This facilitates the downstream operations as we know the name of the\n    # output tensor.\n    # In some environments (not half-cheetah), there may be more than one\n    # observation vector: in this case this code snippet will concatenate them\n    # all.\n    selected_keys = list(env.observation_spec.keys())\n    out_key = \"observation_vector\"\n    env.append_transform(CatTensors(in_keys=selected_keys, out_key=out_key))\n\n    # we normalize the states, but for now let's just instantiate a stateless\n    # version of the transform\n    env.append_transform(ObservationNorm(in_keys=[out_key], standard_normal=True))\n\n    double_to_float_list.append(out_key)\n    env.append_transform(\n        DoubleToFloat(\n            in_keys=double_to_float_list, in_keys_inv=double_to_float_inv_list\n        )\n    )\n\n    return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Normalization of the observations\n\nTo compute the normalizing statistics, we run an arbitrary number of random\nsteps in the environment and compute the mean and standard deviation of the\ncollected observations. The :func:`ObservationNorm.init_stats()` method can\nbe used for this purpose. To get the summary statistics, we create a dummy\nenvironment and run it for a given number of steps, collect data over a given\nnumber of steps and compute its summary statistics.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_env_stats():\n    \"\"\"Gets the stats of an environment.\"\"\"\n    proof_env = make_transformed_env(make_env())\n    proof_env.set_seed(seed)\n    t = proof_env.transform[2]\n    t.init_stats(init_env_steps)\n    transform_state_dict = t.state_dict()\n    proof_env.close()\n    return transform_state_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parallel execution\n\nThe following helper function allows us to run environments in parallel.\nRunning environments in parallel can significantly speed up the collection\nthroughput. When using transformed environment, we need to choose whether we\nwant to execute the transform individually for each environment, or\ncentralize the data and transform it in batch. Both approaches are easy to\ncode:\n\n```python\nenv = ParallelEnv(\n    lambda: TransformedEnv(GymEnv(\"HalfCheetah-v4\"), transforms),\n    num_workers=4\n)\nenv = TransformedEnv(\n    ParallelEnv(lambda: GymEnv(\"HalfCheetah-v4\"), num_workers=4),\n    transforms\n)\n```\nTo leverage the vectorization capabilities of PyTorch, we adopt\nthe first method:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def parallel_env_constructor(\n    transform_state_dict,\n):\n    if env_per_collector == 1:\n\n        def make_t_env():\n            env = make_transformed_env(make_env())\n            env.transform[2].init_stats(3)\n            env.transform[2].loc.copy_(transform_state_dict[\"loc\"])\n            env.transform[2].scale.copy_(transform_state_dict[\"scale\"])\n            return env\n\n        env_creator = EnvCreator(make_t_env)\n        return env_creator\n\n    parallel_env = ParallelEnv(\n        num_workers=env_per_collector,\n        create_env_fn=EnvCreator(lambda: make_env()),\n        create_env_kwargs=None,\n        pin_memory=False,\n    )\n    env = make_transformed_env(parallel_env)\n    # we call `init_stats` for a limited number of steps, just to instantiate\n    # the lazy buffers.\n    env.transform[2].init_stats(3, cat_dim=1, reduce_dim=[0, 1])\n    env.transform[2].load_state_dict(transform_state_dict)\n    return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building the model\n\nWe now turn to the setup of the model and loss function. DDPG requires a\nvalue network, trained to estimate the value of a state-action pair, and a\nparametric actor that learns how to select actions that maximize this value.\nIn this tutorial, we will be using two independent networks for these\ncomponents.\n\nRecall that building a torchrl module requires two steps:\n\n- writing the :class:`torch.nn.Module` that will be used as network\n- wrapping the network in a :class:`tensordict.nn.TensorDictModule` where the\n  data flow is handled by specifying the input and output keys.\n\nIn more complex scenarios, :class:`tensordict.nn.TensorDictSequential` can\nalso be used.\n\nIn :func:`make_ddpg_actor`, we use a :class:`torchrl.modules.ProbabilisticActor`\nobject to wrap our policy network. Since DDPG is a deterministic algorithm,\nthis is not strictly necessary. We rely on this class to map the output\naction to the appropriate domain. Alternatively, one could perfectly use a\nnon-linearity such as :class:`torch.tanh` to map the output to the right\ndomain.\n\nThe Q-Value network is wrapped in a :class:`torchrl.modules.ValueOperator`\nthat automatically sets the ``out_keys`` to ``\"state_action_value`` for q-value\nnetworks and ``state_value`` for other value networks.\n\nSince we use lazy modules, it is necessary to materialize the lazy modules\nbefore being able to move the policy from device to device and achieve other\noperations. Hence, it is good practice to run the modules with a small\nsample of data. For this purpose, we generate fake data from the\nenvironment specs.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_ddpg_actor(\n    transform_state_dict,\n    device=\"cpu\",\n):\n    proof_environment = make_transformed_env(make_env())\n    proof_environment.transform[2].init_stats(3)\n    proof_environment.transform[2].load_state_dict(transform_state_dict)\n\n    env_specs = proof_environment.specs\n    out_features = env_specs[\"action_spec\"].shape[0]\n\n    actor_net = MLP(\n        num_cells=[num_cells] * num_layers,\n        activation_class=nn.Tanh,\n        out_features=out_features,\n    )\n    in_keys = [\"observation_vector\"]\n    out_keys = [\"param\"]\n\n    actor_module = TensorDictModule(actor_net, in_keys=in_keys, out_keys=out_keys)\n\n    # We use a ProbabilisticActor to make sure that we map the network output\n    # to the right space using a TanhDelta distribution.\n    actor = ProbabilisticActor(\n        module=actor_module,\n        in_keys=[\"param\"],\n        spec=CompositeSpec(action=env_specs[\"action_spec\"]),\n        safe=True,\n        distribution_class=TanhDelta,\n        distribution_kwargs={\n            \"min\": env_specs[\"action_spec\"].space.minimum,\n            \"max\": env_specs[\"action_spec\"].space.maximum,\n        },\n    ).to(device)\n\n    q_net = MLP(\n        num_cells=[num_cells] * num_layers,\n        activation_class=nn.Tanh,\n        out_features=1,\n    )\n\n    in_keys = in_keys + [\"action\"]\n    qnet = ValueOperator(\n        in_keys=in_keys,\n        module=q_net,\n    ).to(device)\n\n    # init: since we have lazy layers, we should run the network\n    # once to initialize them\n    with torch.no_grad(), set_exploration_mode(\"random\"):\n        td = proof_environment.fake_tensordict()\n        td = td.expand((*td.shape, 2))\n        td = td.to(device)\n        actor(td)\n        qnet(td)\n\n    return actor, qnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluator: building your recorder object\n\nAs the training data is obtained using some exploration strategy, the true\nperformance of our algorithm needs to be assessed in deterministic mode. We\ndo this using a dedicated class, ``Recorder``, which executes the policy in\nthe environment at a given frequency and returns some statistics obtained\nfrom these simulations.\n\nThe following helper function builds this object:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_recorder(actor_model_explore, transform_state_dict):\n    base_env = make_env()\n    recorder = make_transformed_env(base_env)\n    recorder.transform[2].init_stats(3)\n    recorder.transform[2].load_state_dict(transform_state_dict)\n\n    recorder_obj = Recorder(\n        record_frames=1000,\n        frame_skip=frame_skip,\n        policy_exploration=actor_model_explore,\n        recorder=recorder,\n        exploration_mode=\"mean\",\n        record_interval=record_interval,\n    )\n    return recorder_obj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Replay buffer\n\nReplay buffers come in two flavors: prioritized (where some error signal\nis used to give a higher likelihood of sampling to some items than others)\nand regular, circular experience replay.\n\nTorchRL replay buffers are composable: one can pick up the storage, sampling\nand writing strategies. It is also possible to\nstore tensors on physical memory using a memory-mapped array. The following\nfunction takes care of creating the replay buffer with the desired\nhyperparameters:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_replay_buffer(buffer_size, prefetch=3):\n    if prb:\n        sampler = PrioritizedSampler(\n            max_capacity=buffer_size,\n            alpha=0.7,\n            beta=0.5,\n        )\n    else:\n        sampler = RandomSampler()\n    replay_buffer = TensorDictReplayBuffer(\n        storage=LazyMemmapStorage(\n            buffer_size,\n            scratch_dir=buffer_scratch_dir,\n            device=device,\n        ),\n        sampler=sampler,\n        pin_memory=False,\n        prefetch=prefetch,\n    )\n    return replay_buffer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters\n\nAfter having written our helper functions, it is time to set the\nexperiment hyperparameters:\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# The backend can be gym or dm_control\nbackend = \"gym\"\n\nexp_name = \"cheetah\"\n\n# frame_skip batches multiple step together with a single action\n# If > 1, the other frame counts (e.g. frames_per_batch, total_frames) need to\n# be adjusted to have a consistent total number of frames collected across\n# experiments.\nframe_skip = 2\nfrom_pixels = False\n# Scaling the reward helps us control the signal magnitude for a more\n# efficient learning.\nreward_scaling = 5.0\n\n# Number of random steps used as for stats computation using ObservationNorm\ninit_env_steps = 1000\n\n# Exploration: Number of frames before OU noise becomes null\nannealing_frames = 1000000 // frame_skip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Collection\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We will execute the policy on cuda if available\ndevice = (\n    torch.device(\"cpu\") if torch.cuda.device_count() == 0 else torch.device(\"cuda:0\")\n)\n\n# Number of environments in each data collector\nenv_per_collector = 2\n\n# Total frames we will use during training. Scale up to 500K - 1M for a more\n# meaningful training\ntotal_frames = 5000 // frame_skip\n# Number of frames returned by the collector at each iteration of the outer loop\nframes_per_batch = 1000 // frame_skip\ninit_random_frames = 0\n# We'll be using the MultiStep class to have a less myopic representation of\n# upcoming states\nn_steps_forward = 3\n\n# record every 10 batch collected\nrecord_interval = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimizer and optimization\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lr = 5e-4\nweight_decay = 0.0\n# UTD: Number of iterations of the inner loop\nupdate_to_data = 32\nbatch_size = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gamma = 0.99\ntau = 0.005  # Decay factor for the target network\n\n# Network specs\nnum_cells = 64\nnum_layers = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Replay buffer\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# If True, a Prioritized replay buffer will be used\nprb = True\n# Number of frames stored in the buffer\nbuffer_size = min(total_frames, 1000000 // frame_skip)\nbuffer_scratch_dir = \"/tmp/\"\n\nseed = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialization\n\nTo initialize the experiment, we first acquire the observation statistics,\nthen build the networks, wrap them in an exploration wrapper (following the\nseminal DDPG paper, we used an Ornstein-Uhlenbeck process to add noise to the\nsampled actions).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Seeding\ntorch.manual_seed(seed)\nnp.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Normalization stats\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "transform_state_dict = get_env_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Models: policy and q-value network\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "actor, qnet = make_ddpg_actor(\n    transform_state_dict=transform_state_dict,\n    device=device,\n)\nif device == torch.device(\"cpu\"):\n    actor.share_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create a copy of the q-value network to be used as target network\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "qnet_target = deepcopy(qnet).requires_grad_(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The policy is wrapped in a :class:`torchrl.modules.OrnsteinUhlenbeckProcessWrapper`\nexploration module:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "actor_model_explore = OrnsteinUhlenbeckProcessWrapper(\n    actor,\n    annealing_num_steps=annealing_frames,\n).to(device)\nif device == torch.device(\"cpu\"):\n    actor_model_explore.share_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parallel environment creation\n\nWe pass the stats computed earlier to normalize the output of our\nenvironment:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "create_env_fn = parallel_env_constructor(\n    transform_state_dict=transform_state_dict,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data collector\n\nTorchRL provides specialized classes to help you collect data by executing\nthe policy in the environment. These \"data collectors\" iteratively compute\nthe action to be executed at a given time, then execute a step in the\nenvironment and reset it when required.\nData collectors are designed to help developers have a tight control\non the number of frames per batch of data, on the (a)sync nature of this\ncollection and on the resources allocated to the data collection (e.g. GPU,\nnumber of workers etc).\n\nHere we will use\n:class:`torchrl.collectors.MultiaSyncDataCollector`, a data collector that\nwill be executed in an async manner (i.e. data will be collected while\nthe policy is being optimized). With the :class:`MultiaSyncDataCollector`,\nmultiple workers are running rollouts separately. When a batch is asked, it\nis gathered from the first worker that can provide it.\n\nThe parameters to specify are:\n\n- the list of environment creation functions,\n- the policy,\n- the total number of frames before the collector is considered empty,\n- the maximum number of frames per trajectory (useful for non-terminating\n  environments, like dm_control ones).\n\nOne should also pass:\n\n- the number of frames in each batch collected,\n- the number of random steps executed independently from the policy,\n- the devices used for policy execution\n- the devices used to store data before the data is passed to the main\n  process.\n\nCollectors also accept post-processing hooks.\nFor instance, the :class:`torchrl.data.postprocs.MultiStep` class passed as\n``postproc`` makes it so that the rewards of the ``n`` upcoming steps are\nsummed (with some discount factor) and the next observation is changed to\nbe the n-step forward observation. One could pass other transforms too:\nusing :class:`tensordict.nn.TensorDictModule` and\n:class:`tensordict.nn.TensorDictSequential` we can seamlessly append a\nwide range of transforms to our collector.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if n_steps_forward > 0:\n    multistep = MultiStep(n_steps_max=n_steps_forward, gamma=gamma)\nelse:\n    multistep = None\n\ncollector = MultiaSyncDataCollector(\n    create_env_fn=[create_env_fn, create_env_fn],\n    policy=actor_model_explore,\n    total_frames=total_frames,\n    max_frames_per_traj=1000,\n    frames_per_batch=frames_per_batch,\n    init_random_frames=init_random_frames,\n    reset_at_each_iter=False,\n    postproc=multistep,\n    split_trajs=True,\n    devices=[device, device],  # device for execution\n    storing_devices=[device, device],  # device where data will be stored and passed\n    pin_memory=False,\n    update_at_each_batch=False,\n    exploration_mode=\"random\",\n)\n\ncollector.set_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Replay buffer\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "replay_buffer = make_replay_buffer(buffer_size, prefetch=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Recorder\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "recorder = make_recorder(actor_model_explore, transform_state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimizer\n\nFinally, we will use the Adam optimizer for the policy and value network,\nwith the same learning rate for both.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer_actor = optim.Adam(actor.parameters(), lr=lr, weight_decay=weight_decay)\noptimizer_qnet = optim.Adam(qnet.parameters(), lr=lr, weight_decay=weight_decay)\ntotal_collection_steps = total_frames // frames_per_batch\n\nscheduler1 = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer_actor, T_max=total_collection_steps\n)\nscheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer_qnet, T_max=total_collection_steps\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time to train the policy\n\nSome notes about the following training loop:\n\n- :func:`torchrl.objectives.utils.hold_out_net` is a TorchRL context manager\n  that temporarily sets :func:`torch.Tensor.requires_grad_()` to False for\n  a designated set of network parameters. This is used to\n  prevent :func:`torch.Tensor.backward()`` from writing gradients on\n  parameters that need not to be differentiated given the loss at hand.\n- The value network is designed using the\n  :class:`torchrl.modules.ValueOperator` subclass from\n  :class:`tensordict.nn.TensorDictModule` class. As explained earlier,\n  this class will write a ``\"state_action_value\"`` entry if one of its\n  ``in_keys`` is named ``\"action\"``, otherwise it will assume that only the\n  state-value is returned and the output key will simply be ``\"state_value\"``.\n  In the case of DDPG, the value if of the state-action pair,\n  hence the ``\"state_action_value\"`` will be used.\n- The :func:`torchrl.envs.utils.step_mdp(tensordict)` helper function is the\n  equivalent of the ``obs = next_obs`` command found in multiple RL\n  algorithms. It will return a new :class:`tensordict.TensorDict` instance\n  that contains all the data that will need to be used in the next iteration.\n  This makes it possible to pass this new tensordict to the policy or\n  value network.\n- When using prioritized replay buffer, a priority key is added to the\n  sampled tensordict (named ``\"td_error\"`` by default). Then, this\n  TensorDict will be fed back to the replay buffer using the\n  :func:`torchrl.data.replay_buffers.TensorDictReplayBuffer.update_tensordict_priority`\n  method. Under the hood, this method will read the index present in the\n  TensorDict as well as the priority value, and update its list of priorities\n  at these indices.\n- TorchRL provides optimized versions of the loss functions (such as this one)\n  where one only needs to pass a sampled tensordict and obtains a dictionary\n  of losses and metadata in return (see :mod:`torchrl.objectives` for more\n  context). Here we write the full loss function in the optimization loop\n  for transparency.\n  Similarly, the target network updates are written explicitly but\n  TorchRL provides a couple of dedicated classes for this\n  (see :class:`torchrl.objectives.SoftUpdate` and\n  :class:`torchrl.objectives.HardUpdate`).\n- After each collection of data, we call :func:`collector.update_policy_weights_()`,\n  which will update the policy network weights on the data collector. If the\n  code is executed on cpu or with a single cuda device, this part can be\n  omitted. If the collector is executed on another device, then its weights\n  must be synced with those on the main, training process and this method\n  should be incorporated in the training loop (ideally early in the loop in\n  async settings, and at the end of it in sync settings).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rewards = []\nrewards_eval = []\n\n# Main loop\nnorm_factor_training = (\n    sum(gamma**i for i in range(n_steps_forward)) if n_steps_forward else 1\n)\n\ncollected_frames = 0\npbar = tqdm.tqdm(total=total_frames)\nr0 = None\nfor i, tensordict in enumerate(collector):\n\n    # update weights of the inference policy\n    collector.update_policy_weights_()\n\n    if r0 is None:\n        r0 = tensordict[\"reward\"].mean().item()\n    pbar.update(tensordict.numel())\n\n    # extend the replay buffer with the new data\n    if (\"collector\", \"mask\") in tensordict.keys(True):\n        # if multi-step, a mask is present to help filter padded values\n        current_frames = tensordict[\"collector\", \"mask\"].sum()\n        tensordict = tensordict[tensordict.get((\"collector\", \"mask\"))]\n    else:\n        tensordict = tensordict.view(-1)\n        current_frames = tensordict.numel()\n    collected_frames += current_frames\n    replay_buffer.extend(tensordict.cpu())\n\n    # optimization steps\n    if collected_frames >= init_random_frames:\n        for _ in range(update_to_data):\n            # sample from replay buffer\n            sampled_tensordict = replay_buffer.sample(batch_size).clone()\n\n            # compute loss for qnet and backprop\n            with hold_out_net(actor):\n                # get next state value\n                next_tensordict = step_mdp(sampled_tensordict)\n                qnet_target(actor(next_tensordict))\n                next_value = next_tensordict[\"state_action_value\"]\n                assert not next_value.requires_grad\n            value_est = (\n                sampled_tensordict[\"reward\"]\n                + gamma * (1 - sampled_tensordict[\"done\"].float()) * next_value\n            )\n            value = qnet(sampled_tensordict)[\"state_action_value\"]\n            value_loss = (value - value_est).pow(2).mean()\n            # we write the td_error in the sampled_tensordict for priority update\n            # because the indices of the samples is tracked in sampled_tensordict\n            # and the replay buffer will know which priorities to update.\n            sampled_tensordict[\"td_error\"] = (value - value_est).pow(2).detach()\n            value_loss.backward()\n\n            optimizer_qnet.step()\n            optimizer_qnet.zero_grad()\n\n            # compute loss for actor and backprop:\n            # the actor must maximise the state-action value, hence the loss\n            # is the neg value of this.\n            sampled_tensordict_actor = sampled_tensordict.select(*actor.in_keys)\n            with hold_out_net(qnet):\n                qnet(actor(sampled_tensordict_actor))\n            actor_loss = -sampled_tensordict_actor[\"state_action_value\"]\n            actor_loss.mean().backward()\n\n            optimizer_actor.step()\n            optimizer_actor.zero_grad()\n\n            # update qnet_target params\n            for (p_in, p_dest) in zip(qnet.parameters(), qnet_target.parameters()):\n                p_dest.data.copy_(tau * p_in.data + (1 - tau) * p_dest.data)\n            for (b_in, b_dest) in zip(qnet.buffers(), qnet_target.buffers()):\n                b_dest.data.copy_(tau * b_in.data + (1 - tau) * b_dest.data)\n\n            # update priority\n            if prb:\n                replay_buffer.update_tensordict_priority(sampled_tensordict)\n\n    rewards.append(\n        (i, tensordict[\"reward\"].mean().item() / norm_factor_training / frame_skip)\n    )\n    td_record = recorder(None)\n    if td_record is not None:\n        rewards_eval.append((i, td_record[\"r_evaluation\"].item()))\n    if len(rewards_eval):\n        pbar.set_description(\n            f\"reward: {rewards[-1][1]: 4.4f} (r0 = {r0: 4.4f}), reward eval: reward: {rewards_eval[-1][1]: 4.4f}\"\n        )\n\n    # update the exploration strategy\n    actor_model_explore.step(current_frames)\n    if collected_frames >= init_random_frames:\n        scheduler1.step()\n        scheduler2.step()\n\ncollector.shutdown()\ndel collector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment results\n\nWe make a simple plot of the average rewards during training. We can observe\nthat our policy learned quite well to solve the task.\n\n**Note**: As already mentioned above, to get a more reasonable performance,\nuse a greater value for ``total_frames`` e.g. 1M.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure()\nplt.plot(*zip(*rewards), label=\"training\")\nplt.plot(*zip(*rewards_eval), label=\"eval\")\nplt.legend()\nplt.xlabel(\"iter\")\nplt.ylabel(\"reward\")\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sampling trajectories and using TD(lambda)\n\nTD(lambda) is known to be less biased than the regular TD-error we used in\nthe previous example. To use it, however, we need to sample trajectories and\nnot single transitions.\n\nWe modify the previous example to make this possible.\n\nThe first modification consists in building a replay buffer that stores\ntrajectories (and not transitions).\n\nSpecifically, we'll collect trajectories of (at most)\n250 steps (note that the total trajectory length is actually 1000 frames, but\nwe collect batches of 500 transitions obtained over 2 environments running in\nparallel, hence only 250 steps per trajectory are collected at any given\ntime). Hence, we'll divide our replay buffer size by 250:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "buffer_size = 100000 // frame_skip // 250\nprint(\"the new buffer size is\", buffer_size)\nbatch_size_traj = max(4, batch_size // 250)\nprint(\"the new batch size for trajectories is\", batch_size_traj)\n\nn_steps_forward = 0  # disable multi-step for simplicity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code is identical to the initialization we made earlier:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(seed)\nnp.random.seed(seed)\n\n# get stats for normalization\ntransform_state_dict = get_env_stats()\n\n# Actor and qnet instantiation\nactor, qnet = make_ddpg_actor(\n    transform_state_dict=transform_state_dict,\n    device=device,\n)\nif device == torch.device(\"cpu\"):\n    actor.share_memory()\n\n# Target network\nqnet_target = deepcopy(qnet).requires_grad_(False)\n\n# Exploration wrappers:\nactor_model_explore = OrnsteinUhlenbeckProcessWrapper(\n    actor,\n    annealing_num_steps=annealing_frames,\n).to(device)\nif device == torch.device(\"cpu\"):\n    actor_model_explore.share_memory()\n\n# Environment setting:\ncreate_env_fn = parallel_env_constructor(\n    transform_state_dict=transform_state_dict,\n)\n# Batch collector:\nif n_steps_forward > 0:\n    multistep = MultiStep(n_steps_max=n_steps_forward, gamma=gamma)\nelse:\n    multistep = None\ncollector = MultiaSyncDataCollector(\n    create_env_fn=[create_env_fn, create_env_fn],\n    policy=actor_model_explore,\n    total_frames=total_frames,\n    max_frames_per_traj=1000,\n    frames_per_batch=frames_per_batch,\n    init_random_frames=init_random_frames,\n    reset_at_each_iter=False,\n    postproc=multistep,\n    split_trajs=False,\n    devices=[device, device],  # device for execution\n    storing_devices=[device, device],  # device where data will be stored and passed\n    seed=None,\n    pin_memory=False,\n    update_at_each_batch=False,\n    exploration_mode=\"random\",\n)\ncollector.set_seed(seed)\n\n# Replay buffer:\nreplay_buffer = make_replay_buffer(buffer_size, prefetch=0)\n\n# trajectory recorder\nrecorder = make_recorder(actor_model_explore, transform_state_dict)\n\n# Optimizers\noptimizer_actor = optim.Adam(actor.parameters(), lr=lr, weight_decay=weight_decay)\noptimizer_qnet = optim.Adam(qnet.parameters(), lr=lr, weight_decay=weight_decay)\ntotal_collection_steps = total_frames // frames_per_batch\n\nscheduler1 = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer_actor, T_max=total_collection_steps\n)\nscheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer_qnet, T_max=total_collection_steps\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training loop needs to be slightly adapted.\nFirst, whereas before extending the replay buffer we used to flatten the\ncollected data, this won't be the case anymore. To understand why, let's\ncheck the output shape of the data collector:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for data in collector:\n    print(data.shape)\n    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that our data has shape ``[2, 250]`` as expected: 2 envs, each\nreturning 250 frames.\n\nLet's import the td_lambda function:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.objectives.value.functional import vec_td_lambda_advantage_estimate\n\nlmbda = 0.95"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training loop is roughly the same as before, with the exception that we\ndon't flatten the collected data. Also, the sampling from the replay buffer\nis slightly different: We will collect at minimum four trajectories, compute\nthe returns (TD(lambda)), then sample from these the values we'll be using\nto compute gradients. This ensures that do not have batches that are\n'too big' but still compute an accurate return.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rewards = []\nrewards_eval = []\n\n# Main loop\nnorm_factor_training = (\n    sum(gamma**i for i in range(n_steps_forward)) if n_steps_forward else 1\n)\n\ncollected_frames = 0\n# # if tqdm is to be used\n# pbar = tqdm.tqdm(total=total_frames)\nr0 = None\nfor i, tensordict in enumerate(collector):\n\n    # update weights of the inference policy\n    collector.update_policy_weights_()\n\n    if r0 is None:\n        r0 = tensordict[\"reward\"].mean().item()\n\n    # extend the replay buffer with the new data\n    current_frames = tensordict.numel()\n    collected_frames += current_frames\n    replay_buffer.extend(tensordict.cpu())\n\n    # optimization steps\n    if collected_frames >= init_random_frames:\n        for _ in range(update_to_data):\n            # sample from replay buffer\n            sampled_tensordict = replay_buffer.sample(batch_size_traj)\n            # reset the batch size temporarily, and exclude index\n            # whose shape is incompatible with the new size\n            index = sampled_tensordict.get(\"index\")\n            sampled_tensordict.exclude(\"index\", inplace=True)\n\n            # compute loss for qnet and backprop\n            with hold_out_net(actor):\n                # get next state value\n                next_tensordict = step_mdp(sampled_tensordict)\n                qnet_target(actor(next_tensordict.view(-1))).view(\n                    sampled_tensordict.shape\n                )\n                next_value = next_tensordict[\"state_action_value\"]\n                assert not next_value.requires_grad\n\n            # This is the crucial part: we'll compute the TD(lambda)\n            # instead of a simple single step estimate\n            done = sampled_tensordict[\"done\"]\n            reward = sampled_tensordict[\"reward\"]\n            value = qnet(sampled_tensordict.view(-1)).view(sampled_tensordict.shape)[\n                \"state_action_value\"\n            ]\n            advantage = vec_td_lambda_advantage_estimate(\n                gamma, lmbda, value, next_value, reward, done\n            )\n            # we sample from the values we have computed\n            rand_idx = torch.randint(0, advantage.numel(), (batch_size,))\n            value_loss = advantage.view(-1)[rand_idx].pow(2).mean()\n\n            # we write the td_error in the sampled_tensordict for priority update\n            # because the indices of the samples is tracked in sampled_tensordict\n            # and the replay buffer will know which priorities to update.\n            value_loss.backward()\n\n            optimizer_qnet.step()\n            optimizer_qnet.zero_grad()\n\n            # compute loss for actor and backprop: the actor must maximise the state-action value, hence the loss is the neg value of this.\n            sampled_tensordict_actor = sampled_tensordict.select(*actor.in_keys)\n            with hold_out_net(qnet):\n                qnet(actor(sampled_tensordict_actor.view(-1))).view(\n                    sampled_tensordict.shape\n                )\n            actor_loss = -sampled_tensordict_actor[\"state_action_value\"]\n            actor_loss.view(-1)[rand_idx].mean().backward()\n\n            optimizer_actor.step()\n            optimizer_actor.zero_grad()\n\n            # update qnet_target params\n            for (p_in, p_dest) in zip(qnet.parameters(), qnet_target.parameters()):\n                p_dest.data.copy_(tau * p_in.data + (1 - tau) * p_dest.data)\n            for (b_in, b_dest) in zip(qnet.buffers(), qnet_target.buffers()):\n                b_dest.data.copy_(tau * b_in.data + (1 - tau) * b_dest.data)\n\n            # update priority\n            sampled_tensordict.batch_size = [batch_size_traj]\n            sampled_tensordict[\"td_error\"] = advantage.detach().pow(2).mean(1)\n            sampled_tensordict[\"index\"] = index\n            if prb:\n                replay_buffer.update_tensordict_priority(sampled_tensordict)\n\n    rewards.append(\n        (i, tensordict[\"reward\"].mean().item() / norm_factor_training / frame_skip)\n    )\n    td_record = recorder(None)\n    if td_record is not None:\n        rewards_eval.append((i, td_record[\"r_evaluation\"].item()))\n    #     if len(rewards_eval):\n    #         pbar.set_description(f\"reward: {rewards[-1][1]: 4.4f} (r0 = {r0: 4.4f}), reward eval: reward: {rewards_eval[-1][1]: 4.4f}\")\n\n    # update the exploration strategy\n    actor_model_explore.step(current_frames)\n    if collected_frames >= init_random_frames:\n        scheduler1.step()\n        scheduler2.step()\n\ncollector.shutdown()\ndel create_env_fn\ndel collector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can observe that using TD(lambda) made our results considerably more\nstable for a similar training speed:\n\n**Note**: As already mentioned above, to get a more reasonable performance,\nuse a greater value for ``total_frames`` e.g. 1000000.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure()\nplt.plot(*zip(*rewards), label=\"training\")\nplt.plot(*zip(*rewards_eval), label=\"eval\")\nplt.legend()\nplt.xlabel(\"iter\")\nplt.ylabel(\"reward\")\nplt.tight_layout()\nplt.title(\"TD-labmda DDPG results\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}